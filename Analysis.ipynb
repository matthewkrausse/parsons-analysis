{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_repo_data import main\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import nbformat\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "commits = pd.read_csv('data/commits.csv')\n",
    "users = pd.read_csv('data/users.csv')\n",
    "issues = pd.read_csv('data/issues.csv')\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Analytics\n",
    "## Issues: User Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart of the user issues counts\n",
    "issues_counts = issues.value_counts(subset=['user'])\n",
    "\n",
    "issues_counts = issues_counts.reset_index()\n",
    "\n",
    "issues_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "\n",
    "fig = px.bar(data_frame=issues_counts, x='user', y='count', title='Issues per User')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Users: User Type, Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart of the user type counts\n",
    "users_counts = users.value_counts(subset=['type'])\n",
    "\n",
    "users_counts = users_counts.reset_index()\n",
    "\n",
    "users_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "\n",
    "fig = px.bar(data_frame=users_counts, x='type', y='count', title='User Type Counts')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users: User Company, Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart of the user company counts\n",
    "users_counts = users.value_counts(subset=['company'])\n",
    "\n",
    "users_counts = users_counts.reset_index()\n",
    "\n",
    "users_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "\n",
    "fig = px.bar(data_frame=users_counts, x='company', y='count', title='User Company Counts')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users: User Location, Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart of the user location counts\n",
    "users_counts = users.value_counts(subset=['location'])\n",
    "\n",
    "users_counts = users_counts.reset_index()\n",
    "\n",
    "users_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "\n",
    "fig = px.bar(data_frame=users_counts, x='location', y='count', title='User Location Counts')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users: User Bios, Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart of the user bio counts\n",
    "users_counts = users.value_counts(subset=['bio'])\n",
    "\n",
    "users_counts = users_counts.reset_index()\n",
    "\n",
    "users_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "\n",
    "fig = px.bar(data_frame=users_counts, x='bio', y='count', title='User Bio Counts')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Analytics\n",
    "## Issues: Issue Titles, Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word cloud from issue titles (https://github.com/amueller/word_cloud/blob/main/examples/simple.py)\n",
    "\n",
    "text = ' '.join(issues['title'])\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "text_lemmatized = [lemma.lemmatize(w.lower()) for w in text.split()]\n",
    "text = ' '.join(text_lemmatized)\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\n",
    "    'parson','python', 'connector', 'fix', 'upsert', 'add','addition', 'update',\n",
    "    'remove', 'change', 'doc', 'docs','documentation', 'table',\n",
    "    'function', 'use', 'error', 'data', 'bump', 'version','type',\n",
    "    'test', 'release', 'feat', 'feature', 'support','method',    \n",
    "    'column','bug','code','added','file','phone','string','added',\n",
    "    'name','class','list','issue','py','number','empty','github',\n",
    "    'create','query','option','row','import'\n",
    "])\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40,stopwords=stopwords).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues: Open Issues Titles, Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud of open issues\n",
    "\n",
    "open_issues = issues[issues['state'] == 'open']\n",
    "\n",
    "import inspect, parsons\n",
    "\n",
    "modules = [module[0] for module in inspect.getmembers(parsons, inspect.ismodule)]\n",
    "\n",
    "text = open_issues['title'].tolist()\n",
    "\n",
    "text = [item.lower().strip() for item in text]\n",
    "\n",
    "norm_text = [item.split() for item in text]\n",
    "\n",
    "flattened_list = [item for sublist in norm_text for item in sublist]\n",
    "\n",
    "\n",
    "filtered_text = [item.lower().strip() for item in flattened_list if item in modules]\n",
    "\n",
    "text = \" \".join(filtered_text)\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "text_lemmatized = [lemma.lemmatize(w.lower()) for w in text.split()]\n",
    "text = ' '.join(text_lemmatized)\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\n",
    "    'parson','python', 'connector', 'fix', 'upsert', 'add','addition', 'update',\n",
    "    'remove', 'change', 'doc', 'docs','documentation', 'table',\n",
    "    'function', 'use', 'error', 'data', 'bump', 'version','type',\n",
    "    'test', 'release', 'feat', 'feature', 'support','method',    \n",
    "    'column','bug','code','added','file','phone','string','added',\n",
    "    'name','class','list','issue','py','number','empty','github',\n",
    "    'create','query','option','row','import'\n",
    "])\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40,stopwords=stopwords).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues: Open Issues, Wordcloud--Only Names of Parsons Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordclould with only names of parsons modules for open issues\n",
    "import inspect, parsons\n",
    "from collections import Counter\n",
    "\n",
    "modules = [module[0] for module in inspect.getmembers(parsons, inspect.ismodule)]\n",
    "\n",
    "titles_unnorm = open_issues['title'].tolist()\n",
    "\n",
    "titles = [list(set(title.lower().split())) for title in titles_unnorm] #use set because don't want repeated module name per title\n",
    "\n",
    "print(titles)\n",
    "\n",
    "titles_modules = [title_item for title in titles for title_item in title if title_item in modules ] \n",
    "\n",
    "print(Counter(titles_modules))\n",
    "\n",
    "titles_modules = \" \".join(titles_modules)\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40,stopwords=stopwords).generate(titles_modules)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues: Open Issues, Wordcloud (Other Than Parsons Modules Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordclould with only names of *non* parsons-modules for open issues\n",
    "import inspect, parsons\n",
    "from collections import Counter\n",
    "\n",
    "modules = [module[0] for module in inspect.getmembers(parsons, inspect.ismodule)]\n",
    "\n",
    "titles_unnorm = open_issues['title'].tolist()\n",
    "\n",
    "titles = [list(set(title.lower().split())) for title in titles_unnorm] #use set because don't want repeated module name per title\n",
    "\n",
    "print(titles)\n",
    "\n",
    "titles_not_modules = [title_item for title in titles for title_item in title if title_item not in modules ] #do this for each title and then take set\n",
    "\n",
    "print(Counter(titles_not_modules))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "titles_not_modules = \" \".join(titles_not_modules)\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40,stopwords=stopwords).generate(titles_not_modules)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues: Open Issues, States, Barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart of the issue states\n",
    "issues_counts = issues.value_counts(subset=['state'])\n",
    "\n",
    "issues_counts = issues_counts.reset_index()\n",
    "\n",
    "issues_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "\n",
    "fig = px.bar(data_frame=issues_counts, x='state', y='count', title='Issue States')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues: Issues, Body, Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze issues body\n",
    "\n",
    "body = issues['body'].dropna().tolist()\n",
    "\n",
    "body = \" \".join(body)\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "body_lemmatized = [lemma.lemmatize(w.lower()) for w in body.split()]\n",
    "\n",
    "body = ' '.join(body_lemmatized)\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40,stopwords=stopwords).generate(body)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues: Issues, Body, Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue details two problems: Mac+Python 3.9 causing psycopg2 install errors and a global issue with limited dependency installs due to a pydantic release.\n",
      "Added a method to return the full export unparsed for ELT, branching from @jburchard's Empower connector PR.\n",
      "This PR updates the 'civis' package from version 1.16.1 to 2.4.0, adding features like customizable retries, YAML validation, and Python 3.13 support, while also deprecating and removing some methods and features.\n",
      "Bumps grpcio from 1.62.2 to 1.68.0, including core improvements, C++ build updates, Python support for 3.13, and various bug fixes.\n",
      "This GitHub issue updates the CodeQL Action from version 3.26.12 to 3.27.4 and discusses changes such as fixing a setup issue, performance improvements with Zstandard for bundles, and updating the default CodeQL bundle version.\n",
      "Dependabot updates the [install-pinned/uv](https://github.com/install-pinned/uv) dependency from de03c60 to 5743f94 with multiple README and pins updates.\n",
      "The issue discusses an update to the Dockerfile to use Python 3.11.\n",
      "This PR updates the [install-pinned/uv](https://github.com/install-pinned/uv) library from a specific commit to a newer one with updates to README.md and pin changes.\n",
      "Dependabot updated the [github/codeql-action](https://github.com/github/codeql-action) from version 3.26.12 to 3.27.1 to improve CodeQL tools installation speed on Linux/macOS and update the default CodeQL bundle version.\n",
      "Dependabot updates the [install-pinned/uv](https://github.com/install-pinned/uv) dependency from de03c60 to 079b372, detailing commits and update instructions.\n",
      "Dependabot updates grpcio from 1.62.2 to 1.67.1, including bug fixes, improvements, and support for Python 3.13.\n",
      "The function incorrectly appends paginated data as lists of lists; it should concatenate lists using '+' or '.extend()' for results over PAGE_LIMIT.\n",
      "Propose adding an Introspection call to Parsons API to efficiently check connection and key permissions with VAN, detailing its benefits and a possible Python implementation.\n",
      "Dependabot updated the [install-pinned/uv](https://github.com/install-pinned/uv) dependency from de03c60 to dd25a3a, with multiple commits updating README and pins.\n",
      "The PR updates actions/checkout from version 4.2.1 to 4.2.2, making changes like leveraging environment variables in `url-helper.ts` and expanding unit test coverage for `isGhes`.\n",
      "Updated the [github/codeql-action](https://github.com/github/codeql-action) from 3.26.12 to 3.27.0, which includes a minimum CodeQL bundle version bump and fixes an issue with the `upload-sarif` action failing due to missing inputs.\n",
      "Dependabot updated [actions/setup-python](https://github.com/actions/setup-python) from 5.2.0 to 5.3.0 with enhancements, bug fixes, and workflow improvements.\n",
      "Dependabot updated the [install-pinned/uv](https://github.com/install-pinned/uv) dependency from de03c60 to e185bca with commits to README and pin updates.\n",
      "This PR updates the github/codeql-action to version 3.26.13, which includes no user-facing changes and some internal commits.\n",
      "Updated grpcio from 1.62.2 to 1.67.0 including bug fixes, support for Python 3.13, and dropped Ruby 2.7 support.\n",
      "The issue details a version bump for gspread from 3.7.0 to 6.1.4, highlighting key changes, bug fixes, and contributions in recent releases.\n",
      "Dependabot updated suds-py3 from 1.4.4.1 to 1.4.5.0, which is a wheel distribution release with no code changes.\n",
      "Updated google-cloud-storage from 2.16.0 to 2.18.2, adding a regression test for range read retry and other bug fixes, and introducing OpenTelemetry Tracing support.\n",
      "Dependabot updated mysql-connector-python from 8.0.18 to 9.1.0, adding new features, dropping Python 3.8 support, and fixing bugs.\n",
      "Reversion of move-coop/parsons#1154 due to test failures.\n",
      "Python 3.13 has been released; details are available in the what's new documentation.\n",
      "This PR updates joblib from version 1.2.0 to 1.4.2, fixing backward incompatible changes in `MemorizedFunc.call` and adding various features and improvements.\n",
      "Upgraded `censusgeocode` from 0.4.3.post1 to 0.5.2, which includes accepting \"zip\" or \"zipcode\" in `CensusGeocode.address` and warnings for batches over 10,000 records.\n",
      "Dependabot will manage PR conflicts unless modified and offers commands to manage PR actions, like rebase and merge.\n",
      "Updated gspread from 3.7.0 to 6.1.3, including bug fixes, new contributions, and dependency updates.\n",
      "Updated google-cloud-storage from 2.16.0 to 2.18.2 with bug fixes and new features like OpenTelemetry Tracing support.\n",
      "Upgraded civis from 1.16.1 to 2.3.0, adding new features, refining polling intervals, enhancing startup speed, and addressing a security vulnerability.\n",
      "Updated grpcio from version 1.62.2 to 1.66.2, including bug fixes and Python 3.13 support.\n",
      "Dependabot updates Docker package, bumping Python from 3.8 to 3.13, and manual rebase may be needed due to auto-rebase being turned off.\n",
      "Updated ossf/scorecard-action from 2.3.1 to 2.4.0, introducing the Scorecard v5 release and Maintainer Annotation feature for suppressing Code Scanning false positives.\n",
      "Upgraded pygithub from 1.51 to 2.4.0, comprising new features such as custom authentication, improvements like support for workflow jobs, bug fixes, and dropped Python 3.7 support.\n",
      "Dependabot updated step-security/harden-runner to 2.10.1 fixing a DNS resolution issue and added ARM support for GitHub-hosted runners.\n",
      "This GitHub issue details an automated Dependabot version bump for xmltodict from 0.11.0 to 0.14.1 with improvements, new features, and deprecated support for older Python versions.\n",
      "Dependabot updates the [install-pinned/uv](https://github.com/install-pinned/uv) dependency and awaits merge after CI passes with options to manage integration.\n",
      "The PR updates the google-cloud-bigquery package from 3.23.1 to 3.26.0, introducing new features, bug fixes, dependency updates, and documentation improvements.\n",
      "Dependabot updated [actions/checkout](https://github.com/actions/checkout) from 4.1.1 to 4.2.1 with new contributions and ref/commit outputs, along with several dependency updates.\n",
      "Dependabot updated `github/codeql-action` from 3.26.6 to 3.26.12, including a deprecation warning for CodeQL CLI versions <=2.14.5 and changes for `actions/download-artifact@v4` compatibility.\n",
      "Fix applied for incorrect `kwargs` passing in issue #916, now directed to the correct variable.\n",
      "The issue is regarding the Catalist SFTP server hanging indefinitely, which has been highlighted as an important concern.\n",
      "This update allows the GCS connector to authenticate via gcloud oauth without the need for service account credentials.\n",
      "The author suggests modifying the GoogleBigQuery class setup to mirror Redshift, simplifying the use of multiple instances in one environment.\n",
      "The `email.parseaddr` function's behavior varies with different Python patch versions, as seen in Python's issue 102988 and changelogs.\n",
      "Recent Python patches introduce a breaking change to the `email.parseaddr` library, necessitating test updates for compatibility.\n",
      "Fixed error in Issue #1142 by aligning the method with the API changes that no longer require a phone number for phone type contacts.\n",
      "This update adds automatic conversion of dict columns to JSON for `BigQuery().copy()`, with an option to disable via `convert_dict_columns_to_json` flag.\n",
      "The apply_response method in VAN class incorrectly raises an error for missing phone numbers, despite VAN no longer requiring them for bulk uploads.\n",
      "The Action Builder API's DELETE verb can remove entity records from campaigns, but the associated test is not very meaningful.\n",
      "Replace `python setup.py develop` instructions with the command from the provided link; consider adding docs for editable installs.\n",
      "A temporary if/else statement was added to resolve a security issue flagged by bandit, to be removed after dropping Python 3.8 support.\n",
      "This change allows the Postgres copy method to work even when the source table's columns are a subset or in different order than the destination table's.\n",
      "The db_sync module raises errors when the source table is empty due to a row count attempt, even if verify_row_count is set to false.\n",
      "The Dockerhub workflow is failing due to pip dependency resolution timeouts, and switching to uv has been proposed as a solution.\n",
      "Issue: Request to add `skipMatching()` to the canvass context in the `apply response` method in Parsons project.\n",
      "Request to add the `skipMatching()` parameter to the `apply_response()` function to address issue with contact attempts not being recorded properly when synced with different dates.\n",
      "Rebuilt GitHub action YAMLs for improved UI experience, faster lint/formatting, with added security checks, dependabot, and updated test coverage requirements.\n",
      "Fixes issue #1129 by adding missing Email and Contact notes to the RST file.\n",
      "Updates to the latest patched version to resolve issue #1127.\n",
      "The NGPVAN email module lacks documentation and needs to be included in the main project docs.\n",
      "A user reports a failed pytest due to an expectation that ',com' should validate but notes confusion as to why this didn't appear in GitHub actions.\n",
      "Installation with 'uv' fails due to 'requests==2.32.0' being yanked for CVE-2024-35195 conflict.\n",
      "The author created coverage testing due to Issue #1114 but is unsure if reporting on each version is useful.\n",
      "Upgrading mysql-connector-python to 8.3.0 may break compatibility with MySQL 5.5 and 5.6, despite passing tests.\n",
      "The `test_smartmatch` test is skipped on Windows due to a permission error when writing to a CSV file, with a link to the relevant commit and error log.\n",
      "Writing date/datetime data to Google Sheets with Gspread results in strings; a `USER_ENTERED` flag integration could solve it, and the issuer offers to work on a PR.\n",
      "This update accelerates doc creation by updating CircleCI requirements and the Python version, preventing easy timeouts.\n",
      "Summary: Updating packages listed in `requirements.txt` due to findings from a security review.\n",
      "The issue discusses implementing Step Security tool to enhance the security of CI/CD workflows following a security review.\n",
      "Issue ready to merge with one failing test related to a permissions error when reading a test CSV, alongside improvements to Windows testing and installation.\n",
      "Combine requirements-dev.txt and docs/requirements.txt into one and update CI workflows and documentation accordingly.\n",
      "The change introduces `--output-format=github` to `ruff check` for GitHub PR annotations and links to code errors.\n",
      "Issue summary: Addresses reference to issue #1113.\n",
      "The issue addresses inconsistent attributes for CSVView across views, causing AttributeError, which was temporarily fixed by treating the source as a normal CSV.\n",
      "Evaluate the effectiveness and identify any gaps in our current tests.\n",
      "The user is referencing the Scorecard documentation with mentions of @anzelpwj and @shaunagm for potential discussion or action.\n",
      "Introduces a new connector for the Community.com API.\n",
      "This PR modifies a function to accept an external GCS instance for dependency injection while maintaining backward compatibility.\n",
      "The issue requests an update to setup.py for preparing a new software release.\n",
      "Issue #1046: CONTRIBUTING.md file accidentally reverted to older version in #938; this update restores the minimal version pointing to Parsons website's guide.\n",
      "This submission continues PR #981's changes for the BigQuery connector, aiming to resolve issue #910, and seeks input on default timeout implementation.\n",
      "Implemented cautious pauses in test scripts to prevent rate limiting, and removed the redundant `test_read_sheet` due to deprecation.\n",
      "Resolves issue #1084 by updating references to the obsolete \"Parsons\" API key with instructions for requesting a custom integration from NGPVAN.\n",
      "Implemented a new SSH utility function for querying databases and added SQL Mirror support for ActionNetwork with read-only access to all tables.\n",
      "The author has created two new functions to support ActionNetwork's API and split a previous problematic PR into two, apologizing for any inconvenience.\n",
      "A commit adding SQL Mirror support was reverted due to issues detailed in the original pull request #1025.\n",
      "The author suggests that a specific item should be included in requirements.txt and setup.py, but not in the current location.\n",
      "The author suggests not making redshift and sshtunnel mandatory dependencies for all users of the ActionNetwork connector.\n",
      "A concern was raised that unit testing failed to catch a specific issue.\n",
      "The PR separates dbt utility code from logging, introduces breaking changes for a cleaner interface, and provides improved compatibility and documentation.\n",
      "Dependabot updates setuptools from 68.0.0 to 70.0.0, bringing new features, bug fixes, improved documentation, deprecations, and removals.\n",
      "A typo was introduced in commit 6dfaaec26dfee18e6f8f00f83bc7f02396f22944.\n",
      "This PR updates the Zoom connector with date filtering for `get_meetings()` and fixes post-processing in `get_past_meeting_poll_metadata`.\n",
      "This PR updates the Dockerfile base image due to Parsons image failure and deprecates support for Python 3.7.\n",
      "This pull request introduces a new 'campaignId' parameter to the People class's apply_responses method, addressing issue #1075.\n",
      "The author notes that quotes are no longer necessary since commit 617d06f5b6e036e83c60bac7f5b6bc35b0bbb6ab.\n",
      "The pull request #1070 in the move-coop/parsons repository addresses an issue with handling quotes in data.\n",
      "Issue regarding Python 3.7 reaching End-of-Life status on 2023-06-27, with potential implications for the project that KasiaHinkson can provide insight on.\n",
      "The `geocode_address_batch` function should include \"id\" in its column checks.\n",
      "NGPVAN revoked Parsons API key; users must now request custom integrations, necessitating high-priority documentation updates for this widely used connector.\n",
      "A recent PR caused a bug where a custom delimiter in CSV files isn't passed to BigQuery copy job, resulting in failed loads.\n",
      "The `airtable-python-wrapper` renamed to `pyairtable` added new methods and a `typecast` option for `insert_record`.\n",
      "Added a feature to set a timeout for SFTP connections that raises TimeoutExcept if the server doesn't respond.\n",
      "Issue: Providing an empty data table to the `overwrite` method raises a `ValueError`; a warning should be logged instead, similar to `paste_data_in_sheet`.\n",
      "Updated urllib3 from 1.26.18 to 1.26.19; includes Proxy-Authorization header management and OpenSSL error handling improvements.\n",
      "Added two new bulk import methods, fixed docstring issues, and expanded contact column mappings for zip and email.\n",
      "The connector supports Airmeet's Event Details API but lacks certain data verifications and doesn't implement Manage Registrations or Events APIs.\n",
      "This pull request introduces methods to add a phone number to a user, create/update custom event fields, and interact with the high-level event search API.\n",
      "Add an optional `campaignId` parameter to the `apply_response()` method in Parsons to sync survey responses with VAN myVoters campaign IDs.\n",
      "This pull request adds new methods and unit tests for updating phone, order user detail, and importing action in ActionKit, and fixes documentation for `update_user()`.\n",
      "Fixed issue where events were not associated with correct locations due to incorrect list within tuple definition.\n",
      "Tests using a flawed method need rewriting for accuracy and Python 3.12 compatibility, impacting twilio, salesforce tests, and `called_with()` documentation.\n",
      "The bug report states that `called_with` on MagicMock improperly returns truthy, leading to tests that always pass, and documentation incorrectly supports this usage.\n",
      "The issue involves updating dependencies for installation, adding Setuptools due to PEP 632, supporting Python 3.12 in workflows and setup.py, and fixing syntax-related test errors for Python 3.12.\n",
      "Summary: This text references issue #1021 in the GitHub repository.\n",
      "A suggestion is offered to resolve issue #1028.\n",
      "A developer acknowledges using dict.merge() incorrectly instead of dict.update() in the last update.\n",
      "Python 3.8 is nearing end-of-life in October 2024, causing some dependencies like Civis to drop support for it.\n",
      "Added an optional parameter to the NGP email endpoint for disaggregating A/B test results, with code cleanup and new tests.\n",
      "Reverts the changes from pull request #1029 in the move-coop/parsons repository.\n",
      "Request to add a sample script demonstrating how to print a list in the documentation.\n",
      "The author suggests avoiding rewriting a CSV for a non-transformed petl loaded from disk, to save time with large CSVs.\n",
      "Dependabot updated azure-storage-blob from 12.3.2 to 12.13.0, including changes like API feedback, fixing tests, and adding progress hooks.\n",
      "The GitHub issue details required fixes in the `test_google_sheets.py` file such as re-enabling `tearDown`, adding `time.sleep` to avoid rate limits, and fixing broken tests with better resource access.\n",
      "Proposal to add an optional parameter to `ngpvan.email` to prevent A/B test result aggregation in `get_email_stats` method.\n",
      "The GoogleAdmin connector's methods don't return data, instead giving a 'Bad Request' error, but they fail to recognize it and return empty tables.\n",
      "Added _post function for connector and create ticket function for Freshdesk integration; seeking feedback/questions from @shaunagm.\n",
      "Updated the 'requests' library from 2.31.0 to 2.32.0, addressing a Session verification issue, performance improvements, and packaging changes.\n",
      "Fixed a pagination bug in GET function, corrected phone number format in get_profiles, and enhanced create_profiles to also update records with custom column data.\n",
      "This PR updates the braintree dependency to the earliest non-yanked version (4.17.1) due to critical bugs, addressing deprecation warnings later.\n",
      "The issue requests making the linting output more verbose for better error identification.\n",
      "Request to add `format` parameter to Redshift unload function with CSV, JSON, and PARQUET support, noting format-specific limitations and unsupported options.\n",
      "Upgraded psycopg2 to version 2.9.9 to fix failing `macos-latest` CI test for Python 3.11 as earlier version wasn't officially supported.\n",
      "The author plans to update the version to align with the Prefect requirements file to resolve a conflict related to SQLAlchemy.\n",
      "Fix proposed for breaking tests hindering PR merges.\n",
      "Installation of Parsons on Python 3.11 fails due to PyYAML dependency issues; upgrading civis library may resolve it.\n",
      "The author is testing whether the `macos-latest-large` runner functions correctly, suspecting it's an M1 machine.\n",
      "The first PR revealed that the project's contributor guidelines are inconsistent between the website guide and `CONTRIBUTING.md`.\n",
      "The author created a new method `paste_data_in_sheet` to address issue #1036 and seeks advice on refining the code for merging, noting untested sandbox functionality.\n",
      "A GitHub user requests additional team members to be looped in for maintaining the Parsons project.\n",
      "Added a function to post for a connector and a function to create Freshdesk tickets, also includes changes from PR #1025; seeks feedback or issues on including those changes.\n",
      "Introduced OAuth2 authentication for the connector as an alternative to using a service account.\n",
      "The issue notes the inability to export query results directly to GCS, contrasting with Redshift's feature of only allowing table extractions.\n",
      "The issue discusses passing credentials directly to GCP connectors to resolve overwrites when multiple clients are used, along with a checklist for fixing and testing each connector.\n",
      "When initializing multiple Google connectors in Parsons, each overwrites the same environment variable, preventing the use of different credentials.\n",
      "The `match_glob` argument in `google.storage.Client().list_blobs()` is invalid and it's unclear when it was deprecated.\n",
      "@matthewkrausse requested quotes feature following a conversation with @austinweisgrau in Slack.\n",
      "Request to add offset rows and columns parameters in `GoogleSheets.append_to_sheet` to facilitate data placement starting at specific spreadsheet positions.\n",
      "Upgraded to black version 24, resulting in formatting changes applied to all files for compliance as per commit 44cacb9.\n",
      "The issue addresses removing the Bluelink connector due to the platform's retirement in March 2024.\n",
      "The BigQuery connector syntax error occurs when parameterizing an `in` statement, unlike with Redshift.\n",
      "A fix has been implemented for a missed failure in #976, but the fix-copy-s3 branch isn't running linting or tests due to outdated CI workflow.\n",
      "The author writes their first tests, which passed, but was unable to run docs to check if new functions were documented.\n",
      "The issue is that an except block may raise a \"variable not defined\" exception if the variable initialization fails beforehand.\n",
      "New DBSync feature supports appending or upserting rows by comparing updated_at_column values between source and target databases.\n",
      "The RECORD type fails with dict values in Parsons table; using JSON type instead successfully loads New/Mode data.\n",
      "The issue discusses reverting the changes made in move-coop/parsons#1026.\n",
      "The author requests the implementation of mapping a list to an ARRAY data type.\n",
      "Added functions for new ActionNetwork's API Unique ID Lists routes and a utility for querying the SQL mirror database using SSHTunnelUtility and Redshift connector.\n",
      "The issue requests returning the response after using `update_user` to parse the status code for an AK user.\n",
      "The `GoogleBigQuery.upsert()` function fails when column order differs between the Parsons table and the database table, causing data insertion errors or type mismatch failures.\n",
      "This GitHub issue details the update of the `black` formatting tool from version 22.12.0 to 24.3.0, addressing a CVE vulnerability and various bugs related to comment handling and f-strings.\n",
      "Update Parsons for Python 3.12 compatibility; Python 3.11 compatibility PR #1012 pending, low priority.\n",
      "Fixed workflow errors from issue #827 by updating quote formatting and adjusting line wrapping, and cherry-picked commit to pass doc build.\n",
      "This summary addresses a bug fix for an `AssertionError` in workflow #976 where the wrong method call check caused test failure, also including a cherry-pick to pass doc build.\n",
      "The issue describes a bug where a query to drop and create a table in BigQuery fails, but is temporarily resolved by adding a select statement at the end.\n",
      "Cherry-picked commit for doc build pass; branch needs basic linting fixes as per discussion in #845.\n",
      "The author suggests using 'uv' for faster macOS testing within Python package workflows, while acknowledging its limitations like lack of support for Python 3.7.\n",
      "The author suggests using the new standard for config in a file, grouping packages with an alias, and addresses potential inconsistencies with LIMITED_DEPENDENCIES.\n",
      "This issue suggests consolidating action workflows into a single matrix for Linux, with additional macOS-specific tests included separately.\n",
      "The author suggests unifying tooling configurations for consistency, updates tools to latest versions, and is open to changing the line length setting.\n",
      "The author has compiled minor bugfixes for tests and lints into one place, resolving the last issue blocking support for 3.11.\n",
      "Developer suggests starting with Conversations in integrating OpenField with Parsons, noting State Voices' use and growing interest in OpenField.\n",
      "Support for loading data using the Python `Decimal` class into BigQuery tables has been added.\n",
      "The author improved the GitHub actions runtime, updated packages and workflows, fixed code issues, but dropped support for Python 3.7.\n",
      "The `GoogleBigQuery.get_columns()` method causes a \"400 Invalid project ID\" error due to a faulty `{self.project}` reference.\n",
      "Parsons 3.1.0 installation fails due to a dependency on a yanked braintree 4.0.0 version, which contains critical bugs.\n",
      "The Pipelines project seeks community input on data orchestration tools to design an abstraction allowing integration with various platforms, moving beyond the current Prefect-only setup.\n",
      "The Pipelines project should be accessible for new Python users, enabling them to easily perform ETL tasks and use data orchestration features without needing to write or extend the framework.\n",
      "This pull request introduces an email endpoint to VANConnector for retrieving email messages and stats, including relevant test cases.\n",
      "This pull request introduces email retrieval and statistics via a new `Email` class in the VAN connector, complete with tests and documentation.\n",
      "This pull request introduces `head` and `tail` methods to `Parsons Table` in the `ETL` module, including tests and documentation.\n",
      "The BigQuery connector no longer raises an AttributeError when creating tables or views, making the return_value=False flag unnecessary.\n",
      "A bug occurs when creating a view with BigQuery, causing an AttributeError due to an unexpected NoneType object.\n",
      "This pull request adds a warning on the performance impacts of using `get_row_count` with `SELECT COUNT(*)` in BigQuery for large tables.\n",
      "The issue highlights the need to raise an error in Big Query's `.copy()` method when `GCS_TEMP_BUCKET` or `tmp_gcs_bucket` is not provided.\n",
      "A fix addressing issue #944.\n",
      "The issue reports that using `SELECT COUNT(*)` for row counts is inefficient compared to querying `INFORMATION_SCHEMA.TABLES`, suggesting a change due to performance concerns.\n",
      "Fixes the upsert bug in core methods by including \"template_table\" in the API and refactors schema logic into a private function, requests further integration testing.\n",
      "The BigQuery connector's `upsert` method in Parsons is failing due to an incorrect `template_table` argument being passed.\n",
      "Updated grpcio from version 1.53.0 to 1.53.2, including bug fixes and improvements related to server file descriptor exhaustion and security vulnerabilities (CVE-2023-32731, CVE-2023-32732).\n",
      "Request to implement a 'head' function, like petl's, in Parsons to preview the first X rows of a table for testing purposes.\n",
      "Using download_saved_list() from Parsons only retrieves 18 out of 25 expected records from VAN, using Ubuntu and VS Code, priority medium.\n",
      "Psycopg3 introduces the ability to cancel running queries with Ctrl-C, but some psycopg2 parameterization patterns and `copy` methods have breaking changes.\n",
      "GitHub suggests using their caching dependencies workflow, but acknowledges the current setup is functional and doesn't require immediate change.\n",
      "Request for progress updates feature to monitor SFTP file uploads and identify hanging issues.\n",
      "Allows `Table(None)` to function like `Table()`, potentially breaking code that expects a ValueError.\n",
      "Refactored Parsons table initialization to avoid iteration over rows for performance, with a sentinel default argument to maintain existing behavior, planning a separate PR for potential breaking changes.\n",
      "This PR adds a configurable timeout to the `load_job.result()` call in [Issue #910](https://github.com/move-coop/parsons/issues/910) to prevent indefinite hanging and handles `DeadlineExceeded` errors.\n",
      "This PR introduces a new Pipelines API to simplify ETL script creation in Parsons, with feedback-driven syntax simplifications, Prefect integration for data orchestration, and a focus on usability for both new and experienced users.\n",
      "Summary: Issue encountered with bulk uploads in ActionKit using a CSV file with invalid rows resulting in fewer errors reported than expected.\n",
      "Fixed a typo in index.html and updated the draft census connector after linting and re-running tests.\n",
      "Charlie has created a draft census connector with unit tests and documentation and is seeking feedback.\n",
      "While migrating, a function was generalized to use the Transfer Service for copying files, but one reference to the old method was overlooked; manual testing and a unit test are needed post-Feb 5th release.\n",
      "Upgrading Sphinx to the latest 5.x version and corresponding sphinx-rtd-theme to fix CI failures, with potential adjustments for myst-parser.\n",
      "This PR fixes suppressed exceptions in the `GoogleBigQuery` connector by ensuring `LoadJobConfig` object is returned when decompressing flat files.\n",
      "Fixes `get_columns_list()` to handle table names with spaces or leading invalid characters.\n",
      "The update aligns the behavior of an empty table with columns defined to return `None`, consistent with the Redshift connector, addressing issue #971.\n",
      "The BigQuery connector returns None for queries with no rows, unlike the Redshift connector which returns a table with column names, making downstream use easier.\n",
      "The BigQuery client initialization emits a warning due to a missing `google-cloud-bigquery-storage` dependency, which is not required by parsons.\n",
      "Enhancement request: Implement a `client_options` parameter in the client function for specifying default scopes and other options needed to query external tables in Google Drive.\n",
      "Enhances list_blobs to include additional file details for improved filtering capabilities such as by size.\n",
      "Dependabot updates jinja2 from 3.0.2 to 3.1.3, addressing a security issue for users of `xmlattr` with user input as attribute keys.\n",
      "Parsons needs compatibility with Pandas since it requires python-dateutil 2.8.2 and v3.0.0 is upcoming.\n",
      "This PR introduces a `deactivate_connection()` method and updates `upsert_connection()` to set the `inactive` flag to `False` by default, with tests confirming the functionality.\n",
      "An issue about an argument mismatch in the match function needs fixing.\n",
      "This update increases email data configurability in EveryAction upserts, enabling unsubscribed defaults, while maintaining backward compatibility.\n",
      "Dependabot updated paramiko from 2.11.0 to 3.4.0 with rebase option available for merging the PR.\n",
      "Added recursive re-attempt and multi-gsheet combine functions to parsons; unit tests have an unresolved import issue.\n",
      "The author introduces two SQL helper functions for querying and deduplication and seeks help with unrelated failing Gmail unit tests.\n",
      "Fixed issues with BigQuery.copy() method, improved type mapping and schema inference, resolved timezone-related datetime parsing, and optimized schema handling for existing tables.\n",
      "The proposed change makes the BigQuery connector use the first non-`NoneType` from `petl.typecheck()` and includes a test for parsing columns with nulls.\n",
      "The author requests the addition of a Google Slides Connector to Parsons to assist with automating slides population, providing a code example.\n",
      "Issue with `petl.typeset` not including the source module in type name, leading to ambiguous type identification like `date` instead of `datetime.date`.\n",
      "The `rename_columns()` method is missing from the `Table` class in Parsons version 1.0.0, causing an AttributeError.\n",
      "Tests in the Parsons project are failing due to a missing 'mocker' pytest fixture for various test functions leading to setup errors.\n",
      "The author believes the changes are minimal and the project is ready to proceed.\n",
      "The issue highlights inconsistencies between dependencies in `requirements.txt` and `setup.py`, impacting tests and installations with `PARSONS_LIMITED_DEPENDENCIES`.\n",
      "Request for a new Parsons method in the Airtable class to support `performUpsert` with `fieldsToMergeOn` arguments for efficiency.\n",
      "This issue suggests adding a `major-release` branch to the GitHub workflow for running tests and code style checks with `black` and `flake8`.\n",
      "This PR fixes failing unit tests for new GCP connectors before a major release.\n",
      "Updated BigQuery client in Parsons to enable query parameterization and corrected the corresponding documentation.\n",
      "The issue highlights that the import requires the inclusion of the `google-cloud-storage-transfer` dependency to function properly.\n",
      "This PR duplicates issue #876 to revert a commit that was mistakenly included in #873 during a merge with the major-release branch.\n",
      "The Parsons PyPi page lacks a description, which appears unprofessional; the author suggests using the GitHub README.md as the PyPi description by updating `setup.py`.\n",
      "Commit 766cfae enables boolean parsing by default when copying a table to a database, with comments on disabling it, partly addressing issue #942.\n",
      "The `parsons.databases.database.DatabaseCreateStatement.detect_data_type()` method needs enhancement to detect and infer more data types such as datetimes, dates, and booleans for efficient table creation.\n",
      "Propose Redshift copy method in `parsons.databases.redshift` to infer column types from source table like `parsons.google.bigquery.BigQuery.copy()` does.\n",
      "The author has implemented Round 2 of functions including POST, PUT, and DELETE for taggings and seeks feedback.\n",
      "Tests don't run on the major-release branch, causing potential delays and context loss during the release process.\n",
      "Merged main into major-release following @Jason94's advice; the branch should now be clean.\n",
      "The `gcs.unzip_blob()` is being updated to support compression formats beyond `gzip`.\n",
      "This PR adds the ability for users to specify a destination path for nested sub-bucket storage in `gcs -> gcs` transfers.\n",
      "The author is developing a Python library for the Legiscan API and seeks collaborators to help complete it. (Priority: low)\n",
      "This PR introduces a connector for the IDRT library in Parsons for matching duplicate contacts in large databases, which lacks a simple match function for Parsons tables and model training capabilities, and modifies the dependency mechanism.\n",
      "The author requests the addition of quotes to create consistency with the Table.to_redshift() function.\n",
      "The README.md and PyPI information on Python version support for Parsons are inconsistent; clarity is needed.\n",
      "Fixed BigQuery query parameterization issue and updated documentation for correct `client.query()` usage.\n",
      "This issue discusses a change to the object that is retrieved when an error is raised.\n",
      "The BigQuery client `query()` method's documentation erroneously copies from `parsons.Redshift.query()` and lacks proper parametrization examples.\n",
      "Added a `format_phone_number` method to format phone numbers in E.164, with tests for correctness and various scenarios.\n",
      "This pull request addresses a bug in the `apply_canvass_result` function by ensuring the phone is sent for all required contact types and raises an error otherwise.\n",
      "Added `get_representative_info_by_address` method to retrieve US representative data with validation checks, error handling, and comprehensive tests.\n",
      "The `apply_canvas_result()` method in Parsons is failing with a 'phone required' error for certain contact types even when a phone number is provided.\n",
      "Issue: The author highlights the need for the API to return a response or status code to enable error checking.\n",
      "This pull request adds a `rename_columns` method to efficiently rename multiple columns in a Parsons Table using a dictionary.\n",
      "PR adds a QuickBooks connector to Parsons for pulling users, groups, jobcodes, timesheets, and schedules data.\n",
      "Add BigQuery feature to export query results to a file in GCS, enhancing workflow parity with Redshift's \"unload\" method; medium priority.\n",
      "This PR updates import statements in the `major-release` branch to match the new BigQuery module path and updates `discover_database` unit tests accordingly.\n",
      "This pull request adds a `rename_columns` method with tests for efficient multiple column renaming in the ETL process using Parsons Table.\n",
      "Implemented optional authorization kwargs in oauth_api_connector for broader OAuth2 support; needs testing with Catalist Match, Zoom, and Controlshift.\n",
      "PR enhances `get_messages` in `action_network.py` by adding `unpack_statistics` parameter for detailed statistics in Parsons Table.\n",
      "The author proposes enhancing the NGPVAN API's `upsert_person` method for simplicity, allowing more fields to be added with fewer lines of code.\n",
      "Added type check for dict in email addresses and updated documentation to clarify acceptable argument types and correct dict keys.\n",
      "Updated job config logic to prioritize explicitly passed-in schema over `autodetect=True`.\n",
      "Proposal to add `get_row_count` function to BigQuery connector, discussing if separate functions for tables and views are needed.\n",
      "This issue details the implementation of a connector for the Catalist Match API, which requires OAuth 2.0 and IP whitelisting, along with changes to error handling and added testing dependencies in parsons.\n",
      "Added two new column helpers to the BigQuery connector via a quick pull request.\n",
      "Propose adding a configurable default timeout for BigQuery queries in Parsons to enhance robustness and follow best practices.\n",
      "Implement a cost-efficient `get_columns_list()` equivalent for BigQuery, avoiding direct table queries, with high priority.\n",
      "Request for a function to transfer BigQuery tables across projects, potentially using `bq cp` command, with medium priority.\n",
      "The PR fixes `airtable.batch_insert` by allowing both list and table inputs, resolving TypeError and AttributeError exceptions related to Parsons table insertions.\n",
      "The PR fixes `airtable.batch_insert` to accept either Parsons tables or lists, resolving `TypeError` and `AttributeError` during usage.\n",
      "Proposing the addition of `get_messages()` and `get_message()` methods for complex email analysis outside of Action Network, with medium priority.\n",
      "Dependency update: Bumps urllib3 from 1.26.17 to 1.26.18 with important changes for handling HTTP 303 redirects.\n",
      "First-time contributor implemented a new API call in the Hustle module as per the docs, tagged as a good first issue.\n",
      "This connector integrates [Empower](https://empowerproject.us/empower-app/) app support, handling all JSON objects in their API.\n",
      "Upgrades urllib3 from 1.26.5 to 1.26.17, addressing security issues, thread-safety, socket timeout fixes, and deprecated methods.\n",
      "The author has added getter functions to the ActionNetwork module and plans to add create and update functions pending approval.\n",
      "User reports that setting `notify = False` in `share_spreadsheet()` still results in receiving a share notification email, contradicting expected behavior.\n",
      "New Zoom poll code crashes when retrieving results from meetings or webinars with no poll data due to unhandled empty data scenarios.\n",
      "A first-time contributor is open to feedback on their initial class addition and encourages further expansion with more methods.\n",
      "The author suggests that the current methods are limited but provide a foundation for future expansion.\n",
      "A developer encounters a `TypeError` when running the `pytest -rf test/` command for unit tests, specifically in `test_etl.py`, and asks if changes can be made despite the failing test.\n",
      "This commit resolves issue #816 and includes a test for the bug when a single field is passed as a string in the `fields` argument.\n",
      "This PR adds deprecation warnings for `as_user` in `chat.postMessage`, and supports new authorship args `username`, `icon_url`, `icon_emoji`.\n",
      "The person signup helper endpoint lacks a feature to remove tags, requiring specific argument combinations for deletion.\n",
      "Fixed edge case where ActionNetwork API fails to return a person_id and improved type checking to be more Pythonic.\n",
      "The issue reports inconsistency in Action Builder's API regarding the required key for entity names, affecting error handling in `insert_entity_record()`.\n",
      "This PR adds multiple functions to the `Zoom` connector for retrieving metadata and results of polls from meetings and webinars and requests testing.\n",
      "The example JSON incorrectly uses `SurveyResponse` for the `action` key when it should be `type`.\n",
      "Developer experiences CSRF Token error after 29 consecutive calls using `get_outreach()` in Parsons, but not when using Canales NewMode code.\n",
      "Issue: A change is requested to update the version number in `setup.py`.\n",
      "Issue discusses merging the main branch into the `major-release` branch before deployment.\n",
      "The author is testing a header addition to the YAML config to fix the issue with the autogeneration of release notes not working.\n",
      "Issue: Request to automate the generation of raw release notes using GitHub's configuration YAML.\n",
      "This PR changes behavior to throw an exception when Auth0 returns a 429 too many requests error due to excessive record retrieval.\n",
      "Reverted PR #861 due to severe consequences of using non-unique identifiers in ActionNetwork, as strongly warned by AN support.\n",
      "The enhancements extend the BigQuery database connector, update the DB Connector class, and relocate BigQuery to the databases folder to facilitate migration from Redshift to BigQuery.\n",
      "Request for new functions to retrieve all users, block users, and get connection IDs.\n",
      "This PR updates the Zoom connector for Server to Server OAuth replacing JWT before its deprecation and adds API functions for Zoom poll queries.\n",
      "This PR enables specifying file extensions when using Redshift.unload() to create appropriately named files on S3.\n",
      "The issue discusses functions related to email blacklisting and user data deletion within the system.\n",
      "Proposing a way to pass parameters to `Redshift.unload()` to prevent SQL injection, contingent on upstream fixes or driver migration.\n",
      "TMC user requests the inclusion of PyPika in their Docker image.\n",
      "Renaming `api_key` to `personal_access_token` due to upcoming Airtable API keys deprecation in favor of personal access tokens.\n",
      "Added @cmdelrio's helper functions to the `Events()` object for post-authentication use in the `PDI()` connector.\n",
      "The README has been updated with new installation and setup instructions, addressing issue #863.\n",
      "Implement an automatic pre-commit setup to run black and flake8, reducing failed checks in pull requests.\n",
      "Propose a more predictable Docker image tagging system, with options for CI/CD implementation and tag configurations, amidst concerns of unstable 'latest' images.\n",
      "The issue describes a limitation where `upsert_person()` does not support `identifiers` as `**kwargs` for matching or creating contacts, alongside minor `get_tags()` readability improvements.\n",
      "The issue involves adding Windows testing to the Parsons test suite, debugging the resulting failures, fixing problems, and submitting a PR.\n",
      "Added methods for retrieving and updating `orderrecurring` objects in ActionKit, including tests.\n",
      "Attempted to migrate from CircleCI to GitHub Actions, involving test setup, caching, moving a Windows error to a new issue, checking usage limits, and ensuring documentation builds on CircleCI.\n",
      "This PR introduces a `GET` request for ALL targets in the NewMode client, offering Parsons users expanded API query capabilities.\n",
      "The author proposes adding a SQLite database connector to Parsons, highlighting its benefits and use cases for complex or multi-table analyses.\n",
      "The update adds an optional `remove_tags` field to the `upsert_person` method in the actionnetwork API.\n",
      "The issue suggests adding an optional argument to enable ActionNetwork's background processing feature, as per their documentation.\n",
      "Dependabot updates `grpcio` from 1.51.1 to 1.53.0, including various refinements, improvements, and bug fixes across several languages and platforms.\n",
      "This PR updates docs to link to the website for instructions, leaving the original pages temporarily, with plans to delete them later and move the testing guide when ready.\n",
      "The proposal adds a `get_targets()` function to the Newmode class to reduce redundancy and enable direct querying of targets.\n",
      "Zoom's JWT authentication will be deprecated by September 1st, 2023, affecting data syncs that require updating to OAuth.\n",
      "This PR introduces a Nation Builder connector with `get_people`, `update_person`, and `upsert_person` methods, refined for Python 3.7.17 compatibility.\n",
      "The author reverted PR #837 due to an unexpected error caused by type hints.\n",
      "Request to add JSON as a supported file type for the `copy_s3` function, noting no expected downstream issues due to Parsons table conversion.\n",
      "The ActionNetwork Connector should explicitly require users to define custom fields to prevent implicit updates and future conflicts, but this change is not backward-compatible.\n",
      "This PR adds a `deduplicate` method to `parsons/etl/etl.py` with optional keys and sorting, updates documentation, and provides unit tests, but notes potential confusion regarding the `sort` parameter terminology.\n",
      "This new feature allows for execution of dbt commands from Python with logging support and an option to send formatted results to Slack.\n",
      "The issue lists desired features for non-RedShift connectors, such as BigQuery, with priorities from high to low for enhancing feature parity.\n",
      "This PR introduces a new Nation Builder connector with methods to fetch, update, and upsert person records, including robust error handling.\n",
      "The issue suggests updating the documentation for helper code in `parsons.utilities`, reorganizing the utilities folder, and possibly renaming `etl` to `table` for clarity.\n",
      "The issue discusses improving the readability and maintainability of connector tests by potentially utilizing tools like Faker or Json Schemas and considering dependency management.\n",
      "The issue discusses reverting the changes introduced in move-coop/parsons#819.\n",
      "An error in metadata generation when installing parsons from GitHub was caused by invalid `~=` syntax in `setup.py`, which was fixed by changing to `<2`.\n",
      "The issue suggests adding support for the Python `len` function to obtain the number of rows in a `Table` by calling `Table.num_rows`.\n",
      "The code has been updated to reform nested dicts from Salesforce into a format similar to the method 'describe_fields' using the json library.\n",
      "Added `upsert_user` function to the Auth0 connector for improved user management.\n",
      "Initial submission of a minimal Action Builder class for template sync in the TMC environment, needing further development.\n",
      "Installing `parsons[slack]` installs slackclient 2.9.4, causing `ModuleNotFoundError` due to a version mismatch; this PR aligns versions in `setup.py`.\n",
      "Updated `requests` library from version 2.25.1 to 2.31.0 to address security risks related to `Proxy-Authorization` headers and support urllib3 2.0.\n",
      "Added `Contacts` -> `Suppressions` bulk import mapping type, requires API key enablement via support request.\n",
      "Extend the `to_s3_csv` fix to handle `InvalidToken` error during S3 uploads in Lambda production environments.\n",
      "The issue highlights the need to update the client for new endpoint access and remove an outdated PyJWT dependency that may conflict with future connectors like DocuSign.\n",
      "Request to add a deduplication feature to Parsons table to improve efficiency and avoid converting to Pandas dataframe.\n",
      "The issue reports a bug related to rendering that closes #818 when resolved.\n",
      "The issue describes a failure with `rs.upsert()` when using `alter_table_cascade=True` due to multiple levels of dependent views, suggesting an addition of CASCADE to the code.\n",
      "Adding a column with a function that may raise an exception in Parsons doesn't fail upfront, causing subsequent operations on that column to throw the initial exception due to lazy evaluation.\n",
      "Using `get_records(field='ID')` in Parsons incorrectly splits 'ID' into 'I' and 'D', but `get_records(field=['ID'])` works; a fix or doc update is needed.\n",
      "This PR introduces an `ABC, DatabaseConnector` interface for shared methods across database connectors and documents existing behavior, with minor cleanup and a change to Postgres' `strict_length` default.\n",
      "The author requests to synchronize the major-release staging branch with the latest changes from the main branch.\n",
      "This PR removes documentation for a non-existent `view` parameter in the MySql class.\n",
      "Petl's `to_csv` and `from_csv` functions need an option to handle booleans and possibly translate int/float values, not just strings.\n",
      "The PR fixes hardcoding of the version number in Parsons docs and updates the repository location in the setup script.\n",
      "The issue suggests enhancements in documentation and error handling mechanisms.\n",
      "\n",
      "The issue discusses implementing conditional logic to bypass prefix alteration for route handlers when a file extension is provided, awaiting input from @KasiaHinkson.\n",
      "The `parsons.Redshift().unload()` method needs a feature to support unloading data in Parquet format, which is more efficient than CSV.\n",
      "Fixed issue where update failed to consider `None` as the default for `prefix` and now ensures trailing `/` only if prefix is supplied.\n",
      "Propose the addition of a bulk deletion function to the VAN/EA API to save costs and easily remove individuals.\n",
      "Redshift.duplicate_table()'s 'drop' option causes errors with dependent views; suggest adding new options or updating docs with workarounds.\n",
      "TargetSmart SmartMatch code exists in the repo but is missing from the documentation page.\n",
      "This PR adds a temporary fix for Issue #784 by checking `prefix/` in `s3.list_keys()`, alerting users about rejected calls due to missing prefix inputs.\n",
      "Proposed addition of `get_data_source` and `update_data_source` methods to the Redash connector.\n",
      "Request to add an access_token authentication option for Shopify and address the requirement for importing Shopify refunds with Actionkit orders/transactions.\n",
      "The summary highlights an internal code review and discussion within MoveOnOrg's Parsons project, specifically PR #41.\n",
      "Issue discussing internal MoveOn code reviews and providing context with links to two related pull requests on the Parsons repository.\n",
      "The issue requests the ability to pass an AWS session token with access and secret keys for the `to_redshift()` method, either via environment variables or `copy_args`.\n",
      "This commit adds support for passing AWS session tokens for authentication in `copy_args` and through environment variables, with a new test for validation.\n",
      "Issue addresses resolution of problem identified in issue #791 in the move-coop/parsons repository.\n",
      "Issue #791 is marked as resolved.\n",
      "Package conflicts are occurring when setting up Mage.AI with ActionKit due to incompatible versions of cryptography, jinja2, PyJWT, python-dateutil, requests, and SQLAlchemy as required by parsons 1.0.0.\n",
      "Request for a Parsons connector to facilitate easier access to census data, starting with the American Community Survey; low priority.\n",
      "Contributors find manual linting time-consuming; proposal to switch to Black for easier linting, despite needing a massive, initial PR due to its strict style.\n",
      "Add a test step to install Parsons on various OSs and versions to improve installation reliability; consider polling for popular systems.\n",
      "Airtable API keys will be deprecated after Feb 1, 2024; transition to OAuth or Personal Access Tokens is necessary.\n",
      "Addresses issue 780 and 781 related to Parsons project bugs.\n",
      "Rotated SSH keys for Parsons project and updated docs key fingerprint in CircleCI config.yml per security recommendations.\n",
      "S3 utilities in Parsons need adjustments for proper handling of subfolders with distinct permissions within S3 buckets.\n",
      "Request to add a user-configurable polling interval parameter to the `match()` function in TargetSmart Automation to reduce the frequency of SFTP checks.\n",
      "Miscellaneous documentation improvements were made, including fixing issue #671, adding a Scytl connector to the sidebar, updating the Parsons logo, and reorganizing the homepage for readability with a link to the installation guide.\n",
      "The instructions for creating a service account for Google Sheets integration lack information on sharing the account with relevant Sheets or Drive folders, causing user confusion.\n",
      "The issue points out that the description mentions creating a spreadsheet from a Parsons table, but the code doesn't reference a Parsons table, which is misleading and needs high-priority attention.\n",
      "User encounters segmentation fault in tests on macOS Ventura due to `pyarrow==7.0.0` installed by `google-cloud-bigquery==3.0.1`; upgrading to `google-cloud-bigquery==3.4.0` resolves the issue.\n",
      "This PR implements a new `merge_contacts` function in the VAN class and adds corresponding tests. #772\n",
      "Added two functions to ActionKit connector, a parameter to S3 initializer, and introduced ContactNotes class to NGPVan.\n",
      "Proposed new class for handling AP Elections API updates for fields and URL, and a script for fetching election data with specific parameters, noting TMC's use of Postgres.\n",
      "A new Donorbox connector with endpoints detailed in the provided link has been added, addressing issue #718, including tests and documentation.\n",
      "Add a method to the VAN class in Parsons to merge contact records, a feature now available in the VAN API, to save users time and money.\n",
      "The 'create leads' function fails to load 'State' as a custom field, although it should, as shown in the provided screenshot.\n",
      "Fixed a bug preventing list overwrites and corrected a typo in the logging code.\n",
      "The author suggests fixing current tests to improve their relevancy and effectiveness.\n",
      "This PR merges work by @AndrewRook due to CI configuration issue.\n",
      "\n",
      "Issue aims to address CI configuration problems that hindered testing for issues #753 and #761.\n",
      "Upgrades joblib from 0.14.1 to 1.2.0, fixing a security issue and several bugs, and improving support for PyPy and multiprocessing.\n",
      "The tests failed due to incorrect zip file request mocks, urllib.request bypassing mocks, and Scytl's non-browser request block; adding a UserAgent header should fix it.\n",
      "A new attempt to fix CI/CD, superseding issue #753.\n",
      "Merging to main caused outgoing requests to fail with a \"Forbidden\" response; considering migrating to the requests library to resolve the issue.\n",
      "The issue concerns a flawed comparison in `utils.py` that only checks `dict` headers instead of full contents, leading to a false test pass in `test_google_admin.py`.\n",
      "The get_records method in Parsons v0.21.0 does not return columns with all null values in the resulting table.\n",
      "This PR aims to integrate a new connector class to Parsons for the Strive SMS tool but requires fixing GET requests, adding unit tests, and creating documentation.\n",
      "Changed contact email from \"justin@movementcooperative.org\" to \"hr@movementcooperative.org\" with additional formatting adjustments.\n",
      "The `van.upload_scores()` function causes memory issues when submitting full state scores, suggesting a file input might be better.\n",
      "Improved ActionKit, Braintree, and S3 connectors in Congregate.\n",
      "Using `@wraps` on the inner function of `_wrap_method` propagates docstrings, which tested successfully without breaking unit tests.\n",
      "Fix proposed for CircleCI config causing false negatives on new PR builds.\n",
      "The author proposes reverse engineering an npm package to access the NGPVAN API swagger spec for automated Python API client generation, highlighting benefits and potential tradeoffs.\n",
      "Propose adding central retry logic with exponential backoff to the `APIConnector` class in the Parsons project refactor.\n",
      "This is a fix for issue 707 in the move-coop/parsons repository.\n",
      "Propose adding a `ParsonsHandler` for Python's logging module to automatically write logs to a database, enhancing reliability and reducing code duplication.\n",
      "Unable to reproduce issue described in #685 through written unit tests.\n",
      "Issue report states that custom setting of quotes was limited to AWSConnection, which cannot be passed to S3 objects.\n",
      "Parsons is incompatible with Python 3.10 due to changes in the Collections library affecting dependencies like urllib3, botocore, and gspread on M1 Macs.\n",
      "Public methods of the GitHub connector are not documented due to potential interference from the `wrapgithub404` decorator, affecting usability.\n",
      "The user reports an error with Slack API's `as_user` parameter, suggesting removal when set to `False` and advocating for full `resp` exposure.\n",
      "Using Zappa for asynchronous processing with environment variables causes mismatches in session tokens, leading to InvalidTokenErrors, and needs a fix to force `aws_session_token` to None.\n",
      "A new developer encountered installation errors related to `psycopg2` build prerequisites on WSL (Ubuntu) and proposed additional documentation to help.\n",
      "The Redshift connector modification enables directing temporary S3 storage into simulated subfolders for Redshift copy operations.\n",
      "The connector simplifies the process of fetching and formatting complex election result data from Scytl sites into readable lists or tables, with a live polling feature.\n",
      "The `APIConnector` class in Parsons needs a refactor to simplify API handling and pagination, with an implementation plan that minimizes impact on existing connectors.\n",
      "Added simple documentation for the SMTP class, based on Gmail connector docs, requests review for potential missing information; related to issues #703, #717, and closes #635.\n",
      "Deprecation warnings added to update_person and add_person, related to issue #706.\n",
      "This PR fixes issue #714 by updating a connector method to a correct endpoint and preventing stack exceptions with `Table.materialize()` for large results, without refactoring to APIConnector.\n",
      "The issue is that Redshift.copy fails due to unspecified encoding when handling non-ascii characters while writing a temp .csv before transferring to Redshift.\n",
      "PR opened for updating psycopg2-binary to the latest version to resolve numerous issues experienced.\n",
      "The author expresses gratitude to Daniel and offers further clarification if needed.\n",
      "User encountered an error using `geocode_address_batch()` in the Census Geocode API with correct field names, and needs a resolution due to the time-consuming nature of alternative methods.\n",
      "The issue suggests that `parsons.Table.convert_columns_to_str()` should convert `None` to empty string for intuitiveness, despite serialization concerns.\n",
      "The issue describes a mismatch between expected Redshift environment variable names by Parsons and those created by Civis, causing extra code for adaptation.\n",
      "The `van.upload_saved_list_rest()` method fails to overwrite an existing list with the overwrite argument, though manual REST API calls succeed.\n",
      "The change requires explicitly setting the session token to prevent issues with mismatched keys when using Zappa asynchronous processing.\n",
      "Requesting feedback on the new `etl_helpers` folder with reusable code for user transformations and analysis, different from existing 'utilities'.\n",
      "Proposal to add `select_row` method to `Table` class for fetching a single row as a dictionary with discussion on its implementation and behavior.\n",
      "Implement a default error message with a help link across Parsons connectors to encourage users to report issues.\n",
      "PR #717 failed to trigger all CI tests; this PR fixes pep8 compliance issues with no functional changes, requiring a quick review.\n",
      "The utility parameter in Redshift's `copy` and `upsert` functions unnecessarily cascades changes on all calls when true, not just for column widening.\n",
      "Propose adding a Parsons connector to CallHub to facilitate custom integrations, such as managing Do Not Call lists, for various organizations.\n",
      "A TMC member inquired about syncing Donorbox data; a low priority issue has been created in Parsons for class creation.\n",
      "PR transforms Sendmail connector into abstract base class and removes it from global `init` to avoid direct calls; requests review for any missed usages.\n",
      "Fixes documentation for an Action Network connector parameter and a bug preventing the addition of postal addresses using `add_person`.\n",
      "A bug where splitting a Redshift table name over two lines introduced leading whitespace, causing table existence checks and rs.copy() to fail.\n",
      "The Mobilize connector is partly broken due to API changes; a rewrite using `APIConnector()` and endpoint validation is recommended.\n",
      "This PR updates remaining references in the CI config file from Python 3.7 to 3.10, following a move to a newer Docker image.\n",
      "The author is fixing a CI configuration issue missed in #703 to comply with pep8 and restore the main branch to a passing state.\n",
      "The user encountered a bug where string splitting for PEP 8 compliance added leading whitespace to a table name, causing database errors.\n",
      "Attempting to copy or upsert a Parsons table to Redshift with `alter_table=True` fails when varchar exceeds Redshift's max width, a fix is proposed to resize columns and truncate values.\n",
      "Updated the Mobilize connector's base URI to HTTPS and fixed authentication passing and JSON error handling issues.\n",
      "The Mobilize connector documentation lacks a quickstart code example which is present for other connectors like NGPVAN.\n",
      "The `update_person()` function in Parsons 0.19.0 returns the person's data without updating it as intended, despite correct usage.\n",
      "Fixes issue #525 by adding a `phone` parameter to `apply_canvass_result` and ensuring it's provided for `canvassTypeId` `1` or `37` or throws an error.\n",
      "The method incorrectly assumes VAN returns a list of files, not individual targets, and needs updating to reflect the current API behavior.\n",
      "The user added tests for the `SendMail` class to improve coverage and questions if user-facing documentation is needed for the class that behaves like an abstract base class.\n",
      "Implemented the try-imports method for `from parsons import ...` to work without all dependencies; seeks confirmation on \"Table\" callout and will update docs post PyPI release.\n",
      "The author fixed Google authentication issues in Parsons related to environmental variables being mistaken for JSON credentials, and seeks a quick review and release.\n",
      "A commit two years ago introduced a filter logic bug in the `get_person` VAN method, causing 404 errors and display issues with `expand_fields`.\n",
      "The recent PR causing Google authentication issues treats path strings as JSON in `utilities.py`, necessitating a quick review and fix release.\n",
      "The issue report suggests that the Parsons wheel package on pypi ignores `setup.py` configurations due to build environment variables, thereby not respecting the `PARSONS_LIMITED_DEPENDENCIES` flag.\n",
      "PR restores accidentally removed template metadata to fix templates as per issue #668.\n",
      "Two bug fixes involve renaming methods and parameters (`get_code` and `apply_code`) in VANConnector class; future PR may refactor `_supported_entities` parameter for better abstraction.\n",
      "Enhanced `copy` function mapping to handle `NoneType` as 'String' in Python to BigQuery data type conversion for better flexibility with column data types.\n",
      "Contributor submits documentation updates, including lazy loading instructions and materialize methods, to address issue #662, seeks confirmation on placement.\n",
      "An Auth0 connector has been implemented with several methods available.\n",
      "The PR updates pyjwt from 2.0.1 to 2.4.0, adding Python 3.10 support and addressing the CVE-2022-29217 security issue.\n",
      "The `_generate_schema()` method in Parsons fails on NoneType values due to `BIGQUERY_TYPE_MAP` limitations, needing a workaround or process integration.\n",
      "The `query()` method in `GoogleBigQuery` returns `None` for empty tables; a proposed solution involves returning a table with headers only.\n",
      "The `GoogleBigQuery` connector's `copy()` method fails due to repeated credential file handling in `setup_google_application_credentials()`, with a proposed solution to not reprocess a found `.json`.\n",
      "Updated the `parakimo` package to version `2.10.4` to address fix related to issue #683.\n",
      "PR submitted for issue #669, lacking API documentation and assuming continued use of 'phone2action' branding in URLs and environment variables.\n",
      "This fix addresses missing commas in setup.py for the supported Python versions from a recent update.\n",
      "Using `setup_google_application_credentials()` twice with `GoogleBigQuery` and `GoogleCloudStorage` creates a nested .json issue, fix proposed by modifying `if` condition.\n",
      "Updating the Parsons package resulted in a CryptographyDeprecationWarning for Blowfish when loading Redshift in a notebook.\n",
      "PR updates simple-salesforce dependency to 1.11.6, fixing issue #642, but lacks Salesforce account to fully test changes.\n",
      "The author discusses updating the minor version to reflect the changes in active Python version support and to publish a new release on PyPI.\n",
      "The author suggests standardizing the `client` terminology and its instantiation across different connectors in the Parsons project for consistency.\n",
      "The issue suggests adding a Table of Contents to the ReadMe documentation for improved navigation.\n",
      "The issue highlights outdated authentication info in Google Sheets connector documentation, updates made, and questions if similar issues exist in Google Admin, BigQuery, and Cloud Storage docs.\n",
      "The author reports that the `changed_entity` method fails when there's no data for provided dates and suggests returning `None` or an empty table with headers.\n",
      "Updated 'upload_saved_list_rest' to remove previously implemented workaround, addressing issue #513.\n",
      "Implemented TargetSmart SmartMatch list matching and updated existing TargetSmart routines with modernization and documentation for 2022.\n",
      "The function `update_person_json()` does not update a contact's record as intended and might be due to incorrect documentation or a malfunction in the function itself.\n",
      "The \"Create your own sandbox\" section is missing in the online \"How to build a connector\" documentation.\n",
      "Consider opening the TMC-maintained `movementcooperative/parsons` Docker image to the public and documenting its usage in Parsons docs.\n",
      "This PR introduces a script and use case for opting out phone numbers in EveryAction.\n",
      "The issue suggests renaming the Phone2Action connector due to the company's rebranding to Capitol Canary, advising to review their API documentation to confirm changes.\n",
      "This issue addresses a build bug caused by Google dependencies, with further details discussed in the Parsons Slack.\n",
      "The function has a typo (missing comma on line 329) and is unused, suggesting it can be deleted.\n",
      "Developer updates `parsons` dependencies to fix import errors on Python 3.10 but drops Python 3.6 support and limits compatibility to Python 3.7 to 3.10.\n",
      "The ``van.apply_activist_code`` method's default now omits contact responses, addressing issues with non-contact-related activist codes affecting reporting.\n",
      "The `materialize()` and `materialize_to_file()` methods from `table.py` are not documented on the Parsons project website.\n",
      "The NGPVAN connector's current use of a single DB parameter for `MyVoters` or `EveryAction` is outdated and potentially confusing, suggesting a split into two separate connectors.\n",
      "This PR fixes an issue where `make html` fails due to a missing variable that is only set during `make deploy_docs`.\n",
      "This issue suggests implementing DBSync functionality in Parsons for transferring data from Azure to BigQuery or Redshift to benefit users.\n",
      "The issue suggests that quotes should point to the `main` branch instead of `master`.\n",
      "CircleCI currently triggers only on release branch changes; the goal is to adjust it for documentation builds to update 'latest' more frequently.\n",
      "The author fixed the `.gitignore` to stop tracking built docs mistakenly included in Git and untracked the current files, but is uncertain of the outcome.\n",
      "The issue body only contains the phrase \"Fixes #653,\" which implies a pull request or commit is intended to resolve issue #653 in the repository.\n",
      "Improved PDI API with more endpoints covered, token auto-reauthentication, support for over 2000 value retrievals, and updated documentation.\n",
      "Documentation needs clearer authentication guidance, a fix for incorrect `get_people_list` method reference, and correction of a misdisplayed Python code block.\n",
      "Dependabot updates paramiko from 2.7.2 to 2.10.1, fixing a CVE and various other issues, with actions available for rebase or merge.\n",
      "The user is encountering a missing 'bucket' argument error when using `upload_saved_list_rest` in NGPVAN's Saved List class.\n",
      "This is a fix for issue #426.\n",
      "This fix resolves a bug where `get_campaigns` failed for accounts with no campaigns by preventing an attempt to expand a non-existent column, ensuring backward compatibility.\n",
      "A user has made a contribution and is asking @shaunagm for feedback or required changes.\n",
      "Documentation updated to include a comparison between pandas DataFrame and Parsons table.\n",
      "The 'from' methods in Parsons' code documentation contain incorrect links, requiring a careful review and correction.\n",
      "Add staging API URLs for connectors to the \"Connector-specific guidance\" section of the docs, starting with the Mobilize staging URL.\n",
      "Salesforce is retiring old API versions; the Parsons simple-salesforce dependency is outdated and needs updating for compatibility.\n",
      "New contributor implemented a `filter` argument to optimize database syncing and reduce API calls by filtering resources by fields like `modified_date`.\n",
      "Added get_events to improve debugging.\n",
      "The issue describes the initial setup for multiversion documentation using sphinx-multiversion and includes a `deploy_docs.sh` script, with drawbacks such as needing to manually update `DOCUMENTED_VERSIONS`.\n",
      "Update the connector list by adding ActionKit, creating a section for self-service sandboxes, revising the Bluelink intro, and fixing a typo in the Airtable intro.\n",
      "The Slack notification code in the Parsons project is outdated, causing script issues and needs updating to the new `slack_sdk` module.\n",
      "The issue involves adding sandbox account credentials to CI to un-skip existing live tests for connectors and documenting the process for future ease.\n",
      "PR #634 revealed missing documentation for SMTP and Sendmail notifications connectors; consider improving test coverage before adding docs.\n",
      "A PR updates tests for importing directly from Parsons and adds missing connectors to `__init__.py`, including Shopify and others, some of which may be unmaintained.\n",
      "The issue mentions parsons having too many dependencies and problems with AWS Lambda credentials not being read correctly.\n",
      "Proposed a prototype for Parsons to manage dependencies upon installation using an environment variable for more flexibility and avoid dependency conflicts.\n",
      "This PR adds a training guide created during the 'Getting Started With Parsons' Parsons Party event.\n",
      "This PR adds a script for Civis job status alerts to Slack and another for querying MYSQL and saving results to Google Sheets.\n",
      "The issue is concerning the confusing wording in the previous license attribution.\n",
      "The author points out a superfluous addition from PR #581 that seemingly isn't utilized and suggests not tracking it.\n",
      "This PR, co-authored by @neverett, offers guidelines for contributors on accessing and using the project's sandboxes.\n",
      "Proposing to rename `master` branch to `main`, noting that it will require substantial documentation updates.\n",
      "Sunrise data staff request a Parsons connector for Slack to track total members, membership over time, and daily messages per channel.\n",
      "The issue points out that quotes are not linked to the sidebar and are invisible, also noting that Shopify has been moved after SFTP for alphabetical order.\n",
      "Introducing initial documentation for creating use cases and sample scripts, plus a placeholder for future use case files.\n",
      "The Parsons VAN API has inconsistencies in handling snake_case versus camelCase search parameters, particularly for JSON methods like `find_person_json`, which don't auto-wrap or transform data as expected, necessitating either documentation updates or code changes for consistency.\n",
      "The author forgot to include quotes in the earlier Controlshift pull request #598.\n",
      "Updates the documentation to correctly describe the Parsons license.\n",
      "The PR introduces an ActBlue Parsons connector, expands success codes, and fixes a Google Sheets connector bug.\n",
      "The team needs Parsons to support returning data as a Table object or JSON to aid in building custom source connectors for Airbyte.\n",
      "Parsons is not compatible with Google Composer due to conflicts in `pyjwt` and `google-cloud-storage` version requirements, impeding its use in Composer Airflow.\n",
      "This PR enhances Parsons documentation by adding a virtual environments section and detailed Windows installation instructions, with a call for Linux/Mac contributions.\n",
      "This commit resolves the error where `drop_dependencies_for_cols` would fail when running an empty query with no dependent views.\n",
      "The issue suggests configuring CircleCI to skip builds for the auto-generated gh-pages branch, referencing CircleCI documentation for workflows on tags.\n",
      "The documentation updates on the main branch are not synchronized with the PyPi releases, leading to discrepancies.\n",
      "Several new endpoints including calendar events, event activities, contact operations, and event invitations are being added to the PDI class.\n",
      "Replaced the markdown parser with Sphinx's recommended extension and added a CI job to test docs build on PRs, addressing issue #608.\n",
      "The issue is related to an unresolved problem with the `m2r` dependency, as evidenced by the failed build of the docs pages.\n",
      "The issue involves a test failure when using upsert with `alter_table=True` and `alter_table_cascade=False` due to a dependent view's dependency on a column.\n",
      "The `query_records` function in Quickbase now materializes the target table to prevent problems due to Python's recursion depth limits related to lazy loading.\n",
      "The author offers to help develop the connector for Mobile Commons API, which was updated, and is familiar with its API but new to Parsons development.\n",
      "The author suggests adding support for S3 session tokens in the Parsons S3 connector by passing them from environment variables to Boto.\n",
      "Fixed `Table.fill_column` and `Table.fillna_column` by using petl `convert` and `update` functions for in-place updates, also allowing calculated values.\n",
      "The issue requests the creation of a SendGrid connector for transactional emails, referencing documentation for API setup.\n",
      "The README has been updated to indicate the shift from the Parsons Google Group to the more actively used Slack group for discussions.\n",
      "Accidental successful log from rs.copy() when pushing incorrectly formatted CSV; suggests error should be thrown when non-Parsons Table objects used.\n",
      "The Zoom API's pagination changed from `page_number` to `next_page_token`, which necessitates updates to the `_get_request()` method for compatibility.\n",
      "Implemented OAuth2 for the LIST `petitions` endpoint, added placeholders for future methods, and updated flake8, with relevant files linked.\n",
      "The issue highlights a discrepancy where the path to retrieve webinar registrants doesn't include 'report' unlike the path for past participants, against documentation expectations.\n",
      "The `create_signup` logs incorrectly omit the formatted string due to a missing \"f\" prefix in the log message.\n",
      "The author refers to previous work on making Parsons package lightweight enough to run on AWS Lambda, linked in PR #186.\n",
      "Collating Redshift-related improvement suggestions and bug fixes for the Parsons project to address in a group effort.\n",
      "Reverted an ETL column mapper fuzzy matching default due to new bugs in connectors like the hustle connector.\n",
      "Tracking parent issue for validation-related tasks: phone number, email, and address validation.\n",
      "The author is suggesting enhancements to a method's user-friendliness, referencing a discussion in a pull request for context.\n",
      "Reinstates original code by reverting merge from PR #336.\n",
      "This update cherry-picks changes from PR #373 and includes requested handling for primary key and sort key lists.\n",
      "The Salesforce query method returns garbled data due to a change in the response object; proposed fix and updating unit tests are suggested.\n",
      "The Quickbase connector implementation needs further testing and function expansion before it's ready to merge; a PR is opened for CI results access.\n",
      "This PR enables custom GraphQL query calls to Shopify's Admin API.\n",
      "WealthEngine integration with EveryAction has low match rates and lacks customization; API enhancement using `find_person` methods similar to VAN class in Parsons is suggested.\n",
      "The Parsons connector for New/Mode needs reimplementing to directly use the API rather than the vendor's limited SDK.\n",
      "The author proposes creating standard table objects like EventTable with predefined inclusive columns to simplify data exchange between systems.\n",
      "The new `VAN.get_changed_entities()` method enables downloading of contact history and survey responses from the last 90 days.\n",
      "The author requests a feature to specify AWS credentials, including profile name and region, for better control.\n",
      "The issue suggests adding cleanup functionality for cloud storage files after use, similar to `cleanup_s3_file` in Redshift, to methods like `post_bulk_import` and others in VAN.\n",
      "The issue, which has been marked as closed, refers to a previously reported problem identified by issue number 549.\n",
      "Documentation update needed for newly added Shopify connector and ActionKit functions.\n",
      "The issue references ControlShift Labs platform information and API documentation for JSON API endpoints.\n",
      "Users report installation errors with Parsons on Python 3.9.x due to missing Postgres `.DLL` files, some use Homebrew as a workaround.\n",
      "Added new API requests for mailer functionality.\n",
      "This PR updates the `ActionNetwork.add_person()` method to support adding a mobile phone with/without an email and raises errors if neither is provided.\n",
      "The author suggests creating a Parsons wrapper for the Strive API to facilitate its use within the software.\n",
      "A request for additional Alchemer methods to improve efficiency in handling raw data, specifically a `survey_pivot` method for restructuring response tables.\n",
      "The connector's `get_surveys` and `get_survey_responses` methods require documentation.\n",
      "The `ActionNetwork.add_person` method now supports creating a person with just a mobile phone, without an email address, aligning with Action Network API updates.\n",
      "Logging occurs even without user-supplied override credentials, risking potential credential exposure.\n",
      "Fixed a small typo in the code or documentation.\n",
      "Fixed a log message by adding a missing 'F' to correctly format it as an f-string.\n",
      "Upgraded the paramiko version to resolve an existing bug and added update_order and update_transaction functions to ActionKit.\n",
      "\n",
      "The van connector is causing a Type Error due to attempting to concatenate a string with a tuple.\n",
      "The author references an article explaining use cases for Amazon Redshift's SortKey for optimizing query performance.\n",
      "The issue reports a successful patch for `_batch_request` and `batch_insert` in `airtable-python-wrapper`, allowing new select values creation and integration with the Parsons connector.\n",
      "This PR introduces a Bluelink connector for syncing structured person data with various digital tools using Bluelink's Webhook API.\n",
      "The Parsons project uses a deprecated Slack client and needs to migrate to the latest SDK as per Slack's documentation.\n",
      "The update adds pagination to GET requests and resolves problems with attaching Tag IDs during lead creation.\n",
      "Users request that quotes should only be censored when their items are not `None`.\n",
      "MoveOn suggests implementing a generic script in Parsons to simplify data transfer between different services, referencing their own script as an example.\n",
      "New contributor updated API URLs in repository documentation and requests verification of the changes made.\n",
      "Running `pytest` on Python 3.7.3 with the latest master branch triggers an AttributeError in TempDirectory cleanup.\n",
      "The `get_saved_lists()` function is infinitely paginating due to recent NGPVAN API changes, and a quick fix has been proposed.\n",
      "API documentation links in the GitHub repository are outdated, leading to 404 errors, due to NGP VAN's recent updates.\n",
      "Requesting a review to evaluate potential data issues with changing the default data type from `DECIMAL` to `FLOAT`.\n",
      "This PR introduces Google Cloud Storage support to the Cloud Storage Utility for VAN methods that require internet-accessible data, and updates the documentation accordingly.\n",
      "Python floats are incorrectly converted to integers in Redshift tables due to the default `DECIMAL(18, 0)` type assumption in `create_sql()`.\n",
      "The author is testing quotes functionality and does not require a review at this time.\n",
      "The user reports insufficient permissions errors when updating Airtable via Redshift and suggests updating the airtable-python-wrapper to a later version to fix this.\n",
      "The Hustle Load Leads script is failing with a 422 HTTP error due to misinterpretation as a pull instead of a post request.\n",
      "This fix introduces a typecast parameter to enhance the `insert_records` function for Airtable in Parsons.\n",
      "The Airtable connector in Parsons lacks an implemented typecast parameter, causing errors when inserting new values into multiple select fields, which could be a simple fix.\n",
      "Hustle is updating its API, renaming and introducing endpoints, and lowering rate limits, requiring changes to Parsons code.\n",
      "Issue: `Table.fillna_column` not only fills missing values but also unexpectedly moves the column to the last position, potentially causing issues.\n",
      "The issue requests methods in Parsons for adding, removing, or updating action sources, currently modifiable only via API, with methods for batch updates and efficient ID retrieval.\n",
      "The `verify_row_count()` function in Parson's DBSync seems to fail when creating a new table, possibly due to premature execution.\n",
      "Generic database imports cause issues with flow from __init__ to Redshift connector; adding __init__ file for database methods resolves it.\n",
      "Issue: Request to document why Parsons Table uses petl over Pandas, highlighting memory efficiency and cost-effectiveness in handling large datasets.\n",
      "The user is requesting integration with the Reach app, which lacks an open API but offers webhooks for certain organization levels.\n",
      "Fixed the `setup.py` version number to align with the latest Parsons release.\n",
      "This update modifies the config to use Targetsmart's new SFTP host as the old one will be discontinued on June 18, 2021.\n",
      "Update the settings in `targetsmart_automation.py` to use the new `Host` and `Port` configuration for TargetSmart's SFTP.\n",
      "Parsons fails with new Facebook Business Apps due to hardcoded use of deprecated API v6.0; support for API v10.0 and version specification is needed.\n",
      "NGPVAN will mandate phone numbers for applying canvass results with specific canvass types, as detailed in their recent email.\n",
      "Everytown team plans to create an Open States connector for the new v3 API due to the absence of such in the issue tracker.\n",
      "The author requests a feature to integrate petl functions with the `parsons.Table` class for enhanced data manipulation capabilities.\n",
      "The Zoom API's old pagination using `page_count` is deprecated, causing issues with certain endpoints; a workaround with large `page_size` is used until code is updated for `next_page_token`.\n",
      "Hustle tech team reported an unspecified issue.\n",
      "The issue highlights that Parsons lacks events endpoints for Action Network, complicating event data retrieval and parsing without functions akin to `get_people`.\n",
      "Consider implementing a Page Iterator utility to handle endpoints with pagination, as discussed in a referenced Slack thread.\n",
      "Issue with Redshift connector in Python 3.6 due to AttributeError in `re_create_table.py`, not present in Python 3.7/3.8.\n",
      "The issue discusses improving how to install the `parsons` library with fewer dependencies, suggesting a separate `parsons-core` package and seeking community feedback.\n",
      "A request to add functionality for moving files between S3 and GCS in Parsons, citing resources shared by @tonywhittaker.\n",
      "The author is requesting the addition of certain methods from the ActionKit's Mailer API to the project.\n",
      "The `create_spreadsheet()` method in the Google Sheets class now has an optional `folder_id` parameter to address issue #509.\n",
      "The `cloud_storage.post_file` method returns S3 URLs that aren't publicly accessible and cause an unsupported authorization mechanism error.\n",
      "Fixed improper references to constants in rs_create_table.py, resolving related errors.\n",
      "This submission pertains to issue #440.\n",
      "Request to implement a method for downloading a file directly from Box to a local file, bypassing parson table.\n",
      "Updated create_spreadsheet function to support folder_id parameter for Google Sheets creation, needs unit tests.\n",
      "Add an optional feature to parse SQL booleans, disabled by default to avoid disrupting existing workflows.\n",
      "The summary of the issue: A recent merge aimed at reducing memory usage for multi-line JSON files is causing unexpected errors, prompting a rollback while the issue is investigated.\n",
      "The issue suggests updating the gspread package version in Parsons to utilize new features while maintaining backward compatibility.\n",
      "Updated `jinja2` library from 2.10.1 to 2.11.3, addressing regex backtracking speed issues in the `urlize` filter and python 2.7 and 3.5 deprecation notice.\n",
      "The PR introduces a new method for uploading multiple columns by VAN ID using a v4 endpoint and plans deprecation for the older single-column SOAP endpoint in `parsons.VAN.upload_saved_list()`.\n",
      "Performance is enhanced in `DBSync` by optionalizing distinct checks and row counts, and a bug related to an invalid class attribute access is fixed.\n",
      "The issue describes a documentation problem with the `DBSync` `table_sync_incremental` method, where `primary_key` is misleadingly used for ordering rather than identifying new rows.\n",
      "The author requests the addition of `debug` level log messages to the DB sync tool for improved troubleshooting and profiling.\n",
      "The `box.get_table_by_file_id` method throws `UnicodeDecodeError` on non-UTF-8 Excel files, and encoding detection with `chardet` does not solve the issue.\n",
      "The member requests a feature to sync data from TalkWalker into their S3 warehouse and explore integration points with other datasets, referencing the Talkwalker API documentation.\n",
      "The `_unpack_signups()` method in the `Signups()` VAN class raises a `ValueError` for empty tables; a proposed fix is to add a row check.\n",
      "The commit updates the DB Sync class to handle table creation errors, defaulting to using `copy` on failures.\n",
      "The issue reports that the Box class requires manual token refresh every 60 minutes, unlike the Zoom class, which auto-generates tokens.\n",
      "Added `get_state_requirements` method to rtv.py for the state_requirements API call to RockyAPI.\n",
      "The user encounters an error where a view is not recognized as a table by SQLAlchemy, possibly requiring an additional step for inclusion.\n",
      "Issue with `alter_table=True` in `rs.upsert` function, leading to load failures as seen in 'stl_load_errors'.\n",
      "This PR uses SQL Alchemy to create an empty table for error reduction, pending merge of Eliot's DBSync PR.\n",
      "Issue when using DBSync for full refresh from MySQL due to incorrect LIMIT clause configuration in `MySQLTable()` class, prompting its removal in favor of `BaseTable()`.\n",
      "Introduced retry configuration and adjustable chunk sizes in the DBSync class, and a 'strict_length' argument for Redshift to improve performance and stability.\n",
      "Proposes using Postgres' `text` type for variable-length strings to prevent issues with appending data in undersized Redshift tables.\n",
      "The `petl` version requirement has been bumped to `1.6.8`, enabling direct data loading support for json lines.\n",
      "The issue reports that the `Table.to_civis` method incorrectly passes many keyword arguments as `None` to the `CivisClient.table_import` method.\n",
      "The CivisClient `wait` option does not provide access to method results when set to False.\n",
      "This PR introduces methods to retrieve one or multiple events from Action Kit.\n",
      "Upgrades httplib2 from 0.18.0 to 0.19.0, addressing auth parsing and WSSE token string conversion issues.\n",
      "Issue opened to find a free address validation package compatible with Parsons.\n",
      "The DB sync fails if subsequent data chunks have different schema than the first; it should use the source schema for table creation.\n",
      "This issue reports the resolution of linting issues that arose following the merge of the previous PR.\n",
      "CircleCI linting failures reported for trailing whitespace and a bare 'except' in `db_sync.py`.\n",
      "An issue arose from having two packages, PyJWT and python-jwt, that both use `import jwt`, leading to the wrong one taking precedence until PyJWT was explicitly added to requirements.txt; live testing is planned before merging.\n",
      "Proposal to add `order_by` and `order_direction` parameters to three functions and a `last_modified` filter to `get_constituents`.\n",
      "A user is requesting the addition of `orderBy`, `orderDirection`, and `lastModified` parameters to pagination functions in the Bloomerang class.\n",
      "GitHub issue #474 has been resolved and closed.\n",
      "Issue with DBSync failing on `truncate` due to a benign varchar length issue; suggestion to add a `try/except` block with a `drop_if_error` parameter.\n",
      "Rebecca from DSA requests an easier method than manual CSV uploads for new data input to Action Builder, pending API redesign.\n",
      "Error encountered when calling `zoom.get_meetings()` with the `latest` docker image due to `'str' object has no attribute 'decode'`.\n",
      "This commit enhances the performance of the `__bool__` method in the Parsons `Table` by using `petl.head` to check for a single row, avoiding full table scans.\n",
      "This update renames the class from SurveyGizmo to Alchemer, adds documentation, and includes backward compatibility functions related to issue #469.\n",
      "The author suggests updating unspecified components, linking to Alchemer, possibly for survey-related reasons.\n",
      "The user reports a non-urgent issue with the use of the `tabulate` function in `notification_helpers.py` within the `canalespy` project.\n",
      "This commit addresses test failures caused by the introduction of a row count check in the `assert_matching_tables` function.\n",
      "The issue highlights the inefficiency of repeatedly scanning a table to calculate `num_rows` and suggests caching the value until the `petl` table changes.\n",
      "The author suggests adding the newer SmartMatch API to the TargetSmart connector for better matching capabilities.\n",
      "A replica of PR #461 was created as the original changes were mistakenly pushed directly to master instead of a feature branch.\n",
      "This commit corrects pagination in simulated server tests and aligns Copper connector test expectations with actual inputs.\n",
      "This PR provides a fix for issue #454, enabling developers to manually set headers and adjust `sample_size` in `get_records`.\n",
      "This PR addresses issue #459, and the author is open to feedback as it's their first contribution to the project.\n",
      "Dependabot updates petl from version 1.2.0 to 1.6.8, fixing CVE-2020-29128 and improving memory use for JSONL files.\n",
      "Request to add four Zoom API endpoints: get meeting registrants, get webinars, get webinar participants, and get webinar registrants.\n",
      "The author proposes creating a helper function to check if a desired S3 file has arrived by comparing its last modification date with a given timestamp.\n",
      "The `specifycols` argument in `copy_s3()` isn't working as intended and needs code adjustments to function correctly with `copy_statement()`.\n",
      "Using `CivisClient.table_import()` from `movementcooperative/parsons` docker image produces a `ModuleNotFoundError` for missing `pandas` library.\n",
      "Issue: Files are erroneously sent as .txt instead of .csv format.\n",
      "Sparse columns in large Airtable tables are missed by `get_records` unless sorted descending, potentially due to insufficient initial sampling of rows.\n",
      "The `get_new_rows()` in `MySQLTable` expects an integer `cutoff_value`, potentially conflicting with incremental updates; suggest testing removal of its override.\n",
      "The author suggests modifying the class initializer and connection management to integrate environment variable support for database configurations.\n",
      "The author suggests using the upsert command to upload files from S3 instead of relying on a local table.\n",
      "The author updated a driver for the Shopify API and fixed utils.py to correctly compare tables of different lengths.\n",
      "The commit modifies `Table.materialize_to_file` to return the temp file path for better user tracking and cleanup.\n",
      "Fixes a bug related to the failure to unzip archives from S3.\n",
      "Users report encountering `InternalError_: cache lookup failed for attribute 1 of relation 23802817` when employing a specific method.\n",
      "New contributor requests review of their first PR, which includes a unit test based on existing tests for the AN module, and seeks feedback on test logic and improvements.\n",
      "Request to document in Parsons that the `appy_activist_code` function in NGP VAN also applies a canvass status, impacting canvassing counts.\n",
      "Added `GoogleCloudStorage.get_url()` for presigned URLs and `Table.to_gcs_csv()` for exporting tables to GCS.\n",
      "Found a minor typo in the Quickstart documentation.\n",
      "The author is preparing for a release tomorrow.\n",
      "Issue #430 reports a problem with the documentation for editing tables in Python, likely needing corrections or updates.\n",
      "Issue: Alter_table command fails when dependent views exist.\n",
      "The summary would be: Using `to_redshift()` with `if_exists='append'` triggers an unnecessary distkey/sortkey alert; it should be disabled for append/truncate operations.\n",
      "Fixed Issue #282 by creating a Windows-specific base64 encoding test file for `test_create_message_attachments` due to newline differences.\n",
      "Documentation updates in response to issue #416.\n",
      "The issue discusses an oversight where a newly built connector was not added to parson's top-level import.\n",
      "The text addresses issue #425.\n",
      "Users need a feature to retrieve the full list of phone numbers from an account or subaccount to share within TMC, preferably in a Table format.\n",
      "Refactored `Table.match_columns` method to increase efficiency by using `cut` and `setheader` instead of multiple column operations.\n",
      "The `Table.from_json` method should use petl's incremental `fromjson` function for memory efficiency with newline-delimited JSON files.\n",
      "The issue reports invalid Python syntax in documentation with non-string dictionary keys and suggests correcting it and checking for similar issues.\n",
      "The `match_columns` usage is increasing for data standardization, but it's inefficient and needs refactoring for better performance with `petl` functions.\n",
      "Implemented the initial version of the Box connector.\n",
      "The author has provided their initial script for creating a new VAN saved list from a sample of an existing one and seeks feedback on its implementation.\n",
      "Reported issues with the VAN connector involving incorrect note formatting, inconsistent variable names between notes and docstrings, and typos in the documentation.\n",
      "Contributors need clearer instructions for building documentation within the guide, including using `/docs/requirements.txt` for dependencies.\n",
      "Fixed formatting in Bloomerang docstrings and escaped hyperlink in Twilio's `from_` argument documentation to address issue #269.\n",
      "This PR introduces an MVP Sisense/Periscope connector with methods to fetch dashboards, fetch dashboard shares, and publish a dashboard, untested due to lack of credentials.\n",
      "The commit introduces a HOW TO guide for contributors on writing unit tests, providing an overview.\n",
      "The issue requests fixing Twilio bug #419 while retaining other existing documentation updates.\n",
      "Reverted commit 049ab3333650748dd7f19a4dca221f084724bd8c due to issues discussed in issue #419.\n",
      "A script using Twilio's REST API started failing due to an unexpected 'sent_from' argument, likely due to a recent undocumented change.\n",
      "This commit addresses minor lint errors in the `SurveyGizmo` Connector tests that were missed due to outdated linting rules during PR.\n",
      "Updated google.rst documentation with quickstart guides and improved overviews.\n",
      "The \"How to Build a Connector\" documentation lacks instructions on integrating a new connector with Parsons' import system.\n",
      "This PR addresses inconsistencies and errors in documentation across various connectors, fixing quickstart code blocks, missing headers, and formatting issues.\n",
      "The `table_sync_incremental_upsert` method synchronizes tables by upserting rows based on an `updated_at` column, with checks for row distinctiveness and count mismatches.\n",
      "Shared a newly extended `create_attendance` method for the ActionNetwork class to sign up attendees for events and requested assistance with improvements and unit tests.\n",
      "A user shared their custom method for creating events in Action Network, suggesting improvements and unit tests are needed.\n",
      "This PR improves AWS documentation, with a global overview, Lambda and S3 updates, Redshift tweaks, and enhanced links in `databases.rst`.\n",
      "Placeholder issue for a ThruTalk connector in Parsons pending ThruTalk API availability; users are encouraged to request an API from ThruTalk and comment on intended use.\n",
      "Request to create an OutVote connector is on hold until OutVote releases an API; interested users should contact OutVote to expedite the process.\n",
      "Added authentication documentation and made function argument enhancements for issue #269 in the Civis project.\n",
      "Added authentication guide and quickstart; improved docstrings, API links, rate limit info, and renamed private methods in the class.\n",
      "Added Freshdesk Authentication Guide, Quickstart, enhanced class docstrings, and renamed methods to private with underscore convention.\n",
      "Added authentication guide to docs, improved Quickstart, and addressed Redash-related tasks in issue #269.\n",
      "Commit updates Redshift `upsert` function to optionally alter target table, now enabling `truncatecolumns` copy argument for large varchars.\n",
      "This commit changes the warning for missing distkey/sortkeys to trigger only during new table creation, not on every `copy`/`upsert` call.\n",
      "A user reports a `psycopg2.errors.FeatureNotSupported: VACUUM is running` error and suggests implementing a queue for vacuum commands or a retry mechanism.\n",
      "The author suggests adding arguments within automation 'kwargs' to enhance their functionality.\n",
      "Update documentation to include Authentication Guide, enhance Overview and Quickstart sections, correct typos, and update Salesforce items in #269.\n",
      "The author suggests adding an Authentication Guide and Quickstart to the documentation, moving example code to Quickstart, and addressing SFTP concerns in issue #269.\n",
      "The `Database.detect_data_type` method requires testing with various data types like lists and dictionaries to ensure proper SQL type detection.\n",
      "The `MySQL` method lacks a check for reserved words in column names that is present in the `Redshift` and `Postgres` classes.\n",
      "Improve the `TargetSmartConnector` and `TargetSmartAutomation` documentation and refactor code for consistency and simplified authentication handling.\n",
      "Proposed documentation improvements and class method naming updates, plus addressing Mobilize America items in issue #269.\n",
      "The issue suggests adding a Quick Start and Authentication Guide, improving the Overview, and reorganizing documentation for better clarity alongside addressing Facebook Ads points in #269.\n",
      "The author suggests removing the `account_domain` argument from `AzureBlogStorage` class as it is always `blob.core.windows.net` and appears as clutter.\n",
      "This PR introduces a flexible `DatabaseCreateStatement` class for common methods, improving maintainability and addressing issue #341.\n",
      "The author proposes to update documentation, notably the Authentication Guide and Quick Start, and to rename `table_convert` to `_table_convert` for privacy convention.\n",
      "Add Authentication Guide and improve Quick Start, ensure Braintree compatibility as per issue #269.\n",
      "Implemented fuzziness in `map_columns()` and added methods for bulk import results, mapping type fields, and contact upserts, with a placeholder for automatic mapping types.\n",
      "Added an Authentication Guide and formatting edits; need to check Hustle checkboxes in issue #269.\n",
      "The issue involves incorporating a line break into the SQL statement.\n",
      "Documentation updated with method links, transformation tables, doc strings cleanup, and general improvements. #382\n",
      "Improve documentation by adding an Authentication Guide and enhancing the Quickstart section; mark the Mailchimp task as completed in issue #269.\n",
      "Developers are finding it difficult to discover available functions for manipulating `Table` objects within the `SurveyGizmo` connector.\n",
      "User created a new fork and pull request for a single commit after previously failing to squash commits.\n",
      "The text for summarization is missing; please provide the body of the issue for me to summarize.\n",
      "The author requests to add an Authentication Guide, enhance the Overview and Quickstart sections, and fulfill requirements for Bill.com as mentioned in issue #269.\n",
      "The user suggests enhancements to the Overview and Quickstart documentation in reference to issue #269 concerning GitHub's checklist.\n",
      "The author suggests adding an Authentication Guide, enhancing the Overview and Quickstart sections, and addressing PDI concerns from issue #269.\n",
      "The user has added authentication and quickstart guide, and updated the logging to meet current standards (#269).\n",
      "This PR updates the `Newmode` documentation with a new Authentication Guide, enhanced Overview, and an improved Quick Start, contributing to issue #269.\n",
      "This PR adds an Authentication Guide, enhances Zoom connector docs, fixes a docstring error, and updates a private method naming convention.\n",
      "This PR introduces options to define dist/sort keys during an upsert to improve performance, defaulting to the primary key if not set.\n",
      "Proposal to match the default behavior of copying tables with Redshift's own defaults.\n",
      "Feature request: Add a customizable 'tag' to temporary filenames created by `Reshift.copy` for easier debugging.\n",
      "The message addresses issue #340 in the repository.\n",
      "The developer attempted to fix issue #281 by filtering arguments for the data dictionary and raised questions regarding default values, API field requirements, and the necessity of the 'api_url' argument.\n",
      "Draft PR introduces new SFTP methods (`list_files`, `list_subdirectories`, `get_files`, `walk_tree`) with regex pattern matching and a connection decorator, seeks feedback before finalizing tests.\n",
      "The update to `notifications.rst` addresses issue #284 and confirms successful testing of various methods in the `Slack` class after obtaining an API Token.\n",
      "This update introduces the extended report option with extra fields and adds support for a testing URL.\n",
      "Sample script provided works with CSV import/export and needs SurveyGizmo connector merge and reformatting to fit the template library.\n",
      "Developer suggests adding a convenience function in the Redshift class to retrieve distkey and sortkey due to new warnings.\n",
      "Empower data's API provides a single GET request for all data as a large JSON, with unclear reasons for not segmenting it; documentation available on GitHub.\n",
      "The issue requests documentation update to clarify that users need to be superusers or schema owners, and to mention view synchronization.\n",
      "There are issues encountered, @eliotst has an open PR, and increased documentation is needed. CC: @tonywhittaker.\n",
      "Added `aws_region` parameter to `copy_s3` method in `Redshift` to support cross-region S3 bucket loading, needs testing.\n",
      "The author suggests optimizing queries by offering an alternative `select * from {table} limit 1`, which is faster than the current method.\n",
      "Issue: `DBSync` class lacks an option to specify the S3 region for the destination bucket, unlike `parsons.Redshift.unload`.\n",
      "The `Redshift` connector's `upsert` method does not deduplicate records within the Parsons table, causing duplicate entries in Redshift.\n",
      "Introduced a new SurveyGizmo connector based on earlier code, performed manual testing, and suggested delayed microtests and assessment of helper methods.\n",
      "The user suggests adding a quickstart guide and renaming the client variable to `client` in issue #269.\n",
      "Initial pull request submitted to add new features for creating and updating contacts.\n",
      "Added `update_advocate` and `create_advocate` methods to `Phone2Action`, simplified API calls in `APIConnector`, enhanced date utilities, requires integration testing before merge.\n",
      "The text references addressing issue #347 in a GitHub repository.\n",
      "This PR introduces initial Bulk Upload functionality, focusing on activist codes and outlines plans for future updates and improved validations.\n",
      "The issue discusses the development of initial methods to interact with NGPVAN's bulk import endpoints, with six methods listed as completed.\n",
      "The hidden [cloud storage utility](https://github.com/move-coop/parsons/blob/master/parsons/utilities/cloud_storage.py) in use for VAN.upload_scores needs documentation for broader use.\n",
      "Updated Quick Start in README for Parsons to run without credentials, added `download_table` to GitHub connector, and improved `download_file` with token authentication.\n",
      "The `rs.populate_table_from_query()` with append is not adding data due to a conflict with a concurrent transaction, and errors are not being surfaced properly.\n",
      "Issue #269 suggests enhancing the project by adding authentication features and improving the overview display.\n",
      "Issue to create a shared method for common SQL dialect functions to prevent duplication across different class implementations. Assigned to @dannyboy15.\n",
      "The new flake8 config necessitates updating the CI command to lint the entire package, which currently shows ~2800 errors.\n",
      "Documentation for MySQL, Postgres, DBSync, and Database Global Overview has been updated, addressing Issue #269.\n",
      "Updated S3 documentation to include authentication, added a quickstart guide, and fixed a typo in the ActionKit quickstart for issue #269.\n",
      "Developer sought feedback on a drafted Bloomerang connector with authorization methods, documentation, and initial tests, unsure about test-writing and necessary methods.\n",
      "The update includes SQL for queries directly, removing the dependency on database views, addressing issue #301.\n",
      "Summary: This is a confirmation request regarding whether the changes address issue #319 as intended by @eliotst.\n",
      "This text addresses the first two issues listed in GitHub issue #322.\n",
      "The issue requests changing the flake8 lint command to simply `flake8 parsons` for easier use and to facilitate additional linting options.\n",
      "The `generate_data_types` method incorrectly classifies columns with \"NA\" and numeric values as `varchar` instead of `decimal` or `smallint`.\n",
      "The author requests feedback on a new infrastructure for handling sample code, including label names, structure of template_script.py, handling of config variables, and the accuracy of script descriptions.\n",
      "This commit modifies the ActionKit Connector to use the `with` statement for better connection handling with `requests`.\n",
      "The summary states TMC is responsible for maintenance, not VAN Support.\n",
      "Create API endpoints to handle ```update_advocate()``` function for opt-in/opt-out and phone number updates, including data manipulation for specified fields.\n",
      "Implemented initial GitHub connector via PyGithub with concerns about pull request loading in `list_repo_issues` and handling nested JSON responses in `Table`.\n",
      "The request is to update the setup to indicate support for Python 3.8, following the changes made in PR #303.\n",
      "The `latest` Parsons Docker image has incompatible Chrome (84) and ChromeDriver (85) versions, causing selenium errors; version synchronization is suggested.\n",
      "Fix implemented to handle duplicate columns in copied tables by appending list index, resolving Issue #204.\n",
      "The unload command has issues with querying by state, non-parameterizable pipe-delimited data, and a restrictive size limit producing too many files.\n",
      "The author apologizes for multiple unnecessary commits accompanying a minor change in their fork.\n",
      "Ensure that venv instructions specify python3 to avoid syntax failure on systems defaulting to Python 2.7.\n",
      "Request for a method in the `S3` class to load and concatenate multiple CSV files into a single `Table` using a manifest file.\n",
      "A user added a comment to the documentation but encountered an issue with `make html` due to a Sphinx bug, and questions including explicit install instructions in the requirements.\n",
      "This PR resolves Issue #291 and adds documentation for Targets methods.\n",
      "This PR fixes issue #310 by removing default fields from `VAN.get_person()` when using the MyVoters database instead of EveryAction.\n",
      "The issue discusses bulk upload enhancements, including a `set_only_columns` argument, collecting upload errors, and warnings for missing email or user_id columns.\n",
      "This PR improves the ActionKit connector documentation, updates quickstart, fixes typos, and standardizes `**kwargs` info, passing all tests.\n",
      "The issue documents research on which connectors offer free accounts, public APIs, and the possibility of creating shared Parsons dummy accounts, seeking details and links to sources.\n",
      "Consider implementing a `drop_cascade` option for commands that fail due to dependent views.\n",
      "The `get_person()` method's default `expand_fields` includes `codes` causing a 400 error and `contribution_history` and `organization_roles` causing 500 errors; proposing removal for `db_code == 0`.\n",
      "The issue comments on addressing certain items mentioned in issue #286.\n",
      "The author proposes to rename a folder to `useful_resources` and discusses details about virtual environments without running tests, as changes are only in CONTRIBUTOR.md.\n",
      "This PR introduces methods to update events and event signups in ActionKit and updates `.gitignore` for PyCharm files; all tests passed.\n",
      "The enhancement enhances `list_blobs` to match Azure's native blob listing by providing additional blob details beyond just names.\n",
      "Introduced AzureAccountAccess class with initial TokenCredentials authentication, planning to expand with more Azure auth methods later.\n",
      "The commit updates civis and joblib versions for Python 3.8 support.\n",
      "Updated `psycopg2-binary` dependency to fix installation issues with Parsons on Python 3.8 due to old version compatibility problems.\n",
      "The `get_table_definition` and `get_table_definitions` methods in Redshift rely on non-standard views, requiring rework or removal for broader usability.\n",
      "The `map_columns()` function fails when attempting to map to an existing column name; a coalesce behavior is preferred.\n",
      "Reduced log size by changing some table utility methods from `INFO` to `DEBUG`; seeks further input on additional changes.\n",
      "Issue #285 fixed: Redshift class now reads S3 credentials passed as kwargs for the `copy` method.\n",
      "TMC provided working code for a Parsons connector: https://gist.github.com/elyse-weiss/101d251eeed2e4f2fb25e8754ca493bd, seeking to finalize it.\n",
      "Implemented Azure Blob Storage connector with feedback sought on splitting `AZURE_ACCOUNT_URL`, credential support expansion, and `put_blob` usability improvements.\n",
      "The `create_target_export()` function in 'VAN Targets' used an incorrect property (`target_id` instead of `targetId`), and there was a JSON key error due to an unnecessary `[0]`.\n",
      "Updated Python 3 Setup link and removed Local Development Setup link as the file was deleted.\n",
      "The Common Workflows documentation incorrectly refers to the function as `van.score_update_status(job_id,'approved')` instead of `update_score_status()`.\n",
      "This commit introduces a new guide for building connectors in Parsons, updates the documentation TOC, relocates the contributing guide, and adds `m2r` library support.\n",
      "The author needs assistance with testing and is unsure how to proceed.\n",
      "The issue relates to threading in messages as described in the Slack API documentation on message sending.\n",
      "Include an option to set `phoneOptInStatus` for syncing records with phone numbers when upserting a person in VAN using Frakture.\n",
      "Update 'contributing.md' with a clear walkthrough for setup and testing, add communication links, simplify the quickstart guide, link to useful resources, and include doc building instructions; refine best practices.\n",
      "Instantiating Redshift with AWS credentials fails when running the copy method despite correct usage syntax.\n",
      "Legacy tokens for Slack have been deprecated; clarification on generating a new API key for the Slack class needed.\n",
      "The issue reports a missing \"f\" string prefix causing failure.\n",
      "A user reports 8 test failures on Windows within `TestSMTP` and `TestGmail` while running `pytest` on the Parsons test package.\n",
      "The issue is that default empty list arguments cause errors, and the suggested fix is to filter out unsupplied arguments in the final call.\n",
      "Introduced a new method for formatting cells, addressing issue #251.\n",
      "This commit removes several unnecessary `print` calls from the codebase.\n",
      "This PR introduces two new methods for event updates and adding event locations to the VAN API.\n",
      "Upgrades the `gspread` library to version 3.3.0.\n",
      "This PR introduces tests for the recently implemented auto-resizing varchar columns feature in Redshift upsert function (#273).\n",
      "Resolves #250 by resizing Redshift target table columns based on Parsons table before staging, potentially erroring on upsert mismatches to save time.\n",
      "The user suggests altering the destination table to extend varchar size before UPSERT operations using parsons tables.\n",
      "The issue suggests not using `SMALLINT` for columns and recommends a minimum `varchar` length of `varchar(100)`.\n",
      "The issue requests additional key endpoints for Sisense dashboards, specifically for retrieval, sharing, and publishing with filter value settings.\n",
      "The issue requests updated documentation for various connectors to include an overview, authentication guide, and quick start guide, with an example provided and a current status table for reference.\n",
      "The issue suggests adding an argument to control whether the function should search for a header when only a header is present.\n",
      "Add a Bloomerang class to Parsons with abilities to pull and create various constituent and donation data, including custom fields and interactions.\n",
      "The commit improves the `Phone2Action` connector for empty advocate lists and implements `__bool__` in `Table` for emptiness checks.\n",
      "Added `materialize_to_file` method to apply transformations and write table data to a temp file, avoiding in-memory loading limitations.\n",
      "This PR refactors the GoogleSheets class, consolidates worksheet reading methods, deprecates old methods, and adds 'list_worksheets()' functionality.\n",
      "Methods in the library now accept an `SFTP.connection` object, enabling context manager usage; issue #260's method will be updated later.\n",
      "This commit fixes the Phone2Action connector's pagination parameters and adds a regression test for future error detection.\n",
      "Issue #248 reports a method that downloads a CSV file (possibly compressed) from an SFTP server and converts it into a Parsons table.\n",
      "Improved documentation by adding PDI to the Table of Contents and including docstrings for a method.\n",
      "The commit updates the `Phone2Action` connector for manual pagination of advocates and removes `unpack_dict` from `updated_at` and `created_at` columns.\n",
      "The Redshift upsert function sometimes fails to trigger alter_table during a copy function-based staging table creation; changing an `elif` to an `if` could fix this.\n",
      "The issue requests replacing `set_env_var()` in sample code with a more accessible function and adding Windows Powershell setup instructions.\n",
      "Consolidated tools (AWS, Google) and fixed minor issues; no live code affected.\n",
      "Dependabot updates httplib2 from 0.12.0 to 0.18.0, including a fix for the critical CWE-93 CRLF injection vulnerability.\n",
      "The author requests a feature to perform an upsert directly from an existing Redshift table without pulling data back into Parsons.\n",
      "Fixed a timezone-related issue causing tests to pass on CI but fail locally by adding timezone support and setting UTC as default.\n",
      "Issue requests adding `format()` function from gspread to the GoogleSheets class in Parsons project for enhanced capabilities.\n",
      "The author suggests running `alter_varchar_column_widths()` early in the `upsert()` function after table existence verification.\n",
      "The connector now supports automatic retries on rate limits and can upload binary files like PDFs and JPGs with `is_binary=True`.\n",
      "The author suggests enhancing the SFTP driver with connection reusability, pattern-based file listing, and direct downloading to Table object functionality.\n",
      "The PDI class connects to specific PDI's API endpoints, none of which are deprecated, with local tests passing and live tests passing a month ago.\n",
      "Issue with Freshdesk connector where evaluation of an empty table fails due to transform_table function attempting to move non-existent columns.\n",
      "This initial class release adds methods for getting users, meetings, past meetings, and past meeting participants, with plans for future webinar features.\n",
      "The update introduces `get_spreadsheet_permissions()` and `share_spreadsheet()` methods, addressing Issue #238.\n",
      "The GoogleSheets() Parsons class lacks individual sharing and adjustable permissions, a feature available in gspread, which the author desires.\n",
      "This PR fixes a bug where appending data to empty sheets failed and includes minor class structure cleanup.\n",
      "The user reports `append_to_sheet()` function fails on blank sheets; requests fixing the function or updating documentation for clarity.\n",
      "Implement functions in the ActionKit class to update events and signups as per provided API documentation.\n",
      "This commit enhances the Parsons BigQuery connector for DB syncs, aligns table naming with SQL standards, and fixes minor DB sync bugs.\n",
      "EveryAction users with Fundraising packages need CRUD capabilities on contributions, with possible inclusion of Designation and Financial Batch endpoints.\n",
      "Corrected a typo in the documentation from \"specfic\" to \"specific.\"\n",
      "The Parsons repository needs enhanced reporting and tracking for outside contributions, Pull Requests, and Issues data.\n",
      "The RTV connector class needs a new method to support uploading registrations to the Rock the Vote API.\n",
      "Added Rock The Vote documentation to the project's documentation index.\n",
      "Request to add functionality for creating and retrieving Target Export Jobs in Parsons, aligning with new EveryAction endpoint.\n",
      "Added `RockTheVote` connector for Rock the Vote's Rocky API, implementing registrant report endpoints for partner registration records; fixes #119.\n",
      "Add a Parsons connector for the Facebook Insights API to download ad reporting data and track conversations from Facebook pixels.\n",
      "Redshift CPU strain during upserts led to discovery that `copy`'s default `compupdate=True` slows down analysis, especially for tables with many columns, suggesting a possible default change or documentation update.\n",
      "The commit updates `mysql-connector-python` to version 8.0.18 and removes unneeded Google auth libraries in Parsons' `requirements.txt`.\n",
      "The author expresses hope that their contribution will also improve Redshift's `alter_table` functionality.\n",
      "Request to implement functionality in Parsons for updating existing events using the available endpoint in the VAN API.\n",
      "Initial discussion on implementing CRUD operations for individual records referencing Action Network's API documentation.\n",
      "An initial Zoom connector is 50% complete on a stale branch without unit tests; might need a fresh branch start.\n",
      "Implement an initial connector for the Box API to list folders/files and get/save files using the existing Python SDK.\n",
      "The authors have reviewed the New/Mode external library and decided to add it back as a dependency on Parsons.\n",
      "The author mentions that they forgot to stop importing the `Newmode` client after removing the corresponding requirement.\n",
      "Logger.debug throws an exception without %s inclusions when logger level is set to DEBUG.\n",
      "Fixed `convert_columns_to_str` method in Parsons `Table` to handle empty list from `petl` typeset function, preventing index out of bounds exception.\n",
      "The issue discusses altering an API to return a dictionary with 'success' and an array of 'results', impacting how tables split on empty values, and seeks input on naming the parameter.\n",
      "Enhancement request for `Hustle.create_leads()` to return a table of lead_ids after lead creation.\n",
      "An individual acknowledges @retacg for assistance with a project-related matter.\n",
      "Modified the Redshift copy function's query to match DDL with template table and added tests for Parsons table datatype mismatches.\n",
      "The added code aligns staging table columns with the destination table during a Redshift upsert to prevent data copy issues.\n",
      "A user experienced an issue where `to_redshift()` failed without logging due to a Parsons Table having duplicate column names.\n",
      "The issue requests that `create_lead` and `create_leads` Hustle commands return the created lead ids instead of `None`.\n",
      "The issue discusses improvements to Redshift's copy function, adding IAM role support for S3 authentication, and a new template_table argument for automatic schema creation and data casting.\n",
      "Fixed a bug in the VAN.create_code() function.\n",
      "New integration with New/Mode's API is being added.\n",
      "The issue indicates that the documentation, tests, and additional logging tasks have been completed.\n",
      "Add a logger warning for Redshift copy without DIST/SORT key to promote best practices.\n",
      "Propose using `pk` as the distribution and sort key to improve upsert performance.\n",
      "Request for adding DBSync support for BigQuery.\n",
      "Request for a new section in documentation to include scripts for comprehensive workflows involving multiple classes.\n",
      "The issue requests the implementation of a Zoom connector MVP with methods for retrieving users, webinars, and webinar attendees.\n",
      "Proposal to merge Redshift and Postgres code bases to reduce redundancy due to their overlapping methods.\n",
      "New functionality catches failed upserts from missing target tables, rolls back the query, and transfers data to a new table.\n",
      "The author discusses an optimization for user_field-only uploads in ActionKit, adjusting the memory usage threshold and inquiring about the preferred approach for incremental changes.\n",
      "The issue introduces a `MySQL.copy()` method that creates tables if missing, using multi-insert instead of `LOAD DATA` due to security and common configuration restrictions.\n",
      "The author describes an approach for processing data with AWS Lambda and S3, seeks feedback, and discusses implementation details like class instantiation and dependencies.\n",
      "The author acknowledges an embarrassing typo from the initial Pull Request.\n",
      "This PR introduces the MySQL Table object for future development of MySQL DBSync functionality.\n",
      "Removal of two test files that inadvertently entered the repository.\n",
      "Fix needed for API error caused by `max` input incorrectly incrementing with `start`.\n",
      "The issue requests support for batch uploads from a Table or CSV to ActionKit based on their REST API documentation.\n",
      "This PR improves `Redshift.copy()` performance and adds an `alter_table` argument to adjust target column width, with limitations on dependent views.\n",
      "The default `max` value for list calls has been increased to accommodate over 999 items, with added pagination support and a test method.\n",
      "The author requests an SMTP API like Gmail's, supports in-memory file objects without tests yet, and seeks feedback on handling StringIO/BytesIO.\n",
      "A user-reported error was reproduceable locally despite passing all tests, necessitating a new release (v0.11.1) after merging the fix.\n",
      "Added initial MySQL connector with `MySQL.query()` and `MySQL.query_with_connection()` methods.\n",
      "PR updates the handling of VAN records to allow updates using VANIDs; includes cleanup and backwards-compatible changes to search methods.\n",
      "Two new methods for retrieving custom field data and a fix for `VAN.get_code()` were introduced, along with documentation improvements for VAN.\n",
      "Running an upsert fails if the destination table doesn't exist; it should default to using `Redshift.copy()` instead.\n",
      "Issue: VAN API charges for adding custom fields to Pipeline, complicating EveryAction use; a single call to get all custom field data is requested to simplify this process.\n",
      "The commit enhances the `Redshift` connector by enabling the specification of multiple \"primary key\" columns for deduplication in upsert operations.\n",
      "Fixed pagination issue in `VAN.get_events()` method.\n",
      "The `VAN.get_events()` method is experiencing pagination issues.\n",
      "The Graph API v4.0 is outdated and not compatible with new Facebook apps.\n",
      "The author requests the addition of `Table.to_postgres()` and `Table.from_postgres()` convenience methods.\n",
      "Create `to_postgres()` and `from_postgres()` convenience methods for the `Table` class, similar to Redshift's implementation.\n",
      "The database sync class now exits sync gracefully when encountering an empty source table, preventing failures.\n",
      "This PR introduces Redshift database support in the DB Sync class.\n",
      "Kevin (@dekedor) gets credit for contributions, whereas the author accepts responsibility for any issues, including rebase errors.\n",
      "The `updated_since` parameter for Freshdesk's contacts endpoint should be `_updated_since`, as current usage is not recognized by their API.\n",
      "This commit updates the `Redshift.upsert` method, adding a 'stg' prefix and random noise to the staging table name to prevent errors during parallel executions.\n",
      "MoveOn needs support for loading data from Redash queries due to its use for managing SQL queries across various databases.\n",
      "The `DbSync` class syncs tables between Postgres databases either fully or incrementally, with future support planned for Redshift and BigQuery.\n",
      "A new environment variable `PARSONS_SKIP_IMPORT_ALL` was introduced to avoid loading all connectors in resource-conscious environments.\n",
      "The commit corrects a typo in the Salesforce client import and removes `try/except` guards to avoid hiding `ImportError`s.\n",
      "A method is described for assigning codes, such as source codes and tags, to a user record.\n",
      "The `VAN.upsert_person` method now includes a parameter for setting the phone type upon record creation.\n",
      "Initial PR for a Freshdesk connector adds core table extraction and a new `Table.sort()` method due to Freshdesk's unusual API sorting.\n",
      "Take-2 for the fix of issue #146 following @eliotst's feedback on the previous attempt #147.\n",
      "The commit updates the `parsons.aws.s3.S3` class to allow extra arguments for the `copy`, `get_file`, and `list_keys` methods.\n",
      "The author requests preparation of the Parsons package for an upcoming new release.\n",
      "The issue addresses a correction needed in the `client` property.\n",
      "Added `remove_original` argument to `transfer_bucket` method to optionally delete the original bucket, referencing issue #122.\n",
      "The patch fixes issue #146, is invasive yet backwards compatible, and operates even with missing dependencies.\n",
      "The issue highlights the unsustainability of loading all submodules in `parsons/__init__.py`, suggesting splitting core functionality into `parsons_core` to reduce dependencies and size.\n",
      "During live testing, `update_record()` handling was found to be incorrect, prompting adjustments to its code and `test_update()`.\n",
      "Fixed a typo in the Redshift client's copy statement generation from S3 to use `null as` instead of `nullas`.\n",
      "The Redshift integration code has an incorrect SQL parameter; it should use \"null as\" instead of \"nullas\" on line 57.\n",
      "The Mobile Commons connector is non-functional, awaiting a new API version from the vendor for a rebuild later this year.\n",
      "The update checks for an empty table before running `Table.unpack_dict()` to address Issue #140.\n",
      "An error arises in Airtable API when `get_records()` is called with a formula that yields no results, potentially due to `unpack_dict()` issues.\n",
      "This PR adds support for RSA key-based authentication for SFTP connections, allowing password and key file path to be used together and includes relevant tests.\n",
      "User @elyse-weiss suggests a solution to an existing problem.\n",
      "This issue requests the addition of a class for handling Bill.com API calls to manage users, customers, invoices, and queries.\n",
      "The issue emphasizes the impact a single character can have, likely in the context of coding or software development.\n",
      "Standardized language and formatting adjustments made in the Table of Contents of the documentation.\n",
      "This commit updates Parsons to pass a delimiter for CSV loading to Redshift when creating a table if it doesn't exist.\n",
      "Implement string or integer indexing in Parsons tables for easier row and column access.\n",
      "The `VAN.delete_supporter_group()` function removes a supporter group, while `VAN.connection.api_key_profiles()` retrieves API key-associated metadata.\n",
      "Proposal to enhance column selection in a table to align with the Pandas pattern (`tbl['a']`) for user convenience and feedback request.\n",
      "The issue highlights that the current implementation only reveals the doc strings from an underlying Redshift method.\n",
      "Feature request to enable comparison between two Redshift tables as well as between a Parsons table and a Redshift table.\n",
      "This commit introduces a `copy` method allowing a Parsons table to be loaded into BigQuery via Google Cloud Storage upload.\n",
      "The commit improves the `download_blob` method in the `GoogleCloudStorage` connector, enhancing robustness and handling of empty files without extra permission checks.\n",
      "The author believes the text addresses key setup considerations.\n",
      "Adds the capability to specify the S3 Access Control List (ACL) in a method by exposing an argument from S3.put_file().\n",
      "The method compares Parsons and Redshift table column widths and alters Redshift varchar columns to prevent copy errors, aiming for future integration with `Redshift.copy()`.\n",
      "The documentation needs improved clarity.\n",
      "The provided text references the ActBlue webhooks documentation, suggesting the topic involves webhook integration or usage.\n",
      "The commit modifies the `GoogleCloudStorage` connector to download blobs without requiring `get_bucket` access, fixing issue #115.\n",
      "The author mentions having existing code in a private repository that can be utilized to create a class, crediting Gerard at ACRONYM.\n",
      "A user suggests creating a beginner-friendly guide to setting up a development environment and starting with Parsons, asking for collaborators to assist in the effort.\n",
      "This PR ensures the temporary table is dropped if the initial data copy operation fails by moving the copy statement into a try/finally block.\n",
      "The issue discusses that the `rs.upsert()` function leaves behind a timestamp table if the upsert operation fails, and inquires about dropping it.\n",
      "Parsons GCS methods `get_blob` and `download_blob` could be improved to not require listing the bucket, enabling use with fewer permissions.\n",
      "The new Postgres connector adds `query` and `copy` methods, with a `PostgresCore` class for future `Redshift` integration and planned advanced copy features.\n",
      "Added functionality to `ActionKit.create_generic_action()` for better error reporting, addressing user experience concerns. (Addresses #93)\n",
      "Parson's tables do not automatically display as formatted tables in Jupyter Notebook as pandas dataframes do.\n",
      "The author suspects a simple oversight in the creation of the `kwargs` dictionary.\n",
      "Add a URI parameter option for VANConnector to enable connections to various VAN instances, with a default set to the VAN API URL.\n",
      "`Table.to_s3_csv` lacks the ability to set file permissions unlike `s3.put_file`, defaulting to full permissions.\n",
      "Fixed `van.find_person()` bug and removed `scores` from `van.get_person()` default return to avoid permission errors.\n",
      "The VAN API v4 lacks an endpoint for uploading saved lists, so methods utilizing the old SOAP-based v3 API are introduced to push lists.\n",
      "The issue requests a feature to enter formulas in a project without Google Sheets converting them to text, including an untested function.\n",
      "This issue requests cleaning up formatting problems in the codebase.\n",
      "Issue found when using `find_people` in `people.py`, suggesting modification of People constructor to reference `van_object.connection` for correct method calls.\n",
      "The change ensures that both precision and scale are returned for decimal types in Redshift.\n",
      "Fixed typos in various `rst` files including index, Google Cloud, and VAN sections.\n",
      "Updated `get_event` to remove unsupported field, standardized `expand`/`fields` to `expand_fields`, and added missing fields to `get_events` and `get_person`.\n",
      "The `from_s3_csv` method should bypass the `has_data` check to prevent `FileNotFoundError` for remote sources.\n",
      "The issue reports that tabs and newlines within quotes are being interpreted as escape sequences rather than literal text.\n",
      "The update now raises an error when a Slack method encounters an issue.\n",
      "The commit upgrades joblib from version 0.11 to 0.13.\n",
      "The provided text contains a link to documentation on REST API action processing for Robotic Dogs' ActionKit.\n",
      "The commit enhances contributing.md with more details to aid newcomers to open source and the Parsons project.\n",
      "The author shares their messy Python code utilizing `email-validator` and `pydash` libraries to clean up human-entered email addresses from signup sheets.\n",
      "User requests support for OSDI (Open Source DID Infrastructure).\n",
      "Fix implemented to remove a falsely triggered logging notification.\n",
      "Deprecated method warning for `apply_activist_code` shows scary red text but still functions, causing concern among users.\n",
      "The user requests support for writing tables to PostgreSQL in addition to Redshift, with the ability to run queries and transfer data.\n",
      "The author suggests using the Bulk Import endpoint, instead of individual upserts, to efficiently scale data syncs into VAN.\n",
      "The author suggests setting a minimum varchar length of 100 for Redshift syncs and rounding up varchar lengths to avoid failures.\n",
      "Issue encountered with dependencies for Google BigQuery and Google Cloud Storage; proposed solution included.\n",
      "A push caused lint failures that were not detected in local testing due to a series of oversights.\n",
      "@jburchard is informed about the initial Salesforce class creation, with linting done but no unit tests, and there's a question on whether to return Parsons Tables or lists of dicts.\n",
      "Introduced `VAN.upload_scores()` in place of `VAN.upload_create_load_multi()`, simplifying score uploads by automating file posting and average calculations.\n",
      "The commit introduces `TempFilePath`, avoiding `NamedTemporaryFile` open handle issues on Windows by returning just the file path.\n",
      "Refactored all methods and removed old request code; included missed `update_signup()` refactor in this PR.\n",
      "The issue describes a method for checking if an S3 file has data to prevent sync errors due to empty files and incorporates it into the `Table.from_csv()` method.\n",
      "Introduces a new method `tbl.first` for Parsons Table to easily access the first value from database queries, seeking feedback on attribute naming related to Issue #55.\n",
      "Documentation updated to display methods and include new examples.\n",
      "Refactored VAN request methods for `PATCH`, `PUT`, `DELETE` and updated corresponding methods and tests.\n",
      "The Twilio connector has initial methods for accessing account details, usage information, and messages.\n",
      "This issue provides a link to helpful code to assist with a specific task in the GitHub repository.\n",
      "The user encountered an error with `Table.from_csv()` due to an empty file and suggests improving logging to reflect such cases.\n",
      "Users encounter a \"Permission denied\" error on Windows when `Redshift.query` tries to reopen a temporary file, violating Windows' file handling constraints.\n",
      "The provided text is a link to documentation for the 'civicrm_osdi' GitHub repository.\n",
      "The bug in `create_file_load_multi` affects the VAN score helper functionality.\n",
      "Fixed `HTTPError` exception message in `APIConnector` to include status code, reason, and JSON response body if available.\n",
      "Added a Google BigQuery connector to execute queries and retrieve results as a Parsons table, addressing issue #17.\n",
      "The `approve_scores()` function fails to pass `score_id` to `get_score_updates()`, resulting in unfiltered score updates, requiring a simple fix.\n",
      "The issue highlights a possible mistake on line 83 of utilities/api_connector.py, where the `params` argument is not being passed correctly.\n",
      "Error occurs in toggle_activist_code() due to Parsons logger expecting a dictionary, but VAN API provides a string.\n",
      "The author is primarily interested in the Twilio API endpoint for reading multiple SMS message resources.\n",
      "Refactoring now has all POST requests using updated methods.\n",
      "Added tests for the `remove_empty_keys()` function in the `utilities.json_format` module.\n",
      "The Hustle Connector was experiencing problems due to an unspecified issue.\n",
      "This commit adds a `ResourceManager` class to manage temporary files for `Table` usage, but doesn't address temporary files from methods like `S3.get_file`.\n",
      "Added Circle CI and pypi traffic badges to the repository.\n",
      "Updated several GET methods for changed VAN entities.\n",
      "This request suggests adding a method to retrieve a single value from a query without needing to create a Parsons table.\n",
      "Issue with CircleCI caused by a repository name change has been resolved.\n",
      "The issue requests updating the repository name and related links in the documentation.\n",
      "Enhancements include cleaning up a function from the last PR, updating methods to use the new POST request, and improving response validation.\n",
      "The author has successfully migrated various GET requests, including codes, canvass responses, and others, to a new API connector.\n",
      "The author suggests creating a wrapper for the simple-salesforce library for a member project and may contribute back from the experience.\n",
      "Refactor PR #32 to avoid implementing pagination in `GET` requests, making it class-specific due to API variability.\n",
      "The author suggests adding a feature to log the file size for SFTP transfers to aid in monitoring downloads.\n",
      "Introduced `Table.csv_size` to estimate eventual CSV size from the first 1K rows for managing VAN Bulk Import endpoint file size limitations.\n",
      "Fixed incorrect compression in `Table.to_s3_csv()` and added parameter to `Table.to_csv()` to specify the file in the archive, with concerns about adding more parameters.\n",
      "This PR introduces the feature to access the bulk imports endpoint in VAN with a function returning a list of available resources for API users.\n",
      "This PR fixes Sphinx document build failures by adding Parsons dependencies to `./docs/requirements.txt`.\n",
      "This PR changes ActionKit class instantiation to require full domain specification, accommodating custom URLs, which is a breaking change.\n",
      "Removed `pandas` dependency from the project due to its minimal use and contribution to significant bloat in Parsons.\n",
      "Removed `sphinx` dependencies from general requirements to a new `/docs/requirements.txt` to reduce unnecessary 50M in dependencies.\n",
      "The author reorganized the Redshift Documentation into sections and standardized language to improve navigation and clarity.\n",
      "Enhancements include added logging, documentation standardization, and changes in JSON unpacking and return type for certain methods.\n",
      "Implement `van.approve_scores` to approve given score IDs, with an option to either raise an error or log if approval fails.\n",
      "This PR removes custom error checking for `env var`, utilizes standard Parsons utility, and adds direct Civis client access for low-level method use. Future updates will add common methods.\n",
      "Added new agent and tag-related methods, cleaned up documentation, and fixed some bugs from version 1.\n",
      "The user suggests refactoring the `find_person()` method to simplify it by directing users with an ID to `get_person()` and moving `match_map` to a separate method.\n",
      "Propose standardizing API connectors in the project using a basic class for all `requests` to simplify GET requests and pagination, starting with the VAN class.\n",
      "The contributor is updating and standardizing the VAN documentation as part of an ongoing improvement effort.\n",
      "The issue discusses a feature request to pass a phone number to a function to add it to the global Do Not Call (DNC) list.\n",
      "The author has made an initial implementation of a Hustle connector with various methods for organizations, groups, and leads, including smart column mapping for flexible input.\n",
      "Suggestion to incorporate phone number validation/parsing using the python-phonenumbers library.\n",
      "Opened PR to discuss tests on Redshift tables using `Table` objects, encountering successes and expected failures with various append and truncate operations.\n",
      "Refactored certain method arguments from comma-separated strings to lists for better adherence to Python standards.\n",
      "The author successfully tested the `find_person` and `upsert_person` methods in various scenarios without errors.\n",
      "The update fixes most of the warnings and errors previously thrown by Sphinx.\n",
      "The author requests prioritization for the `createLead` endpoint, but also requires implementation of all lead-related endpoints.\n",
      "The author is looking to integrate Hustle's newly released beta API into Parsons, focusing on Leads, Agents, Groups, and Tags endpoints.\n",
      "Feature request for a commonly needed functionality to be added for improved efficiency.\n",
      "The author emphasizes the urgency of implementing a feature or fixing an issue in the near future.\n",
      "The author introduces a method for generating a public URL and seeks feedback and thoughts on it.\n",
      "The VAN connector requires refactoring using standardized APIConnector methods for GET, POST, DELETE, and PATCH operations.\n",
      "Request to enhance `Table.to_csv()` with `.zip` compression and add a new `Table.to_csv_zip()` for zipped CSVs with additional features.\n",
      "The issue discusses appending to target tables when source tables lack some columns or when default values are needed, such as timestamps.\n",
      "The issue involves problems with the connector for CrowdTangle.\n",
      "The text summarizes an update related to Issue #9 and tags the user @kbenker for notification or action.\n",
      "The `upsert_person` function enforces field minimums not required by VAN API, causing issues for users wanting to add records with sparse information.\n",
      "The `upsert_person` method documentation incorrectly states it returns a Parsons Table instead of a dictionary with vanid and status information.\n",
      "Two new methods, `get_events_organization()` and `get_people()`, have been added to the codebase.\n",
      "Add connection info to the top of MobileCommons documentation for clarity, as done in other class documentation.\n",
      "The issue requests the implementation of functions for multiple endpoints such as List mConnects, Count MConnect Calls, List Keywords, etc., prioritized accordingly.\n",
      "The script throws an xml.parsers.expat.ExpatError for incorrect credentials instead of a more descriptive 'Invalid credentials' error.\n",
      "Issues in MobileCommons class: profile filters not working, group members filters failing, and need to implement automatic pagination for groups and members; also requesting testing for add/delete endpoints.\n",
      "The issue discusses updates to setup.py and related files in preparation for the first release of the software.\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4-turbo-preview in organization org-dnsLBIN9IeStdXjb3ul4pJ1n on tokens per min (TPM): Limit 30000, Requested 35743. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m message \u001b[38;5;241m=\u001b[39m format_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, instructions) \u001b[38;5;66;03m# system means high priority \u001b[39;00m\n\u001b[1;32m     66\u001b[0m messages \u001b[38;5;241m=\u001b[39m [message] \u001b[38;5;66;03m# ChatGPT API expects any message to be in a list\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_response\u001b[39m(messages):\n\u001b[0;32m---> 17\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-4-1106-preview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# newest, cheapest model\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     content \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/parsons-analysis/env/lib/python3.12/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4-turbo-preview in organization org-dnsLBIN9IeStdXjb3ul4pJ1n on tokens per min (TPM): Limit 30000, Requested 35743. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# summarize text in issues.body\n",
    "\n",
    "# get openai api key\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# create openai client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "# code used for interacting with the OpenAI API\n",
    "\n",
    "def format_message(role, content):\n",
    "        return {\"role\": role, \"content\": content}\n",
    "\n",
    "\n",
    "def get_response(messages):\n",
    "    completion = client.chat.completions.create(\n",
    "        model='gpt-4-1106-preview', # newest, cheapest model\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = completion.choices[0].message.content\n",
    "    return content\n",
    "\n",
    "# get body fields\n",
    "body_fields = issues['body'].dropna().tolist()\n",
    "\n",
    "# make list to hold summaries that will later itself be summarized\n",
    "summaries = []\n",
    "\n",
    "# get quotes\n",
    "\n",
    "for j in range(len(body_fields)):\n",
    "    \n",
    "    quotes = body_fields[j]\n",
    "\n",
    "    instructions = f\"\"\"\n",
    "\n",
    "    You will be provided with the body of an issue from a GitHub repository. Summarize this text in one sentence, \n",
    "    no more than half the length of the input, or 20 words, whichever is shorter. The summary should be concise and informative, \n",
    "    focusing on the key issues raised by the author. The audience for these summaries are software developers who are familiar with the project. \n",
    "    The summaries will be used to identify common issues and help developers address them more efficiently.\n",
    "\n",
    "    Quotes: {quotes}\n",
    "    \"\"\"\n",
    "\n",
    "    message = format_message(\"system\", instructions) # system means high priority \n",
    "    messages = [message] # ChatGPT API expects any message to be in a list\n",
    "    response = get_response(messages)\n",
    "    print(response)\n",
    "    summaries.append(response)\n",
    "\n",
    "# summarize summaries\n",
    "\n",
    "quotes = \" \".join(summaries)\n",
    "\n",
    "instructions = f\"\"\"\n",
    "\n",
    "You will be provided with your previous summaries of the body of issues from a GitHub repository. Summarize this text in 2-3 paragraphs,\n",
    "focusing on common themes across issues. The audience for this summary of summaries are software developers who are familiar with the project. \n",
    "The 2-3 paragraphs you produce will be used to identify common issues and help developers understand common issues facing users and developers.\n",
    "\n",
    "Quotes: {quotes}\n",
    "\"\"\"\n",
    "\n",
    "message = format_message(\"system\", instructions) # system means high priority \n",
    "messages = [message] # ChatGPT API expects any message to be in a list\n",
    "response = get_response(messages)\n",
    "print(response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
