,title,url,user,number,state,created_at,updated_at,closed_at,body,labels,assignees,comments
0,Kasiah/empower connector,https://api.github.com/repos/move-coop/parsons/issues/1191,Kasia Hinkson,1191,open,2024-11-18 15:38:33+00:00,2024-11-18 15:50:10+00:00,,"Branched off @jburchard 's Empower connector PR. 

Adds a single method to return the full export unparsed, for pure ELT. ",[],[],0
1,Bump civis from 1.16.1 to 2.4.0 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1190,,1190,open,2024-11-18 11:27:01+00:00,2024-11-18 11:27:02+00:00,,"Bumps [civis](https://github.com/civisanalytics/civis-python) from 1.16.1 to 2.4.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/civisanalytics/civis-python/releases"">civis's releases</a>.</em></p>
<blockquote>
<h2>v2.4.0</h2>
<h3>Added</h3>
<ul>
<li>The new kwarg <code>retries</code> has been added to <code>civis.APIClient</code> so that
a <code>tenacity.Retrying</code> instance can be provided to customize retries. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/495"">#495</a>)</li>
<li>Added <code>civis.workflows.validate_workflow_yaml</code>
to validate a Civis Platform workflow YAML definition. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/497"">#497</a>, <a href=""https://redirect.github.com/civisanalytics/civis-python/issues/499"">#499</a>)</li>
<li>The helper I/O functions that create a Civis file
(i.e., <code>civis.io.file_to_civis</code>, <code>civis.io.dataframe_to_file</code>, and <code>civis.io.json_to_file</code>)
accept a new <code>description</code> keyword argument for the new <code>description</code> attribute
of Civis file objects. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/498"">#498</a>, <a href=""https://redirect.github.com/civisanalytics/civis-python/issues/500"">#500</a>)</li>
<li><code>Response</code> objects are now fully typed through the attribute syntax. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>Both <code>Response</code> and <code>PaginatedResponse</code> are now directly available under the <code>civis</code> namespace. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>Added support for Python 3.13. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>Added the new property <code>default_database_credential_id</code> at <code>civis.APIClient</code>,
which is going to replace the existing <code>default_credential</code>. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/502"">#502</a>)</li>
</ul>
<h3>Changed</h3>
<ul>
<li>When a <code>PaginatedResponse</code> object is returned from an API call,
a user-specified <code>limit</code> kwarg is now honored to facilitate speeding up the pagination. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
</ul>
<h3>Deprecated</h3>
<ul>
<li>The method <code>get_database_credential_id</code> at <code>civis.APIClient</code> has been deprecated
and will be removed at civis-python v3.0.0. There's no replacement for this method. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/502"">#502</a>)</li>
<li>The property <code>default_credential</code> at <code>civis.APIClient</code> has been deprecated
and will be removed at civis-python v3.0.0,
in favor of the new property <code>default_database_credential_id</code>. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/502"">#502</a>)</li>
</ul>
<h3>Removed</h3>
<ul>
<li>Dropped support for Python 3.9. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/499"">#499</a>)</li>
</ul>
<h3>Fixed</h3>
<ul>
<li>The repr form of <code>Response</code> objects is now the dict-based <code>Response({‘spam’: 123})</code>
instead of the dataclass-based <code>Response(spam=123)</code>, since response object keys can
be invalid Python identifiers. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>In <code>Response</code> object instantiation, object keys that originate from environment variables
are now preserved for their (customarily upper-) case even in the default snake-case setting. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>In <code>Response</code> object instantiation, an API response that represents a JSONValue object
now has its <code>value</code> attribute unmodified as the Python object representation
of the deserialized JSON form (as opposed to being converted to a <code>Response</code>-based form). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
</ul>
<h2>v2.3.0</h2>
<h3>Added</h3>
<ul>
<li>Added a script for checking if the Civis API spec is up-to-date. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/489"">#489</a>)</li>
<li>Added a new keyword argument <code>sql_params_arguments</code> to the <code>civis.io.*</code> functions that
accept a SQL query, so that the user can run a parameterized SQL script. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
</ul>
<h3>Changed</h3>
<ul>
<li>Refactored the <code>civis.parallel</code> module and related unit tests due to major changes
of joblib from v1.2.0 to v1.3.0 (API-breaking changes for dropping
<code>joblib.my_exceptions.TransportableException</code> and <code>joblib.format_stack.format_exc</code>,</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/civisanalytics/civis-python/blob/main/CHANGELOG.md"">civis's changelog</a>.</em></p>
<blockquote>
<h2>2.4.0 - 2024-11-11</h2>
<h3>Added</h3>
<ul>
<li>The new kwarg <code>retries</code> has been added to <code>civis.APIClient</code> so that
a <code>tenacity.Retrying</code> instance can be provided to customize retries. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/495"">#495</a>)</li>
<li>Added <code>civis.workflows.validate_workflow_yaml</code>
to validate a Civis Platform workflow YAML definition. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/497"">#497</a>, <a href=""https://redirect.github.com/civisanalytics/civis-python/issues/499"">#499</a>)</li>
<li>The helper I/O functions that create a Civis file
(i.e., <code>civis.io.file_to_civis</code>, <code>civis.io.dataframe_to_file</code>, and <code>civis.io.json_to_file</code>)
accept a new <code>description</code> keyword argument for the new <code>description</code> attribute
of Civis file objects. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/498"">#498</a>, <a href=""https://redirect.github.com/civisanalytics/civis-python/issues/500"">#500</a>)</li>
<li><code>Response</code> objects are now fully typed through the attribute syntax. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>Both <code>Response</code> and <code>PaginatedResponse</code> are now directly available under the <code>civis</code> namespace. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>Added support for Python 3.13. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>Added the new property <code>default_database_credential_id</code> at <code>civis.APIClient</code>,
which is going to replace the existing <code>default_credential</code>. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/502"">#502</a>)</li>
</ul>
<h3>Changed</h3>
<ul>
<li>When a <code>PaginatedResponse</code> object is returned from an API call,
a user-specified <code>limit</code> kwarg is now honored to facilitate speeding up the pagination. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
</ul>
<h3>Deprecated</h3>
<ul>
<li>The method <code>get_database_credential_id</code> at <code>civis.APIClient</code> has been deprecated
and will be removed at civis-python v3.0.0. There's no replacement for this method. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/502"">#502</a>)</li>
<li>The property <code>default_credential</code> at <code>civis.APIClient</code> has been deprecated
and will be removed at civis-python v3.0.0,
in favor of the new property <code>default_database_credential_id</code>. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/502"">#502</a>)</li>
</ul>
<h3>Removed</h3>
<ul>
<li>Dropped support for Python 3.9. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/499"">#499</a>)</li>
</ul>
<h3>Fixed</h3>
<ul>
<li>The repr form of <code>Response</code> objects is now the dict-based <code>Response({‘spam’: 123})</code>
instead of the dataclass-based <code>Response(spam=123)</code>, since response object keys can
be invalid Python identifiers. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>In <code>Response</code> object instantiation, object keys that originate from environment variables
are now preserved for their (customarily upper-) case even in the default snake-case setting. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
<li>In <code>Response</code> object instantiation, an API response that represents a JSONValue object
now has its <code>value</code> attribute unmodified as the Python object representation
of the deserialized JSON form (as opposed to being converted to a <code>Response</code>-based form). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/501"">#501</a>)</li>
</ul>
<h2>2.3.0 - 2024-06-14</h2>
<h3>Added</h3>
<ul>
<li>Added a script for checking if the Civis API spec is up-to-date. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/489"">#489</a>)</li>
<li>Added a new keyword argument <code>sql_params_arguments</code> to the <code>civis.io.*</code> functions that
accept a SQL query, so that the user can run a parameterized SQL script. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
</ul>
<h3>Changed</h3>
<ul>
<li>Refactored the <code>civis.parallel</code> module and related unit tests due to major changes</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/24be84d31d41b6e39104ca4581724934c28fe6d2""><code>24be84d</code></a> [CIVIS-9806] MAINT bump version to v2.4.0 (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/503"">#503</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/74bb82fca53ac155650ded2b13da7a96b2d325ff""><code>74bb82f</code></a> [CIVIS-9192] ENH clean up database credential IDs at <code>civis.APIClient</code> (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/502"">#502</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/ecccead0b0df6371106c7548f55668408b819aa2""><code>ecccead</code></a> [CIVIS-7940] [CIVIS-8862] ENH API response objects: typing, repr, fixes for k...</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/6803c2c454b4e989e9e7da0874f60ec57fa20e8a""><code>6803c2c</code></a> [CIVIS-9155] ENH support <code>description</code> in file upload CLI (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/500"">#500</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/87e14a754a9e4478b3dc4cf3c8556eb29d4c5b5f""><code>87e14a7</code></a> [CIVIS-8846] FIX validation checks of a workflow YAML definition; DEP drop Py...</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/d6fa63b9712d0effb575201ffe49352fb985ea05""><code>d6fa63b</code></a> [CIVIS-9155] ENH support setting a Civis file's new <code>description</code> attribute (...</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/f72ec23a7f01ca5bf385d95f3115b15abf78a69c""><code>f72ec23</code></a> [CIVIS-8846] ENH validate a workflow YAML definition (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/497"">#497</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/184a942ada5911f24058b478bc2def5879207272""><code>184a942</code></a> Bump certifi from 2024.6.2 to 2024.7.4 in /docs (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/496"">#496</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/801d8c8970b42bcc988d4aca6a54555b2fb67991""><code>801d8c8</code></a> [CIVIS-8768] ENH customize API call retrying behavior (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/495"">#495</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/0e303267e6757e2023595cd66dab45ab0e1021a6""><code>0e30326</code></a> Bump urllib3 from 2.2.1 to 2.2.2 in /docs (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/494"">#494</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/civisanalytics/civis-python/compare/v1.16.1...v2.4.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=civis&package-manager=pip&previous-version=1.16.1&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
2,Bump grpcio from 1.62.2 to 1.68.0 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1189,,1189,open,2024-11-18 11:24:55+00:00,2024-11-18 11:24:56+00:00,,"Bumps [grpcio](https://github.com/grpc/grpc) from 1.62.2 to 1.68.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/grpc/grpc/releases"">grpcio's releases</a>.</em></p>
<blockquote>
<h2>Release v1.68.0</h2>
<p>This is release 1.68.0 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">groovy</a>) of gRPC Core.</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>
<h2>Core</h2>
<ul>
<li>[XdsClient][Backport] Add missing authority to XdsClient metrics scope (<a href=""https://redirect.github.com/grpc/grpc/issues/38009"">#38009</a>). (<a href=""https://redirect.github.com/grpc/grpc/pull/38023"">#38023</a>)</li>
<li>[Release] Bump core version in preparation for 1.68 Branch Cut. (<a href=""https://redirect.github.com/grpc/grpc/pull/37941"">#37941</a>)</li>
<li>[ConfigFetcher] Set HTTP2 error to NO_ERROR to do graceful GOAWAYs. (<a href=""https://redirect.github.com/grpc/grpc/pull/37939"">#37939</a>)</li>
<li>[ruby] reduce INFO log for server CQ pluck registration to DEBUG. (<a href=""https://redirect.github.com/grpc/grpc/pull/37633"">#37633</a>)</li>
<li>[EventEngine] Enable the PosixEventEngine client experiment. (<a href=""https://redirect.github.com/grpc/grpc/pull/35985"">#35985</a>)</li>
<li>[chttp2_server] Fix race between connection starting and it being orphaned. (<a href=""https://redirect.github.com/grpc/grpc/pull/37683"">#37683</a>)</li>
<li>[Chttp2Server] Fix race between connection manager updates and handshake. (<a href=""https://redirect.github.com/grpc/grpc/pull/37772"">#37772</a>)</li>
<li>[xds] Fix XdsClient race between ResourceDoesNotExist timer and receiving resources. (<a href=""https://redirect.github.com/grpc/grpc/pull/37678"">#37678</a>)</li>
</ul>
<h2>C++</h2>
<ul>
<li>[Build] Minimum version of MSVC is now 2022. (<a href=""https://redirect.github.com/grpc/grpc/pull/37687"">#37687</a>)</li>
<li>[Build] Bumped the minimum version of cmake. (<a href=""https://redirect.github.com/grpc/grpc/pull/37702"">#37702</a>)</li>
</ul>
<h2>Python</h2>
<ul>
<li>Add templating and support for Python 3.13. (<a href=""https://redirect.github.com/grpc/grpc/pull/37643"">#37643</a>)</li>
</ul>
<h2>Release v1.68.0-pre1</h2>
<p>This is a prerelease of gRPC Core 1.68.0 (groovy).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This prerelease contains refinements, improvements, and bug fixes.</p>
<h2>Release v1.67.1</h2>
<p>This is release gRPC Core 1.67.1 (gesundheit). This is a Python-only patch release.</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes.</p>
<h2>Release v1.67.0</h2>
<p>This is release 1.67.0 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">gesundheit</a>) of gRPC Core.</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/grpc/grpc/commit/6b49ae626bc9cd7033e062f89dbe0e0576b1110e""><code>6b49ae6</code></a> [Release] Bump v.1.68.x to 1.68.0 (<a href=""https://redirect.github.com/grpc/grpc/issues/38124"">#38124</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/2bcf835e1f91001107321ef2c2545e6f23c602c2""><code>2bcf835</code></a> [Backport][chttp2] Fix channelz address (<a href=""https://redirect.github.com/grpc/grpc/issues/38022"">#38022</a>) (<a href=""https://redirect.github.com/grpc/grpc/issues/38027"">#38027</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/5801d12f806cf8b25e08e92784a94243a73093ee""><code>5801d12</code></a> [XdsClient][Backport] Add missing authority to XdsClient metrics scope (<a href=""https://redirect.github.com/grpc/grpc/issues/3800"">#3800</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/62e547b3c586b31ffe35e19e2538b0efff904105""><code>62e547b</code></a> Bump v1.68.x version to v1.68.0-pre1 (<a href=""https://redirect.github.com/grpc/grpc/issues/38001"">#38001</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/1178b2d229ad7fd27df743176dbc2ea104d6b28e""><code>1178b2d</code></a> [retry e2e test] add log message about known flakiness (<a href=""https://redirect.github.com/grpc/grpc/issues/37974"">#37974</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/c5999dbac3e507a2cb85e15ed7b8fde30ba541ba""><code>c5999db</code></a> [call-v3] Fix leak with cq-based server (<a href=""https://redirect.github.com/grpc/grpc/issues/37972"">#37972</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/eacb2f72c82d6749e73fc9581ac02783f10ca8f9""><code>eacb2f7</code></a> Changed Bazel/Workspace to use <a href=""https://github.com/com""><code>@​com</code></a>_google_protobuf//python/dist:syst… (<a href=""https://redirect.github.com/grpc/grpc/issues/37971"">#37971</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/14f22c7fd7af7a589623b94353c00de873a6f7b6""><code>14f22c7</code></a> [EventEngine] Disable the backup poller if all EventEngine experiments are ru...</li>
<li><a href=""https://github.com/grpc/grpc/commit/4662017636ed7b9f72d040a0312f551f8fd9cbc4""><code>4662017</code></a> [EE] Prevent crash when address can't be resolved (<a href=""https://redirect.github.com/grpc/grpc/issues/37952"">#37952</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/c3e83b8efd9b8821c1a9f252eac6c22c78af4f1e""><code>c3e83b8</code></a> [Release] Bump core version in preparation for 1.68 Branch Cut (<a href=""https://redirect.github.com/grpc/grpc/issues/37941"">#37941</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/grpc/grpc/compare/v1.62.2...v1.68.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=grpcio&package-manager=pip&previous-version=1.62.2&new-version=1.68.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
3,Bump github/codeql-action from 3.26.12 to 3.27.4,https://api.github.com/repos/move-coop/parsons/issues/1188,,1188,open,2024-11-18 11:22:07+00:00,2024-11-18 11:22:08+00:00,,"Bumps [github/codeql-action](https://github.com/github/codeql-action) from 3.26.12 to 3.27.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/releases"">github/codeql-action's releases</a>.</em></p>
<blockquote>
<h2>v3.27.4</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.4 - 14 Nov 2024</h2>
<p>No user facing changes.</p>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.4/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
<h2>v3.27.3</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.3 - 12 Nov 2024</h2>
<p>No user facing changes.</p>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.3/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
<h2>v3.27.2</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.2 - 12 Nov 2024</h2>
<ul>
<li>Fixed an issue where setting up the CodeQL tools would sometimes fail with the message &quot;Invalid value 'undefined' for header 'authorization'&quot;. <a href=""https://redirect.github.com/github/codeql-action/pull/2590"">#2590</a></li>
</ul>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.2/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
<h2>v3.27.1</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.1 - 08 Nov 2024</h2>
<ul>
<li>The CodeQL Action now downloads bundles compressed using Zstandard on GitHub Enterprise Server when using Linux or macOS runners. This speeds up the installation of the CodeQL tools. This feature is already available to GitHub.com users. <a href=""https://redirect.github.com/github/codeql-action/pull/2573"">#2573</a></li>
<li>Update default CodeQL bundle version to 2.19.3. <a href=""https://redirect.github.com/github/codeql-action/pull/2576"">#2576</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/blob/main/CHANGELOG.md"">github/codeql-action's changelog</a>.</em></p>
<blockquote>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>[UNRELEASED]</h2>
<p>No user facing changes.</p>
<h2>3.27.4 - 14 Nov 2024</h2>
<p>No user facing changes.</p>
<h2>3.27.3 - 12 Nov 2024</h2>
<p>No user facing changes.</p>
<h2>3.27.2 - 12 Nov 2024</h2>
<ul>
<li>Fixed an issue where setting up the CodeQL tools would sometimes fail with the message &quot;Invalid value 'undefined' for header 'authorization'&quot;. <a href=""https://redirect.github.com/github/codeql-action/pull/2590"">#2590</a></li>
</ul>
<h2>3.27.1 - 08 Nov 2024</h2>
<ul>
<li>The CodeQL Action now downloads bundles compressed using Zstandard on GitHub Enterprise Server when using Linux or macOS runners. This speeds up the installation of the CodeQL tools. This feature is already available to GitHub.com users. <a href=""https://redirect.github.com/github/codeql-action/pull/2573"">#2573</a></li>
<li>Update default CodeQL bundle version to 2.19.3. <a href=""https://redirect.github.com/github/codeql-action/pull/2576"">#2576</a></li>
</ul>
<h2>3.27.0 - 22 Oct 2024</h2>
<ul>
<li>Bump the minimum CodeQL bundle version to 2.14.6. <a href=""https://redirect.github.com/github/codeql-action/pull/2549"">#2549</a></li>
<li>Fix an issue where the <code>upload-sarif</code> Action would fail with &quot;upload-sarif post-action step failed: Input required and not supplied: token&quot; when called in a composite Action that had a different set of inputs to the ones expected by the <code>upload-sarif</code> Action. <a href=""https://redirect.github.com/github/codeql-action/pull/2557"">#2557</a></li>
<li>Update default CodeQL bundle version to 2.19.2. <a href=""https://redirect.github.com/github/codeql-action/pull/2552"">#2552</a></li>
</ul>
<h2>3.26.13 - 14 Oct 2024</h2>
<p>No user facing changes.</p>
<h2>3.26.12 - 07 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add a deprecation warning for customers using CodeQL version 2.14.5 and earlier. These versions of CodeQL were discontinued on 24 September 2024 alongside GitHub Enterprise Server 3.10, and will be unsupported by CodeQL Action versions 3.27.0 and later and versions 2.27.0 and later. <a href=""https://redirect.github.com/github/codeql-action/pull/2520"">#2520</a></p>
<ul>
<li>
<p>If you are using one of these versions, please update to CodeQL CLI version 2.14.6 or later. For instance, if you have specified a custom version of the CLI using the 'tools' input to the 'init' Action, you can remove this input to use the default version.</p>
</li>
<li>
<p>Alternatively, if you want to continue using a version of the CodeQL CLI between 2.13.5 and 2.14.5, you can replace <code>github/codeql-action/*@v3</code> by <code>github/codeql-action/*@v3.26.11</code> and <code>github/codeql-action/*@v2</code> by <code>github/codeql-action/*@v2.26.11</code> in your code scanning workflow to ensure you continue using this version of the CodeQL Action.</p>
</li>
</ul>
</li>
</ul>
<h2>3.26.11 - 03 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add support for using <code>actions/download-artifact@v4</code> to programmatically consume CodeQL Action debug artifacts.</p>
<p>Starting November 30, 2024, GitHub.com customers will <a href=""https://github.blog/changelog/2024-04-16-deprecation-notice-v3-of-the-artifact-actions/"">no longer be able to use <code>actions/download-artifact@v3</code></a>. Therefore, to avoid breakage, customers who programmatically download the CodeQL Action debug artifacts should set the <code>CODEQL_ACTION_ARTIFACT_V4_UPGRADE</code> environment variable to <code>true</code> and bump <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> in their workflows. The CodeQL Action will enable this behavior by default in early November and workflows that have not yet bumped to <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> will begin failing then.</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/github/codeql-action/commit/ea9e4e37992a54ee68a9622e985e60c8e8f12d9f""><code>ea9e4e3</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2605"">#2605</a> from github/update-v3.27.4-3ab67a219</li>
<li><a href=""https://github.com/github/codeql-action/commit/845ea9230baa1c02f895802221f61eb448caa267""><code>845ea92</code></a> Update changelog for v3.27.4</li>
<li><a href=""https://github.com/github/codeql-action/commit/3ab67a21932f9425e7dca53353787b8dda8e89d9""><code>3ab67a2</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2597"">#2597</a> from github/mbg/caching/output-improvements</li>
<li><a href=""https://github.com/github/codeql-action/commit/6e3a010dfe7e41114c548b680d885bbd55b2834e""><code>6e3a010</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2583"">#2583</a> from jsoref/use-artifact-4</li>
<li><a href=""https://github.com/github/codeql-action/commit/1c83cd12920695d0a30eb88c71a10f79f5ae22b4""><code>1c83cd1</code></a> Upgrade actions/upload-artifact to v4</li>
<li><a href=""https://github.com/github/codeql-action/commit/024283fcc9914a2f29343fa25558256c1799501f""><code>024283f</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2602"">#2602</a> from github/mergeback/v3.27.3-to-main-396bb3e4</li>
<li><a href=""https://github.com/github/codeql-action/commit/613fe96926eddbad17ae49a608c6cd6fb07c4d10""><code>613fe96</code></a> Update checked-in dependencies</li>
<li><a href=""https://github.com/github/codeql-action/commit/e35d4aa1da5d4ef385c387ee6ad1f286494398f5""><code>e35d4aa</code></a> Update changelog and version after v3.27.3</li>
<li><a href=""https://github.com/github/codeql-action/commit/396bb3e45325a47dd9ef434068033c6d5bb0d11a""><code>396bb3e</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2601"">#2601</a> from github/update-v3.27.3-f04790367</li>
<li><a href=""https://github.com/github/codeql-action/commit/2b1319450a8536cc55c2629acf08b8de0d6974fc""><code>2b13194</code></a> Update changelog for v3.27.3</li>
<li>Additional commits viewable in <a href=""https://github.com/github/codeql-action/compare/c36620d31ac7c881962c3d9dd939c40ec9434f2b...ea9e4e37992a54ee68a9622e985e60c8e8f12d9f"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=github/codeql-action&package-manager=github_actions&previous-version=3.26.12&new-version=3.27.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
4,Bump install-pinned/uv from de03c60d508703a83d3f8f49afcf1249590ecda1 to 5743f9419e8ab31942eab7471341261126a29f03,https://api.github.com/repos/move-coop/parsons/issues/1187,,1187,open,2024-11-18 11:21:54+00:00,2024-11-18 11:21:55+00:00,,"Bumps [install-pinned/uv](https://github.com/install-pinned/uv) from de03c60d508703a83d3f8f49afcf1249590ecda1 to 5743f9419e8ab31942eab7471341261126a29f03.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/install-pinned/uv/commit/5743f9419e8ab31942eab7471341261126a29f03""><code>5743f94</code></a> update README.md (uv 0.5.2)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/01932e7aa5f69ff3145ca1540e6326840746a822""><code>01932e7</code></a> update pins (uv 0.5.2)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/256e3097d2fdc5ffa16479dcbc25254d3ea8c9f4""><code>256e309</code></a> update README.md (uv 0.5.1)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/c70bd7fdc86c17660b7e3a9542d953cb641d5248""><code>c70bd7f</code></a> update pins (uv 0.5.1)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/ad3d6a2241b82101eefbfd3a90e1eba75539f25e""><code>ad3d6a2</code></a> update README.md (uv 0.5.0)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/198f05e57d57b3b61820749e315d09c9cec6119d""><code>198f05e</code></a> update pins (uv 0.5.0)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/9474303e6634428224fbe0012fc67b189b72e320""><code>9474303</code></a> update README.md (uv 0.4.30)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/c2dc718b3c35970f9f18a8036d20a349c26eab8c""><code>c2dc718</code></a> update pins (uv 0.4.30)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/079b3722376f674a95014281a9210848aa6c2f34""><code>079b372</code></a> update README.md (uv 0.4.29)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/82e203f79f1ae6edb04a292df4f1c80c28302704""><code>82e203f</code></a> update pins (uv 0.4.29)</li>
<li>Additional commits viewable in <a href=""https://github.com/install-pinned/uv/compare/de03c60d508703a83d3f8f49afcf1249590ecda1...5743f9419e8ab31942eab7471341261126a29f03"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
5,Update Dockerfile,https://api.github.com/repos/move-coop/parsons/issues/1186,Kasia Hinkson,1186,open,2024-11-14 14:54:30+00:00,2024-11-18 15:39:16+00:00,,Updates Dockerfile to Python 3.11,[],[],1
6,Bump install-pinned/uv from de03c60d508703a83d3f8f49afcf1249590ecda1 to 256e3097d2fdc5ffa16479dcbc25254d3ea8c9f4,https://api.github.com/repos/move-coop/parsons/issues/1185,,1185,closed,2024-11-11 10:14:18+00:00,2024-11-18 11:21:59+00:00,2024-11-18 11:21:57+00:00,"Bumps [install-pinned/uv](https://github.com/install-pinned/uv) from de03c60d508703a83d3f8f49afcf1249590ecda1 to 256e3097d2fdc5ffa16479dcbc25254d3ea8c9f4.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/install-pinned/uv/commit/256e3097d2fdc5ffa16479dcbc25254d3ea8c9f4""><code>256e309</code></a> update README.md (uv 0.5.1)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/c70bd7fdc86c17660b7e3a9542d953cb641d5248""><code>c70bd7f</code></a> update pins (uv 0.5.1)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/ad3d6a2241b82101eefbfd3a90e1eba75539f25e""><code>ad3d6a2</code></a> update README.md (uv 0.5.0)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/198f05e57d57b3b61820749e315d09c9cec6119d""><code>198f05e</code></a> update pins (uv 0.5.0)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/9474303e6634428224fbe0012fc67b189b72e320""><code>9474303</code></a> update README.md (uv 0.4.30)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/c2dc718b3c35970f9f18a8036d20a349c26eab8c""><code>c2dc718</code></a> update pins (uv 0.4.30)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/079b3722376f674a95014281a9210848aa6c2f34""><code>079b372</code></a> update README.md (uv 0.4.29)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/82e203f79f1ae6edb04a292df4f1c80c28302704""><code>82e203f</code></a> update pins (uv 0.4.29)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/5323b902eae0f36bc0abac26a4ac330bdfc36296""><code>5323b90</code></a> update README.md (uv 0.4.28)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/52bf95a91123f9849d76f95f62a757876e002996""><code>52bf95a</code></a> update pins (uv 0.4.28)</li>
<li>Additional commits viewable in <a href=""https://github.com/install-pinned/uv/compare/de03c60d508703a83d3f8f49afcf1249590ecda1...256e3097d2fdc5ffa16479dcbc25254d3ea8c9f4"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],1
7,Bump github/codeql-action from 3.26.12 to 3.27.1,https://api.github.com/repos/move-coop/parsons/issues/1184,,1184,closed,2024-11-11 10:14:11+00:00,2024-11-18 11:22:12+00:00,2024-11-18 11:22:10+00:00,"Bumps [github/codeql-action](https://github.com/github/codeql-action) from 3.26.12 to 3.27.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/releases"">github/codeql-action's releases</a>.</em></p>
<blockquote>
<h2>v3.27.1</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.1 - 08 Nov 2024</h2>
<ul>
<li>The CodeQL Action now downloads bundles compressed using Zstandard on GitHub Enterprise Server when using Linux or macOS runners. This speeds up the installation of the CodeQL tools. This feature is already available to GitHub.com users. <a href=""https://redirect.github.com/github/codeql-action/pull/2573"">#2573</a></li>
<li>Update default CodeQL bundle version to 2.19.3. <a href=""https://redirect.github.com/github/codeql-action/pull/2576"">#2576</a></li>
</ul>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.1/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
<h2>v3.27.0</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.0 - 22 Oct 2024</h2>
<ul>
<li>Bump the minimum CodeQL bundle version to 2.14.6. <a href=""https://redirect.github.com/github/codeql-action/pull/2549"">#2549</a></li>
<li>Fix an issue where the <code>upload-sarif</code> Action would fail with &quot;upload-sarif post-action step failed: Input required and not supplied: token&quot; when called in a composite Action that had a different set of inputs to the ones expected by the <code>upload-sarif</code> Action. <a href=""https://redirect.github.com/github/codeql-action/pull/2557"">#2557</a></li>
<li>Update default CodeQL bundle version to 2.19.2. <a href=""https://redirect.github.com/github/codeql-action/pull/2552"">#2552</a></li>
</ul>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.0/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
<h2>v3.26.13</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.26.13 - 14 Oct 2024</h2>
<p>No user facing changes.</p>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.26.13/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/blob/main/CHANGELOG.md"">github/codeql-action's changelog</a>.</em></p>
<blockquote>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>[UNRELEASED]</h2>
<p>No user facing changes.</p>
<h2>3.27.1 - 08 Nov 2024</h2>
<ul>
<li>The CodeQL Action now downloads bundles compressed using Zstandard on GitHub Enterprise Server when using Linux or macOS runners. This speeds up the installation of the CodeQL tools. This feature is already available to GitHub.com users. <a href=""https://redirect.github.com/github/codeql-action/pull/2573"">#2573</a></li>
<li>Update default CodeQL bundle version to 2.19.3. <a href=""https://redirect.github.com/github/codeql-action/pull/2576"">#2576</a></li>
</ul>
<h2>3.27.0 - 22 Oct 2024</h2>
<ul>
<li>Bump the minimum CodeQL bundle version to 2.14.6. <a href=""https://redirect.github.com/github/codeql-action/pull/2549"">#2549</a></li>
<li>Fix an issue where the <code>upload-sarif</code> Action would fail with &quot;upload-sarif post-action step failed: Input required and not supplied: token&quot; when called in a composite Action that had a different set of inputs to the ones expected by the <code>upload-sarif</code> Action. <a href=""https://redirect.github.com/github/codeql-action/pull/2557"">#2557</a></li>
<li>Update default CodeQL bundle version to 2.19.2. <a href=""https://redirect.github.com/github/codeql-action/pull/2552"">#2552</a></li>
</ul>
<h2>3.26.13 - 14 Oct 2024</h2>
<p>No user facing changes.</p>
<h2>3.26.12 - 07 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add a deprecation warning for customers using CodeQL version 2.14.5 and earlier. These versions of CodeQL were discontinued on 24 September 2024 alongside GitHub Enterprise Server 3.10, and will be unsupported by CodeQL Action versions 3.27.0 and later and versions 2.27.0 and later. <a href=""https://redirect.github.com/github/codeql-action/pull/2520"">#2520</a></p>
<ul>
<li>
<p>If you are using one of these versions, please update to CodeQL CLI version 2.14.6 or later. For instance, if you have specified a custom version of the CLI using the 'tools' input to the 'init' Action, you can remove this input to use the default version.</p>
</li>
<li>
<p>Alternatively, if you want to continue using a version of the CodeQL CLI between 2.13.5 and 2.14.5, you can replace <code>github/codeql-action/*@v3</code> by <code>github/codeql-action/*@v3.26.11</code> and <code>github/codeql-action/*@v2</code> by <code>github/codeql-action/*@v2.26.11</code> in your code scanning workflow to ensure you continue using this version of the CodeQL Action.</p>
</li>
</ul>
</li>
</ul>
<h2>3.26.11 - 03 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add support for using <code>actions/download-artifact@v4</code> to programmatically consume CodeQL Action debug artifacts.</p>
<p>Starting November 30, 2024, GitHub.com customers will <a href=""https://github.blog/changelog/2024-04-16-deprecation-notice-v3-of-the-artifact-actions/"">no longer be able to use <code>actions/download-artifact@v3</code></a>. Therefore, to avoid breakage, customers who programmatically download the CodeQL Action debug artifacts should set the <code>CODEQL_ACTION_ARTIFACT_V4_UPGRADE</code> environment variable to <code>true</code> and bump <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> in their workflows. The CodeQL Action will enable this behavior by default in early November and workflows that have not yet bumped to <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> will begin failing then.</p>
<p>This change is currently unavailable for GitHub Enterprise Server customers, as <code>actions/upload-artifact@v4</code> and <code>actions/download-artifact@v4</code> are not yet compatible with GHES.</p>
</li>
<li>
<p>Update default CodeQL bundle version to 2.19.1. <a href=""https://redirect.github.com/github/codeql-action/pull/2519"">#2519</a></p>
</li>
</ul>
<h2>3.26.10 - 30 Sep 2024</h2>
<ul>
<li>We are rolling out a feature in September/October 2024 that sets up CodeQL using a bundle compressed with <a href=""http://facebook.github.io/zstd/"">Zstandard</a>. Our aim is to improve the performance of setting up CodeQL. <a href=""https://redirect.github.com/github/codeql-action/pull/2502"">#2502</a></li>
</ul>
<h2>3.26.9 - 24 Sep 2024</h2>
<p>No user facing changes.</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/github/codeql-action/commit/4f3212b61783c3c68e8309a0f18a699764811cda""><code>4f3212b</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2585"">#2585</a> from github/update-v3.27.1-3ef4c0845</li>
<li><a href=""https://github.com/github/codeql-action/commit/63b548d59efbc36249fe81c530cd6abbb92dfb6b""><code>63b548d</code></a> Update changelog for v3.27.1</li>
<li><a href=""https://github.com/github/codeql-action/commit/3ef4c0845750690942ece9abe29a853edce0f43c""><code>3ef4c08</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2576"">#2576</a> from github/update-bundle/codeql-bundle-v2.19.3</li>
<li><a href=""https://github.com/github/codeql-action/commit/4e033f0e2665ee6b02a64f636ae76c937e3c7293""><code>4e033f0</code></a> Merge branch 'main' into update-bundle/codeql-bundle-v2.19.3</li>
<li><a href=""https://github.com/github/codeql-action/commit/5ac2ddd6fc78e6c0d2af5b110b19381f38bd84df""><code>5ac2ddd</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2580"">#2580</a> from jsoref/minor-cleanup</li>
<li><a href=""https://github.com/github/codeql-action/commit/3b7b85fbe86fa0c4cbfb2e855f588b595a95609e""><code>3b7b85f</code></a> Conditionally clear runner cache</li>
<li><a href=""https://github.com/github/codeql-action/commit/688ea5370d3de84af47f851b307068890b8de8bf""><code>688ea53</code></a> Fix publish-immutable-action version</li>
<li><a href=""https://github.com/github/codeql-action/commit/1e6d67b13813efa924b55655e325752fc41e4890""><code>1e6d67b</code></a> Give expected-queries-runs permissions</li>
<li><a href=""https://github.com/github/codeql-action/commit/d5e73848c49f7764fd7b8bf826748e767b955fd3""><code>d5e7384</code></a> Strip trailing whitespace generated by ruamel-yaml</li>
<li><a href=""https://github.com/github/codeql-action/commit/756aa649dfdf25203aa333caa3f5f9503f09400e""><code>756aa64</code></a> spelling: macos</li>
<li>Additional commits viewable in <a href=""https://github.com/github/codeql-action/compare/c36620d31ac7c881962c3d9dd939c40ec9434f2b...4f3212b61783c3c68e8309a0f18a699764811cda"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=github/codeql-action&package-manager=github_actions&previous-version=3.26.12&new-version=3.27.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],1
8,Bump install-pinned/uv from de03c60d508703a83d3f8f49afcf1249590ecda1 to 079b3722376f674a95014281a9210848aa6c2f34,https://api.github.com/repos/move-coop/parsons/issues/1183,,1183,closed,2024-11-04 11:08:25+00:00,2024-11-11 10:14:23+00:00,2024-11-11 10:14:21+00:00,"Bumps [install-pinned/uv](https://github.com/install-pinned/uv) from de03c60d508703a83d3f8f49afcf1249590ecda1 to 079b3722376f674a95014281a9210848aa6c2f34.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/install-pinned/uv/commit/079b3722376f674a95014281a9210848aa6c2f34""><code>079b372</code></a> update README.md (uv 0.4.29)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/82e203f79f1ae6edb04a292df4f1c80c28302704""><code>82e203f</code></a> update pins (uv 0.4.29)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/5323b902eae0f36bc0abac26a4ac330bdfc36296""><code>5323b90</code></a> update README.md (uv 0.4.28)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/52bf95a91123f9849d76f95f62a757876e002996""><code>52bf95a</code></a> update pins (uv 0.4.28)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/dd25a3ae8b9f2dda4b22c3a56fe1e5aa2738bbb5""><code>dd25a3a</code></a> update README.md (uv 0.4.27)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/a6d62be15c797cd948385d7dcc088d788a9a418c""><code>a6d62be</code></a> update pins (uv 0.4.27)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/fb62b34d175c1210b527f92ae7ccc9490888c42e""><code>fb62b34</code></a> update README.md (uv 0.4.26)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/8dc03f3bbebfcdca2635078aedd99578539c5e59""><code>8dc03f3</code></a> update pins (uv 0.4.26)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/40730cd7cf5841eb579e9e8b2dde68057803138e""><code>40730cd</code></a> update README.md (uv 0.4.25)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/decb989675525b33f26897317ba4f2819202eac3""><code>decb989</code></a> update pins (uv 0.4.25)</li>
<li>Additional commits viewable in <a href=""https://github.com/install-pinned/uv/compare/de03c60d508703a83d3f8f49afcf1249590ecda1...079b3722376f674a95014281a9210848aa6c2f34"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],2
9,Bump grpcio from 1.62.2 to 1.67.1 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1182,,1182,closed,2024-11-04 10:25:41+00:00,2024-11-18 11:25:00+00:00,2024-11-18 11:24:58+00:00,"Bumps [grpcio](https://github.com/grpc/grpc) from 1.62.2 to 1.67.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/grpc/grpc/releases"">grpcio's releases</a>.</em></p>
<blockquote>
<h2>Release v1.67.1</h2>
<p>This is release gRPC Core 1.67.1 (gesundheit). This is a Python-only patch release.</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes.</p>
<h2>Release v1.67.0</h2>
<p>This is release 1.67.0 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">gesundheit</a>) of gRPC Core.</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>
<h2>Core</h2>
<ul>
<li>[ruby] reduce an INFO log to DEBUG (backport <a href=""https://redirect.github.com/grpc/grpc/pull/37633"">grpc/grpc#37633</a>). (<a href=""https://redirect.github.com/grpc/grpc/pull/37686"">#37686</a>)</li>
<li>[release] Bump core version to 44.0.0. (<a href=""https://redirect.github.com/grpc/grpc/pull/37661"">#37661</a>)</li>
<li>[RlsLB] Fix Deadlock. (<a href=""https://redirect.github.com/grpc/grpc/pull/37459"">#37459</a>)</li>
<li>[Python Otel] Manage call tracer life cycle use call arena. (<a href=""https://redirect.github.com/grpc/grpc/pull/37460"">#37460</a>)</li>
</ul>
<h2>C++</h2>
<ul>
<li>[OTel C++] Fix race when adding and removing callbacks. (<a href=""https://redirect.github.com/grpc/grpc/pull/37485"">#37485</a>)</li>
</ul>
<h2>Python</h2>
<ul>
<li>[Backport to 1.67.x] Add templating and support for Python 3.13 (<a href=""https://redirect.github.com/grpc/grpc/issues/37643"">#37643</a>). (<a href=""https://redirect.github.com/grpc/grpc/pull/37803"">#37803</a>)</li>
<li>Add templating and support for Python 3.13. (<a href=""https://redirect.github.com/grpc/grpc/pull/37643"">#37643</a>)</li>
<li>[Python Distrib] Change warning to RuntimeError for version incompatibility. (<a href=""https://redirect.github.com/grpc/grpc/pull/37466"">#37466</a>)</li>
<li>[reflection]: python: reflection returns <code>original_request</code>. (<a href=""https://redirect.github.com/grpc/grpc/pull/36944"">#36944</a>)</li>
</ul>
<h2>Ruby</h2>
<ul>
<li>[ruby] drop ruby 2.7 support. (<a href=""https://redirect.github.com/grpc/grpc/pull/37430"">#37430</a>)</li>
<li>[ruby] reduce ruby gpr_log invocations from INFO to DEBUG. (<a href=""https://redirect.github.com/grpc/grpc/pull/37426"">#37426</a>)</li>
<li>[ruby] refactor flaky test and expose cancel_with_status. (<a href=""https://redirect.github.com/grpc/grpc/pull/37410"">#37410</a>)</li>
</ul>
<h2>Release v1.67.0-pre1</h2>
<p>This is a prerelease of gRPC Core 1.67.0 (gesundheit).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This prerelease contains refinements, improvements, and bug fixes.</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/grpc/grpc/commit/d3286610f703a339149c3f9be69f0d7d0abb130a""><code>d328661</code></a> [Release] Bump version to 1.67.1 (on v1.67.x branch) (<a href=""https://redirect.github.com/grpc/grpc/issues/38011"">#38011</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/fb19ed1563432c687e882b5fda72ef2c093ac3ae""><code>fb19ed1</code></a> [Backport] Adding <code>noexcept</code> to required Cython functions to fix potential de...</li>
<li><a href=""https://github.com/grpc/grpc/commit/74f245857247b4b3e28a753d85d06ae2d5a55434""><code>74f2458</code></a> [release] Bump release version to v1.67.0 (<a href=""https://redirect.github.com/grpc/grpc/issues/37846"">#37846</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/2139e88bd658be9936d4ca7a3d3f8c38be0be08e""><code>2139e88</code></a> [release] backport &quot;Handle backport PRs without piper info in release notes a...</li>
<li><a href=""https://github.com/grpc/grpc/commit/7e2ea897e425c87888532e84fcb3bf75c3459652""><code>7e2ea89</code></a> increased timeout for armv7 artifact build to 2 hours (<a href=""https://redirect.github.com/grpc/grpc/issues/37807"">#37807</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/fb22b9805ed46118a4f27ddd85c3f0b13c8a4308""><code>fb22b98</code></a> Merge branch 'v1.67.x' of <a href=""https://www.github.com/grpc/grpc"">https://www.github.com/grpc/grpc</a> into v1.67.x</li>
<li><a href=""https://github.com/grpc/grpc/commit/bcfee53b27161324b255ff4d322c75143db4acff""><code>bcfee53</code></a> [Backport to 1.67.x] Add templating and support for Python 3.13 (<a href=""https://redirect.github.com/grpc/grpc/issues/37643"">#37643</a>) (<a href=""https://redirect.github.com/grpc/grpc/issues/37"">#37</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/a4fd219d9d1f42ccc5571aee11e7dd17e6d1a820""><code>a4fd219</code></a> Add templating and support for Python 3.13 (<a href=""https://redirect.github.com/grpc/grpc/issues/37643"">#37643</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/1eb5673cdd2471cfa8f68dbe1123c35e9ee03574""><code>1eb5673</code></a> [objc] backport <a href=""https://redirect.github.com/grpc/grpc/pull/37690"">grpc/grpc#37690</a> to v1.67.x (<a href=""https://redirect.github.com/grpc/grpc/issues/37712"">#37712</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/ace22e307d1bb2f81420cd55a81b2725f50c894b""><code>ace22e3</code></a> [ruby] reduce an INFO log to DEBUG (backport <a href=""https://github.com/grpc/grpc/pul"">https://github.com/grpc/grpc/pul</a>...</li>
<li>Additional commits viewable in <a href=""https://github.com/grpc/grpc/compare/v1.62.2...v1.67.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=grpcio&package-manager=pip&previous-version=1.62.2&new-version=1.67.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],1
10,Update hustle.py,https://api.github.com/repos/move-coop/parsons/issues/1181,,1181,open,2024-10-31 19:04:42+00:00,2024-10-31 20:40:07+00:00,,"This function currently appends the results of pagination to the end of a list, resulting in lists of lists when pulling data with results greater than PAGE_LIMIT (1000). The function should instead join lists.

I'm using the '+' operator, but the '.extend()' method could also be used.",[],[],0
11,parsons[VAN] : Add NGP Introspection Endpoint,https://api.github.com/repos/move-coop/parsons/issues/1180,Ruby Engelhart,1180,open,2024-10-30 14:19:08+00:00,2024-10-30 16:09:31+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
I recently have come across this tool https://docs.ngpvan.com/reference/introspection by NGP VAN that has helped us check both the connection and the key permissions. It has been helpful when checking if a key has access to MyCampaigns, but not MyVoters, for instance. 

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->
Add Introspection call to Parsons API. This method would take in the username and API Key, handle the conversion to base64, and make the introspection call to VAN. The response would return the JSON object, regardless of success or failure. 

## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
This is important because it allows for a quick check of connection and permissions within a given application.

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->
The script on the VAN Documentation suffices for my local setup (with some tweaks), but am not super familiar with the this repository.
```python
import requests

url = ""https://api.securevan.com/v4/apiKeyProfiles""

headers = {
     ""accept"": ""application/json"",
     ""authorization"": ""Basic ****some base64 string ==****""
}

response = requests.get(url, headers=headers)

print(response.text)
```
Response:
```JSON
{
    ""items"": [
        {
            ""databaseName"": ""SmartVAN Massachusetts"",
            ""hasMyVoters"": true,
            ""hasMyCampaign"": true,
            ""committeeName"": ""People for Good"",
            ""apiKeyTypeName"": ""Custom Integration"",
            ""keyReference"": ""1234"",
            ""userFirstName"": ""peopleforgood"",
            ""userLastName"": ""api"",
            ""username"": ""peopleforgood.api"",
          	""userId"": 4321
        }
    ],
    ""nextPageLink"": null,
    ""count"": 1
}
```

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
Low priority, but helpful none the less (:","[Label(name=""enhancement"")]",[],1
12,Bump install-pinned/uv from de03c60d508703a83d3f8f49afcf1249590ecda1 to dd25a3ae8b9f2dda4b22c3a56fe1e5aa2738bbb5,https://api.github.com/repos/move-coop/parsons/issues/1179,,1179,closed,2024-10-28 10:32:30+00:00,2024-11-04 11:08:30+00:00,2024-11-04 11:08:28+00:00,"Bumps [install-pinned/uv](https://github.com/install-pinned/uv) from de03c60d508703a83d3f8f49afcf1249590ecda1 to dd25a3ae8b9f2dda4b22c3a56fe1e5aa2738bbb5.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/install-pinned/uv/commit/dd25a3ae8b9f2dda4b22c3a56fe1e5aa2738bbb5""><code>dd25a3a</code></a> update README.md (uv 0.4.27)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/a6d62be15c797cd948385d7dcc088d788a9a418c""><code>a6d62be</code></a> update pins (uv 0.4.27)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/fb62b34d175c1210b527f92ae7ccc9490888c42e""><code>fb62b34</code></a> update README.md (uv 0.4.26)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/8dc03f3bbebfcdca2635078aedd99578539c5e59""><code>8dc03f3</code></a> update pins (uv 0.4.26)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/40730cd7cf5841eb579e9e8b2dde68057803138e""><code>40730cd</code></a> update README.md (uv 0.4.25)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/decb989675525b33f26897317ba4f2819202eac3""><code>decb989</code></a> update pins (uv 0.4.25)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/e185bcabc5c58ba528e209ce57aad4134e4666bb""><code>e185bca</code></a> update README.md (uv 0.4.24)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/800baab89894e85b8594f9d9dfabb790fc5463b4""><code>800baab</code></a> update pins (uv 0.4.24)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/fae5103f9aa7ff8372d3af5640782b7b426c94ad""><code>fae5103</code></a> update README.md (uv 0.4.23)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/39fe67eea0a51991a768829b9151dbea595e57f4""><code>39fe67e</code></a> update pins (uv 0.4.23)</li>
<li>Additional commits viewable in <a href=""https://github.com/install-pinned/uv/compare/de03c60d508703a83d3f8f49afcf1249590ecda1...dd25a3ae8b9f2dda4b22c3a56fe1e5aa2738bbb5"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],1
13,Bump actions/checkout from 4.2.1 to 4.2.2,https://api.github.com/repos/move-coop/parsons/issues/1178,,1178,open,2024-10-28 10:32:22+00:00,2024-10-28 10:32:23+00:00,,"Bumps [actions/checkout](https://github.com/actions/checkout) from 4.2.1 to 4.2.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/actions/checkout/releases"">actions/checkout's releases</a>.</em></p>
<blockquote>
<h2>v4.2.2</h2>
<h2>What's Changed</h2>
<ul>
<li><code>url-helper.ts</code> now leverages well-known environment variables by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1941"">actions/checkout#1941</a></li>
<li>Expand unit test coverage for <code>isGhes</code> by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1946"">actions/checkout#1946</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/actions/checkout/compare/v4.2.1...v4.2.2"">https://github.com/actions/checkout/compare/v4.2.1...v4.2.2</a></p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/actions/checkout/blob/main/CHANGELOG.md"">actions/checkout's changelog</a>.</em></p>
<blockquote>
<h1>Changelog</h1>
<h2>v4.2.2</h2>
<ul>
<li><code>url-helper.ts</code> now leverages well-known environment variables by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1941"">actions/checkout#1941</a></li>
<li>Expand unit test coverage for <code>isGhes</code> by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1946"">actions/checkout#1946</a></li>
</ul>
<h2>v4.2.1</h2>
<ul>
<li>Check out other refs/* by commit if provided, fall back to ref by <a href=""https://github.com/orhantoy""><code>@​orhantoy</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1924"">actions/checkout#1924</a></li>
</ul>
<h2>v4.2.0</h2>
<ul>
<li>Add Ref and Commit outputs by <a href=""https://github.com/lucacome""><code>@​lucacome</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1180"">actions/checkout#1180</a></li>
<li>Dependency updates by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a>- <a href=""https://redirect.github.com/actions/checkout/pull/1777"">actions/checkout#1777</a>, <a href=""https://redirect.github.com/actions/checkout/pull/1872"">actions/checkout#1872</a></li>
</ul>
<h2>v4.1.7</h2>
<ul>
<li>Bump the minor-npm-dependencies group across 1 directory with 4 updates by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1739"">actions/checkout#1739</a></li>
<li>Bump actions/checkout from 3 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1697"">actions/checkout#1697</a></li>
<li>Check out other refs/* by commit by <a href=""https://github.com/orhantoy""><code>@​orhantoy</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1774"">actions/checkout#1774</a></li>
<li>Pin actions/checkout's own workflows to a known, good, stable version. by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1776"">actions/checkout#1776</a></li>
</ul>
<h2>v4.1.6</h2>
<ul>
<li>Check platform to set archive extension appropriately by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1732"">actions/checkout#1732</a></li>
</ul>
<h2>v4.1.5</h2>
<ul>
<li>Update NPM dependencies by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1703"">actions/checkout#1703</a></li>
<li>Bump github/codeql-action from 2 to 3 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1694"">actions/checkout#1694</a></li>
<li>Bump actions/setup-node from 1 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1696"">actions/checkout#1696</a></li>
<li>Bump actions/upload-artifact from 2 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1695"">actions/checkout#1695</a></li>
<li>README: Suggest <code>user.email</code> to be <code>41898282+github-actions[bot]@users.noreply.github.com</code> by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1707"">actions/checkout#1707</a></li>
</ul>
<h2>v4.1.4</h2>
<ul>
<li>Disable <code>extensions.worktreeConfig</code> when disabling <code>sparse-checkout</code> by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1692"">actions/checkout#1692</a></li>
<li>Add dependabot config by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1688"">actions/checkout#1688</a></li>
<li>Bump the minor-actions-dependencies group with 2 updates by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1693"">actions/checkout#1693</a></li>
<li>Bump word-wrap from 1.2.3 to 1.2.5 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1643"">actions/checkout#1643</a></li>
</ul>
<h2>v4.1.3</h2>
<ul>
<li>Check git version before attempting to disable <code>sparse-checkout</code> by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1656"">actions/checkout#1656</a></li>
<li>Add SSH user parameter by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1685"">actions/checkout#1685</a></li>
<li>Update <code>actions/checkout</code> version in <code>update-main-version.yml</code> by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1650"">actions/checkout#1650</a></li>
</ul>
<h2>v4.1.2</h2>
<ul>
<li>Fix: Disable sparse checkout whenever <code>sparse-checkout</code> option is not present <a href=""https://github.com/dscho""><code>@​dscho</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1598"">actions/checkout#1598</a></li>
</ul>
<h2>v4.1.1</h2>
<ul>
<li>Correct link to GitHub Docs by <a href=""https://github.com/peterbe""><code>@​peterbe</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1511"">actions/checkout#1511</a></li>
<li>Link to release page from what's new section by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1514"">actions/checkout#1514</a></li>
</ul>
<h2>v4.1.0</h2>
<ul>
<li><a href=""https://redirect.github.com/actions/checkout/pull/1396"">Add support for partial checkout filters</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/actions/checkout/commit/11bd71901bbe5b1630ceea73d27597364c9af683""><code>11bd719</code></a> Prepare 4.2.2 Release (<a href=""https://redirect.github.com/actions/checkout/issues/1953"">#1953</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/e3d2460bbb42d7710191569f88069044cfb9d8cf""><code>e3d2460</code></a> Expand unit test coverage (<a href=""https://redirect.github.com/actions/checkout/issues/1946"">#1946</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/163217dfcd28294438ea1c1c149cfaf66eec283e""><code>163217d</code></a> <code>url-helper.ts</code> now leverages well-known environment variables. (<a href=""https://redirect.github.com/actions/checkout/issues/1941"">#1941</a>)</li>
<li>See full diff in <a href=""https://github.com/actions/checkout/compare/eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871...11bd71901bbe5b1630ceea73d27597364c9af683"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/checkout&package-manager=github_actions&previous-version=4.2.1&new-version=4.2.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
14,Bump github/codeql-action from 3.26.12 to 3.27.0,https://api.github.com/repos/move-coop/parsons/issues/1177,,1177,closed,2024-10-28 10:32:17+00:00,2024-11-11 10:14:16+00:00,2024-11-11 10:14:14+00:00,"Bumps [github/codeql-action](https://github.com/github/codeql-action) from 3.26.12 to 3.27.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/releases"">github/codeql-action's releases</a>.</em></p>
<blockquote>
<h2>v3.27.0</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.0 - 22 Oct 2024</h2>
<ul>
<li>Bump the minimum CodeQL bundle version to 2.14.6. <a href=""https://redirect.github.com/github/codeql-action/pull/2549"">#2549</a></li>
<li>Fix an issue where the <code>upload-sarif</code> Action would fail with &quot;upload-sarif post-action step failed: Input required and not supplied: token&quot; when called in a composite Action that had a different set of inputs to the ones expected by the <code>upload-sarif</code> Action. <a href=""https://redirect.github.com/github/codeql-action/pull/2557"">#2557</a></li>
<li>Update default CodeQL bundle version to 2.19.2. <a href=""https://redirect.github.com/github/codeql-action/pull/2552"">#2552</a></li>
</ul>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.0/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/blob/main/CHANGELOG.md"">github/codeql-action's changelog</a>.</em></p>
<blockquote>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>[UNRELEASED]</h2>
<p>No user facing changes.</p>
<h2>3.27.0 - 22 Oct 2024</h2>
<ul>
<li>Bump the minimum CodeQL bundle version to 2.14.6. <a href=""https://redirect.github.com/github/codeql-action/pull/2549"">#2549</a></li>
<li>Fix an issue where the <code>upload-sarif</code> Action would fail with &quot;upload-sarif post-action step failed: Input required and not supplied: token&quot; when called in a composite Action that had a different set of inputs to the ones expected by the <code>upload-sarif</code> Action. <a href=""https://redirect.github.com/github/codeql-action/pull/2557"">#2557</a></li>
<li>Update default CodeQL bundle version to 2.19.2. <a href=""https://redirect.github.com/github/codeql-action/pull/2552"">#2552</a></li>
</ul>
<h2>3.26.13 - 14 Oct 2024</h2>
<p>No user facing changes.</p>
<h2>3.26.12 - 07 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add a deprecation warning for customers using CodeQL version 2.14.5 and earlier. These versions of CodeQL were discontinued on 24 September 2024 alongside GitHub Enterprise Server 3.10, and will be unsupported by CodeQL Action versions 3.27.0 and later and versions 2.27.0 and later. <a href=""https://redirect.github.com/github/codeql-action/pull/2520"">#2520</a></p>
<ul>
<li>
<p>If you are using one of these versions, please update to CodeQL CLI version 2.14.6 or later. For instance, if you have specified a custom version of the CLI using the 'tools' input to the 'init' Action, you can remove this input to use the default version.</p>
</li>
<li>
<p>Alternatively, if you want to continue using a version of the CodeQL CLI between 2.13.5 and 2.14.5, you can replace <code>github/codeql-action/*@v3</code> by <code>github/codeql-action/*@v3.26.11</code> and <code>github/codeql-action/*@v2</code> by <code>github/codeql-action/*@v2.26.11</code> in your code scanning workflow to ensure you continue using this version of the CodeQL Action.</p>
</li>
</ul>
</li>
</ul>
<h2>3.26.11 - 03 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add support for using <code>actions/download-artifact@v4</code> to programmatically consume CodeQL Action debug artifacts.</p>
<p>Starting November 30, 2024, GitHub.com customers will <a href=""https://github.blog/changelog/2024-04-16-deprecation-notice-v3-of-the-artifact-actions/"">no longer be able to use <code>actions/download-artifact@v3</code></a>. Therefore, to avoid breakage, customers who programmatically download the CodeQL Action debug artifacts should set the <code>CODEQL_ACTION_ARTIFACT_V4_UPGRADE</code> environment variable to <code>true</code> and bump <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> in their workflows. The CodeQL Action will enable this behavior by default in early November and workflows that have not yet bumped to <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> will begin failing then.</p>
<p>This change is currently unavailable for GitHub Enterprise Server customers, as <code>actions/upload-artifact@v4</code> and <code>actions/download-artifact@v4</code> are not yet compatible with GHES.</p>
</li>
<li>
<p>Update default CodeQL bundle version to 2.19.1. <a href=""https://redirect.github.com/github/codeql-action/pull/2519"">#2519</a></p>
</li>
</ul>
<h2>3.26.10 - 30 Sep 2024</h2>
<ul>
<li>We are rolling out a feature in September/October 2024 that sets up CodeQL using a bundle compressed with <a href=""http://facebook.github.io/zstd/"">Zstandard</a>. Our aim is to improve the performance of setting up CodeQL. <a href=""https://redirect.github.com/github/codeql-action/pull/2502"">#2502</a></li>
</ul>
<h2>3.26.9 - 24 Sep 2024</h2>
<p>No user facing changes.</p>
<h2>3.26.8 - 19 Sep 2024</h2>
<ul>
<li>Update default CodeQL bundle version to 2.19.0. <a href=""https://redirect.github.com/github/codeql-action/pull/2483"">#2483</a></li>
</ul>
<h2>3.26.7 - 13 Sep 2024</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/github/codeql-action/commit/662472033e021d55d94146f66f6058822b0b39fd""><code>6624720</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2561"">#2561</a> from github/update-v3.27.0-b35b023d9</li>
<li><a href=""https://github.com/github/codeql-action/commit/ce7c2b560da6747133b72eb2f33503a6c4da9c15""><code>ce7c2b5</code></a> Update changelog for v3.27.0</li>
<li><a href=""https://github.com/github/codeql-action/commit/b35b023d9b1296658ca1bcb95dcd0336f9d23f0b""><code>b35b023</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2552"">#2552</a> from github/update-bundle/codeql-bundle-v2.19.2</li>
<li><a href=""https://github.com/github/codeql-action/commit/dafc762411c1755f00abee84283eaa85d438af2e""><code>dafc762</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2560"">#2560</a> from github/aeisenberg/fix-required-checks</li>
<li><a href=""https://github.com/github/codeql-action/commit/0d1eb88b60733b6720210d591c48ebc9e961d42c""><code>0d1eb88</code></a> Remove ESLint from required checks</li>
<li><a href=""https://github.com/github/codeql-action/commit/0a30541440699f21b772e5ef95c912f097514855""><code>0a30541</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2558"">#2558</a> from github/dependabot/npm_and_yarn/npm-6515e6e328</li>
<li><a href=""https://github.com/github/codeql-action/commit/2a6a6ad1c809216132b8a6a8c1f5873fff3f400a""><code>2a6a6ad</code></a> Update checked-in dependencies</li>
<li><a href=""https://github.com/github/codeql-action/commit/26c18c2c1f382ccd402aec5dd18d9ff6d0097891""><code>26c18c2</code></a> Bump the npm group with 3 updates</li>
<li><a href=""https://github.com/github/codeql-action/commit/7080a68cbca9319f3cf869f12f34acea853ad72d""><code>7080a68</code></a> Merge branch 'main' into update-bundle/codeql-bundle-v2.19.2</li>
<li><a href=""https://github.com/github/codeql-action/commit/63eb7bbf1f65c025536b3e7d083d89c1c161043c""><code>63eb7bb</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2551"">#2551</a> from github/cklin/diff-informed-queries-feature</li>
<li>Additional commits viewable in <a href=""https://github.com/github/codeql-action/compare/c36620d31ac7c881962c3d9dd939c40ec9434f2b...662472033e021d55d94146f66f6058822b0b39fd"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=github/codeql-action&package-manager=github_actions&previous-version=3.26.12&new-version=3.27.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],1
15,Bump actions/setup-python from 5.2.0 to 5.3.0,https://api.github.com/repos/move-coop/parsons/issues/1176,,1176,open,2024-10-28 10:32:11+00:00,2024-10-28 10:32:12+00:00,,"Bumps [actions/setup-python](https://github.com/actions/setup-python) from 5.2.0 to 5.3.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/actions/setup-python/releases"">actions/setup-python's releases</a>.</em></p>
<blockquote>
<h2>v5.3.0</h2>
<h2>What's Changed</h2>
<ul>
<li>Add workflow file for publishing releases to immutable action package by <a href=""https://github.com/Jcambass""><code>@​Jcambass</code></a> in <a href=""https://redirect.github.com/actions/setup-python/pull/941"">actions/setup-python#941</a></li>
<li>Upgrade IA publish by <a href=""https://github.com/Jcambass""><code>@​Jcambass</code></a> in <a href=""https://redirect.github.com/actions/setup-python/pull/943"">actions/setup-python#943</a></li>
</ul>
<h3>Bug Fixes:</h3>
<ul>
<li>Normalise Line Endings to Ensure Cross-Platform Consistency by <a href=""https://github.com/priya-kinthali""><code>@​priya-kinthali</code></a> in <a href=""https://redirect.github.com/actions/setup-python/pull/938"">actions/setup-python#938</a></li>
<li>Revise <code>isGhes</code> logic by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/setup-python/pull/963"">actions/setup-python#963</a></li>
<li>Bump pillow from 7.2 to 10.2.0 by <a href=""https://github.com/aparnajyothi-y""><code>@​aparnajyothi-y</code></a> in <a href=""https://redirect.github.com/actions/setup-python/pull/956"">actions/setup-python#956</a></li>
</ul>
<h3>Enhancements:</h3>
<ul>
<li>Enhance workflows and documentation updates by <a href=""https://github.com/priya-kinthali""><code>@​priya-kinthali</code></a> in <a href=""https://redirect.github.com/actions/setup-python/pull/965"">actions/setup-python#965</a></li>
<li>Bump default versions to latest by <a href=""https://github.com/jeffwidman""><code>@​jeffwidman</code></a> in <a href=""https://redirect.github.com/actions/setup-python/pull/905"">actions/setup-python#905</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/Jcambass""><code>@​Jcambass</code></a> made their first contribution in <a href=""https://redirect.github.com/actions/setup-python/pull/941"">actions/setup-python#941</a></li>
<li><a href=""https://github.com/jww3""><code>@​jww3</code></a> made their first contribution in <a href=""https://redirect.github.com/actions/setup-python/pull/963"">actions/setup-python#963</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/actions/setup-python/compare/v5...v5.3.0"">https://github.com/actions/setup-python/compare/v5...v5.3.0</a></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/actions/setup-python/commit/0b93645e9fea7318ecaed2b359559ac225c90a2b""><code>0b93645</code></a> Enhance workflows: Add macOS 13 support, upgrade publish-action, and update d...</li>
<li><a href=""https://github.com/actions/setup-python/commit/9c76e716502b18322365741b762fee22a8cffad8""><code>9c76e71</code></a> Bump pillow from 7.2 to 10.2.0 in /<strong>tests</strong>/data  (<a href=""https://redirect.github.com/actions/setup-python/issues/956"">#956</a>)</li>
<li><a href=""https://github.com/actions/setup-python/commit/f4c5a1183d69690d31b6304b7af403a5b56a88d6""><code>f4c5a11</code></a> Revise <code>isGhes</code> logic (<a href=""https://redirect.github.com/actions/setup-python/issues/963"">#963</a>)</li>
<li><a href=""https://github.com/actions/setup-python/commit/19dfb7b659fa9e60c2f89c33335ab5f6f1792b6e""><code>19dfb7b</code></a> Bump default versions to latest (<a href=""https://redirect.github.com/actions/setup-python/issues/905"">#905</a>)</li>
<li><a href=""https://github.com/actions/setup-python/commit/e9675cc634901ff55d92c575ecd6945e65464b00""><code>e9675cc</code></a> Merge pull request <a href=""https://redirect.github.com/actions/setup-python/issues/943"">#943</a> from actions/Jcambass-patch-1</li>
<li><a href=""https://github.com/actions/setup-python/commit/3226af69c08a4851edf81cffc8849d2db148b21f""><code>3226af6</code></a> Upgrade IA publish</li>
<li><a href=""https://github.com/actions/setup-python/commit/70dcb22d269dc9546a5d97f4b11548f130526421""><code>70dcb22</code></a> Merge pull request <a href=""https://redirect.github.com/actions/setup-python/issues/941"">#941</a> from actions/Jcambass-patch-1</li>
<li><a href=""https://github.com/actions/setup-python/commit/65b48c71155ac3186106d8d8de14787f5914b8d1""><code>65b48c7</code></a> Create publish-immutable-actions.yml</li>
<li><a href=""https://github.com/actions/setup-python/commit/29a37be0a3d3e8bf5bc1eb19cd0502922f5b312a""><code>29a37be</code></a> initial commit (<a href=""https://redirect.github.com/actions/setup-python/issues/938"">#938</a>)</li>
<li>See full diff in <a href=""https://github.com/actions/setup-python/compare/f677139bbe7f9c59b41e40162b753c062f5d49a3...0b93645e9fea7318ecaed2b359559ac225c90a2b"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/setup-python&package-manager=github_actions&previous-version=5.2.0&new-version=5.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
16,Bump install-pinned/uv from de03c60d508703a83d3f8f49afcf1249590ecda1 to e185bcabc5c58ba528e209ce57aad4134e4666bb,https://api.github.com/repos/move-coop/parsons/issues/1175,,1175,closed,2024-10-21 11:12:29+00:00,2024-10-28 10:32:35+00:00,2024-10-28 10:32:33+00:00,"Bumps [install-pinned/uv](https://github.com/install-pinned/uv) from de03c60d508703a83d3f8f49afcf1249590ecda1 to e185bcabc5c58ba528e209ce57aad4134e4666bb.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/install-pinned/uv/commit/e185bcabc5c58ba528e209ce57aad4134e4666bb""><code>e185bca</code></a> update README.md (uv 0.4.24)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/800baab89894e85b8594f9d9dfabb790fc5463b4""><code>800baab</code></a> update pins (uv 0.4.24)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/fae5103f9aa7ff8372d3af5640782b7b426c94ad""><code>fae5103</code></a> update README.md (uv 0.4.23)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/39fe67eea0a51991a768829b9151dbea595e57f4""><code>39fe67e</code></a> update pins (uv 0.4.23)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/8bbba5ed4ca625fc9c4a9f5ab7999d3498e27eff""><code>8bbba5e</code></a> update README.md (uv 0.4.22)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/d6ab1080290de21be37f5c325ed245280a1287b7""><code>d6ab108</code></a> update pins (uv 0.4.22)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/05ff65c930ff9a1dcdcbad479d892c1d50b2a820""><code>05ff65c</code></a> update README.md (uv 0.4.21)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/27e7fd0b99161e4711c890b9a2ea9504099e548e""><code>27e7fd0</code></a> update pins (uv 0.4.21)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/03a68782c27167b267490a9392ac24d6b93a2f14""><code>03a6878</code></a> update README.md (uv 0.4.20)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/6a4598757e2d107c4be3779f53edfe37bbb5714c""><code>6a45987</code></a> update pins (uv 0.4.20)</li>
<li>Additional commits viewable in <a href=""https://github.com/install-pinned/uv/compare/de03c60d508703a83d3f8f49afcf1249590ecda1...e185bcabc5c58ba528e209ce57aad4134e4666bb"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],1
17,Bump github/codeql-action from 3.26.12 to 3.26.13,https://api.github.com/repos/move-coop/parsons/issues/1174,,1174,closed,2024-10-21 11:12:25+00:00,2024-10-28 10:32:22+00:00,2024-10-28 10:32:20+00:00,"Bumps [github/codeql-action](https://github.com/github/codeql-action) from 3.26.12 to 3.26.13.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/blob/main/CHANGELOG.md"">github/codeql-action's changelog</a>.</em></p>
<blockquote>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>[UNRELEASED]</h2>
<ul>
<li>Bump the minimum CodeQL bundle version to 2.14.6. <a href=""https://redirect.github.com/github/codeql-action/pull/2549"">#2549</a></li>
</ul>
<h2>3.26.13 - 14 Oct 2024</h2>
<p>No user facing changes.</p>
<h2>3.26.12 - 07 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add a deprecation warning for customers using CodeQL version 2.14.5 and earlier. These versions of CodeQL were discontinued on 24 September 2024 alongside GitHub Enterprise Server 3.10, and will be unsupported by CodeQL Action versions 3.27.0 and later and versions 2.27.0 and later. <a href=""https://redirect.github.com/github/codeql-action/pull/2520"">#2520</a></p>
<ul>
<li>
<p>If you are using one of these versions, please update to CodeQL CLI version 2.14.6 or later. For instance, if you have specified a custom version of the CLI using the 'tools' input to the 'init' Action, you can remove this input to use the default version.</p>
</li>
<li>
<p>Alternatively, if you want to continue using a version of the CodeQL CLI between 2.13.5 and 2.14.5, you can replace <code>github/codeql-action/*@v3</code> by <code>github/codeql-action/*@v3.26.11</code> and <code>github/codeql-action/*@v2</code> by <code>github/codeql-action/*@v2.26.11</code> in your code scanning workflow to ensure you continue using this version of the CodeQL Action.</p>
</li>
</ul>
</li>
</ul>
<h2>3.26.11 - 03 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add support for using <code>actions/download-artifact@v4</code> to programmatically consume CodeQL Action debug artifacts.</p>
<p>Starting November 30, 2024, GitHub.com customers will <a href=""https://github.blog/changelog/2024-04-16-deprecation-notice-v3-of-the-artifact-actions/"">no longer be able to use <code>actions/download-artifact@v3</code></a>. Therefore, to avoid breakage, customers who programmatically download the CodeQL Action debug artifacts should set the <code>CODEQL_ACTION_ARTIFACT_V4_UPGRADE</code> environment variable to <code>true</code> and bump <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> in their workflows. The CodeQL Action will enable this behavior by default in early November and workflows that have not yet bumped to <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> will begin failing then.</p>
<p>This change is currently unavailable for GitHub Enterprise Server customers, as <code>actions/upload-artifact@v4</code> and <code>actions/download-artifact@v4</code> are not yet compatible with GHES.</p>
</li>
<li>
<p>Update default CodeQL bundle version to 2.19.1. <a href=""https://redirect.github.com/github/codeql-action/pull/2519"">#2519</a></p>
</li>
</ul>
<h2>3.26.10 - 30 Sep 2024</h2>
<ul>
<li>We are rolling out a feature in September/October 2024 that sets up CodeQL using a bundle compressed with <a href=""http://facebook.github.io/zstd/"">Zstandard</a>. Our aim is to improve the performance of setting up CodeQL. <a href=""https://redirect.github.com/github/codeql-action/pull/2502"">#2502</a></li>
</ul>
<h2>3.26.9 - 24 Sep 2024</h2>
<p>No user facing changes.</p>
<h2>3.26.8 - 19 Sep 2024</h2>
<ul>
<li>Update default CodeQL bundle version to 2.19.0. <a href=""https://redirect.github.com/github/codeql-action/pull/2483"">#2483</a></li>
</ul>
<h2>3.26.7 - 13 Sep 2024</h2>
<ul>
<li>Update default CodeQL bundle version to 2.18.4. <a href=""https://redirect.github.com/github/codeql-action/pull/2471"">#2471</a></li>
</ul>
<h2>3.26.6 - 29 Aug 2024</h2>
<ul>
<li>Update default CodeQL bundle version to 2.18.3. <a href=""https://redirect.github.com/github/codeql-action/pull/2449"">#2449</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/github/codeql-action/commit/f779452ac5af1c261dce0346a8f964149f49322b""><code>f779452</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2539"">#2539</a> from github/update-v3.26.13-0c3e00641</li>
<li><a href=""https://github.com/github/codeql-action/commit/532932479b339a320b8f0f106cf17cb6f27625ab""><code>5329324</code></a> Update CHANGELOG.md</li>
<li><a href=""https://github.com/github/codeql-action/commit/007ba256485136f6179e827b998b3c9125fcbcee""><code>007ba25</code></a> Update changelog for v3.26.13</li>
<li><a href=""https://github.com/github/codeql-action/commit/0c3e0064168c8e68650d4d5d2eaa7f5b14c2e089""><code>0c3e006</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2536"">#2536</a> from yoff/python/ff-std-lib-extraction</li>
<li><a href=""https://github.com/github/codeql-action/commit/38469af22844c7bb1302fd0bde937f95fae05417""><code>38469af</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2537"">#2537</a> from github/henrymercer/no-zstd-windows</li>
<li><a href=""https://github.com/github/codeql-action/commit/5b6984ee4d8d4337aa84b946951459213dd36daf""><code>5b6984e</code></a> Assert that Windows downloads gzip</li>
<li><a href=""https://github.com/github/codeql-action/commit/eefb943f7e5f85e6e26fff781c173475ecfe9f4f""><code>eefb943</code></a> Don't use Zstandard bundles on Windows</li>
<li><a href=""https://github.com/github/codeql-action/commit/201e02efe22c245c1d4fc9b579583c1ef037dfe6""><code>201e02e</code></a> rebuild the action</li>
<li><a href=""https://github.com/github/codeql-action/commit/ce5f900bfc4f4f7d32fd4106942380d167a1e43c""><code>ce5f900</code></a> formatting</li>
<li><a href=""https://github.com/github/codeql-action/commit/65dd816de1c1c6fc94d0885d009da0db1545df07""><code>65dd816</code></a> remove unused import</li>
<li>Additional commits viewable in <a href=""https://github.com/github/codeql-action/compare/c36620d31ac7c881962c3d9dd939c40ec9434f2b...f779452ac5af1c261dce0346a8f964149f49322b"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=github/codeql-action&package-manager=github_actions&previous-version=3.26.12&new-version=3.26.13)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],1
18,Bump grpcio from 1.62.2 to 1.67.0 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1173,,1173,closed,2024-10-21 10:22:34+00:00,2024-11-04 10:25:45+00:00,2024-11-04 10:25:43+00:00,"Bumps [grpcio](https://github.com/grpc/grpc) from 1.62.2 to 1.67.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/grpc/grpc/releases"">grpcio's releases</a>.</em></p>
<blockquote>
<h2>Release v1.67.0</h2>
<p>This is release 1.67.0 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">gesundheit</a>) of gRPC Core.</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>
<h2>Core</h2>
<ul>
<li>[ruby] reduce an INFO log to DEBUG (backport <a href=""https://redirect.github.com/grpc/grpc/pull/37633"">grpc/grpc#37633</a>). (<a href=""https://redirect.github.com/grpc/grpc/pull/37686"">#37686</a>)</li>
<li>[release] Bump core version to 44.0.0. (<a href=""https://redirect.github.com/grpc/grpc/pull/37661"">#37661</a>)</li>
<li>[RlsLB] Fix Deadlock. (<a href=""https://redirect.github.com/grpc/grpc/pull/37459"">#37459</a>)</li>
<li>[Python Otel] Manage call tracer life cycle use call arena. (<a href=""https://redirect.github.com/grpc/grpc/pull/37460"">#37460</a>)</li>
</ul>
<h2>C++</h2>
<ul>
<li>[OTel C++] Fix race when adding and removing callbacks. (<a href=""https://redirect.github.com/grpc/grpc/pull/37485"">#37485</a>)</li>
</ul>
<h2>Python</h2>
<ul>
<li>[Backport to 1.67.x] Add templating and support for Python 3.13 (<a href=""https://redirect.github.com/grpc/grpc/issues/37643"">#37643</a>). (<a href=""https://redirect.github.com/grpc/grpc/pull/37803"">#37803</a>)</li>
<li>Add templating and support for Python 3.13. (<a href=""https://redirect.github.com/grpc/grpc/pull/37643"">#37643</a>)</li>
<li>[Python Distrib] Change warning to RuntimeError for version incompatibility. (<a href=""https://redirect.github.com/grpc/grpc/pull/37466"">#37466</a>)</li>
<li>[reflection]: python: reflection returns <code>original_request</code>. (<a href=""https://redirect.github.com/grpc/grpc/pull/36944"">#36944</a>)</li>
</ul>
<h2>Ruby</h2>
<ul>
<li>[ruby] drop ruby 2.7 support. (<a href=""https://redirect.github.com/grpc/grpc/pull/37430"">#37430</a>)</li>
<li>[ruby] reduce ruby gpr_log invocations from INFO to DEBUG. (<a href=""https://redirect.github.com/grpc/grpc/pull/37426"">#37426</a>)</li>
<li>[ruby] refactor flaky test and expose cancel_with_status. (<a href=""https://redirect.github.com/grpc/grpc/pull/37410"">#37410</a>)</li>
</ul>
<h2>Release v1.67.0-pre1</h2>
<p>This is a prerelease of gRPC Core 1.67.0 (gesundheit).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This prerelease contains refinements, improvements, and bug fixes.</p>
<h2>Release v1.66.2</h2>
<p>This is release gRPC Core 1.66.2 (gladiator).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes.</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/grpc/grpc/commit/74f245857247b4b3e28a753d85d06ae2d5a55434""><code>74f2458</code></a> [release] Bump release version to v1.67.0 (<a href=""https://redirect.github.com/grpc/grpc/issues/37846"">#37846</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/2139e88bd658be9936d4ca7a3d3f8c38be0be08e""><code>2139e88</code></a> [release] backport &quot;Handle backport PRs without piper info in release notes a...</li>
<li><a href=""https://github.com/grpc/grpc/commit/7e2ea897e425c87888532e84fcb3bf75c3459652""><code>7e2ea89</code></a> increased timeout for armv7 artifact build to 2 hours (<a href=""https://redirect.github.com/grpc/grpc/issues/37807"">#37807</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/fb22b9805ed46118a4f27ddd85c3f0b13c8a4308""><code>fb22b98</code></a> Merge branch 'v1.67.x' of <a href=""https://www.github.com/grpc/grpc"">https://www.github.com/grpc/grpc</a> into v1.67.x</li>
<li><a href=""https://github.com/grpc/grpc/commit/bcfee53b27161324b255ff4d322c75143db4acff""><code>bcfee53</code></a> [Backport to 1.67.x] Add templating and support for Python 3.13 (<a href=""https://redirect.github.com/grpc/grpc/issues/37643"">#37643</a>) (<a href=""https://redirect.github.com/grpc/grpc/issues/37"">#37</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/a4fd219d9d1f42ccc5571aee11e7dd17e6d1a820""><code>a4fd219</code></a> Add templating and support for Python 3.13 (<a href=""https://redirect.github.com/grpc/grpc/issues/37643"">#37643</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/1eb5673cdd2471cfa8f68dbe1123c35e9ee03574""><code>1eb5673</code></a> [objc] backport <a href=""https://redirect.github.com/grpc/grpc/pull/37690"">grpc/grpc#37690</a> to v1.67.x (<a href=""https://redirect.github.com/grpc/grpc/issues/37712"">#37712</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/ace22e307d1bb2f81420cd55a81b2725f50c894b""><code>ace22e3</code></a> [ruby] reduce an INFO log to DEBUG (backport <a href=""https://github.com/grpc/grpc/pul"">https://github.com/grpc/grpc/pul</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/b44a18e2cc542732b18ae4a5e8e5c086c1fd7436""><code>b44a18e</code></a> [release] Bump release version on v1.67.x (<a href=""https://redirect.github.com/grpc/grpc/issues/37672"">#37672</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/c831e1de795e7f0a6c912be5f38ca352f6606c1a""><code>c831e1d</code></a> [flake] Fix tsan error surfaced by grpc_lb_end2end_test (<a href=""https://redirect.github.com/grpc/grpc/issues/37663"">#37663</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/grpc/grpc/compare/v1.62.2...v1.67.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=grpcio&package-manager=pip&previous-version=1.62.2&new-version=1.67.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],1
19,Bump gspread from 3.7.0 to 6.1.4 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1172,,1172,open,2024-10-21 10:19:53+00:00,2024-10-21 10:19:54+00:00,,"Bumps [gspread](https://github.com/burnash/gspread) from 3.7.0 to 6.1.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/burnash/gspread/releases"">gspread's releases</a>.</em></p>
<blockquote>
<h2>v6.1.4</h2>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.3...v6.1.4"">https://github.com/burnash/gspread/compare/v6.1.3...v6.1.4</a></p>
<h2>v6.1.3</h2>
<p>Bugfix to catch <code>JSONDecodeError</code> better from Google.</p>
<h2>What's Changed</h2>
<ul>
<li>ignore jinja CVE by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1481"">burnash/gspread#1481</a></li>
<li>Remove passing exception as args to super in APIError by <a href=""https://github.com/mike-flowers-airbnb""><code>@​mike-flowers-airbnb</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1477"">burnash/gspread#1477</a></li>
<li>better handler API error parsing. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1510"">burnash/gspread#1510</a></li>
<li>Add test on receiving an invalid JSON in the APIError exception handler. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1512"">burnash/gspread#1512</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/mike-flowers-airbnb""><code>@​mike-flowers-airbnb</code></a> made their first contribution in <a href=""https://redirect.github.com/burnash/gspread/pull/1477"">burnash/gspread#1477</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.2...v6.1.3"">https://github.com/burnash/gspread/compare/v6.1.2...v6.1.3</a></p>
<h2>v6.1.3-alpha1</h2>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.2...v6.1.3-alpha1"">https://github.com/burnash/gspread/compare/v6.1.2...v6.1.3-alpha1</a></p>
<h2>v6.1.2</h2>
<h2>What's Changed</h2>
<ul>
<li>add note about runnings tests to contrib guide by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1465"">burnash/gspread#1465</a></li>
<li>Some updates on <code>get_notes</code> by <a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1461"">burnash/gspread#1461</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.1...v6.1.2"">https://github.com/burnash/gspread/compare/v6.1.1...v6.1.2</a></p>
<h2>v6.1.1</h2>
<h2>What's Changed</h2>
<ul>
<li>Add some missing typing in code by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1448"">burnash/gspread#1448</a></li>
<li>More fixes for <code>Worksheet.update</code> argument ordering &amp; single cell updating (i.e. now <code>Worksheet.update_acell</code>) by <a href=""https://github.com/alexmalins""><code>@​alexmalins</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1449"">burnash/gspread#1449</a></li>
<li>Added 'add_data_validation<code>to</code>Workhsheet` [Issue <a href=""https://redirect.github.com/burnash/gspread/issues/1420"">#1420</a>] by <a href=""https://github.com/muddi900""><code>@​muddi900</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1444"">burnash/gspread#1444</a></li>
<li>Bump typing-extensions from 4.10.0 to 4.11.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1450"">burnash/gspread#1450</a></li>
<li>Bump black from 23.3.0 to 24.4.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1452"">burnash/gspread#1452</a></li>
<li>Fix incorrect version number in HISTORY.rst from 6.0.1 to 6.1.0 by <a href=""https://github.com/yhay81""><code>@​yhay81</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1455"">burnash/gspread#1455</a></li>
<li>add <code>get_notes</code> by <a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1451"">burnash/gspread#1451</a></li>
<li>Bump mypy from 1.9.0 to 1.10.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1459"">burnash/gspread#1459</a></li>
<li>Bump black from 24.4.0 to 24.4.2 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1460"">burnash/gspread#1460</a></li>
<li>bugfix: handle domain name in spreadsheet copy permissions by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1458"">burnash/gspread#1458</a></li>
<li>Fix/api key auth version by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1463"">burnash/gspread#1463</a></li>
<li>Ignore pip vulnerabilities in CI. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1464"">burnash/gspread#1464</a></li>
<li>Remove StrEnum dependency and added custom class[issue <a href=""https://redirect.github.com/burnash/gspread/issues/1462"">#1462</a>] by <a href=""https://github.com/muddi900""><code>@​muddi900</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1469"">burnash/gspread#1469</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/alexmalins""><code>@​alexmalins</code></a> made their first contribution in <a href=""https://redirect.github.com/burnash/gspread/pull/1449"">burnash/gspread#1449</a></li>
<li><a href=""https://github.com/yhay81""><code>@​yhay81</code></a> made their first contribution in <a href=""https://redirect.github.com/burnash/gspread/pull/1455"">burnash/gspread#1455</a></li>
<li><a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> made their first contribution in <a href=""https://redirect.github.com/burnash/gspread/pull/1451"">burnash/gspread#1451</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.0...v6.1.1"">https://github.com/burnash/gspread/compare/v6.1.0...v6.1.1</a></p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/burnash/gspread/blob/v6.1.4/HISTORY.rst"">gspread's changelog</a>.</em></p>
<blockquote>
<h2>6.1.4 (2024-10-21)</h2>
<ul>
<li>remove dependency on requests-2.27.0</li>
</ul>
<h2>6.1.3 (2024-10-03)</h2>
<ul>
<li>ignore jinja CVE by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1481"">burnash/gspread#1481</a></li>
<li>Remove passing exception as args to super in APIError by <a href=""https://github.com/mike-flowers-airbnb""><code>@​mike-flowers-airbnb</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1477"">burnash/gspread#1477</a></li>
<li>better handler API error parsing. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1510"">burnash/gspread#1510</a></li>
<li>Add test on receiving an invalid JSON in the APIError exception handler. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1512"">burnash/gspread#1512</a></li>
</ul>
<h2>6.1.2 (2024-05-17)</h2>
<ul>
<li>add note about runnings tests to contrib guide by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1465"">burnash/gspread#1465</a></li>
<li>Some updates on <code>get_notes</code> by <a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1461"">burnash/gspread#1461</a></li>
</ul>
<h2>6.1.1 (2024-05-16)</h2>
<ul>
<li>Add some missing typing in code by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1448"">burnash/gspread#1448</a></li>
<li>More fixes for <code>Worksheet.update</code> argument ordering &amp; single cell updating (i.e. now <code>Worksheet.update_acell</code>) by <a href=""https://github.com/alexmalins""><code>@​alexmalins</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1449"">burnash/gspread#1449</a></li>
<li>Added 'add_data_validation<code>to</code>Workhsheet` [Issue <a href=""https://redirect.github.com/burnash/gspread/issues/1420"">#1420</a>] by <a href=""https://github.com/muddi900""><code>@​muddi900</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1444"">burnash/gspread#1444</a></li>
<li>Bump typing-extensions from 4.10.0 to 4.11.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1450"">burnash/gspread#1450</a></li>
<li>Bump black from 23.3.0 to 24.4.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1452"">burnash/gspread#1452</a></li>
<li>Fix incorrect version number in HISTORY.rst from 6.0.1 to 6.1.0 by <a href=""https://github.com/yhay81""><code>@​yhay81</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1455"">burnash/gspread#1455</a></li>
<li>add <code>get_notes</code> by <a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1451"">burnash/gspread#1451</a></li>
<li>Bump mypy from 1.9.0 to 1.10.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1459"">burnash/gspread#1459</a></li>
<li>Bump black from 24.4.0 to 24.4.2 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1460"">burnash/gspread#1460</a></li>
<li>bugfix: handle domain name in spreadsheet copy permissions by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1458"">burnash/gspread#1458</a></li>
<li>Fix/api key auth version by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1463"">burnash/gspread#1463</a></li>
<li>Ignore pip vulnerabilities in CI. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1464"">burnash/gspread#1464</a></li>
<li>Remove StrEnum dependency and added custom class[issue <a href=""https://redirect.github.com/burnash/gspread/issues/1462"">#1462</a>] by <a href=""https://github.com/muddi900""><code>@​muddi900</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1469"">burnash/gspread#1469</a></li>
</ul>
<h2>6.1.0 (2024-03-28)</h2>
<ul>
<li>Add py.typed marker by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1422"">burnash/gspread#1422</a></li>
<li>Improve back-off client by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1415"">burnash/gspread#1415</a></li>
<li>Add new auth method API key by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1428"">burnash/gspread#1428</a></li>
<li>Bugfix/add set timeout by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1417"">burnash/gspread#1417</a></li>
<li>Fix wrapper <code>cast_to_a1_notation</code> by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1427"">burnash/gspread#1427</a></li>
<li>Bump bandit from 1.7.5 to 1.7.8 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1433"">burnash/gspread#1433</a></li>
<li>Bump mypy from 1.6.1 to 1.9.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1432"">burnash/gspread#1432</a></li>
<li>Bump typing-extensions from 4.8.0 to 4.10.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1424"">burnash/gspread#1424</a></li>
<li>Bump flake8 from 5.0.4 to 7.0.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1375"">burnash/gspread#1375</a></li>
<li>fix error message readability by <a href=""https://github.com/imrehg""><code>@​imrehg</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1435"">burnash/gspread#1435</a></li>
<li>Add missing method <code>import_csv()</code> by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1426"">burnash/gspread#1426</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/burnash/gspread/commit/1e3564da2c93f335568b6021038ce055e126cf73""><code>1e3564d</code></a> Release v6.1.4</li>
<li><a href=""https://github.com/burnash/gspread/commit/623b32b3745ffd73e5b2b9d4a2d3c28bbfe7d6b1""><code>623b32b</code></a> Remove dependency on requests-2.27.0</li>
<li><a href=""https://github.com/burnash/gspread/commit/2bad9dfb24e1a6f6676b945ebc28a38bbcfd4952""><code>2bad9df</code></a> sort imports</li>
<li><a href=""https://github.com/burnash/gspread/commit/8319285358ef4a1bc75e9b3b75b0a5dde1088a3a""><code>8319285</code></a> import <code>JSONDecodeError</code> straight from <code>requests</code></li>
<li><a href=""https://github.com/burnash/gspread/commit/3476d9f69f50bb9491ab23ce3364f7388bbc201e""><code>3476d9f</code></a> add verbose output to publish action</li>
<li><a href=""https://github.com/burnash/gspread/commit/0c85a30fbf11fbe744b25ed708888148d206b36e""><code>0c85a30</code></a> Release v6.1.3</li>
<li><a href=""https://github.com/burnash/gspread/commit/5967ab7b50cecd51dc68a6f1a9530c44bd24d962""><code>5967ab7</code></a> ignore jinja CVE</li>
<li><a href=""https://github.com/burnash/gspread/commit/75039b9faf523bb0558339ea64a883fca95188ae""><code>75039b9</code></a> Add test on receiving an invalid JSON in the APIError exception handler.</li>
<li><a href=""https://github.com/burnash/gspread/commit/9917d072cc8d96d506cb72538366382b30976452""><code>9917d07</code></a> better handler API error parsing.</li>
<li><a href=""https://github.com/burnash/gspread/commit/a3e1ccd2a5e7a11fd96e3dce4000f0bb08d45ad4""><code>a3e1ccd</code></a> Refactor to implement <strong>reduce</strong> instead</li>
<li>Additional commits viewable in <a href=""https://github.com/burnash/gspread/compare/v3.7.0...v6.1.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=gspread&package-manager=pip&previous-version=3.7.0&new-version=6.1.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
20,Bump suds-py3 from 1.4.4.1 to 1.4.5.0 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1171,,1171,open,2024-10-21 10:19:21+00:00,2024-10-21 10:19:22+00:00,,"Bumps [suds-py3](https://github.com/cackharot/suds-py3) from 1.4.4.1 to 1.4.5.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/cackharot/suds-py3/releases"">suds-py3's releases</a>.</em></p>
<blockquote>
<h2>Wheel release</h2>
<p>No changes, only wheel dist release on PyPI.</p>
<p><a href=""https://pypi.org/project/suds-py3/1.4.5.0/#files"">https://pypi.org/project/suds-py3/1.4.5.0/#files</a></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/cackharot/suds-py3/commit/871d0c363607c6bee06236e743e2804f513ba22a""><code>871d0c3</code></a> bump up to 1.4.5.0</li>
<li><a href=""https://github.com/cackharot/suds-py3/commit/1d92cc6297efee31bfd94b50b99c431505d7de21""><code>1d92cc6</code></a> Bump py from 1.4.32 to 1.10.0</li>
<li>See full diff in <a href=""https://github.com/cackharot/suds-py3/compare/v1.4.4.1...v1.4.5.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=suds-py3&package-manager=pip&previous-version=1.4.4.1&new-version=1.4.5.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
21,Bump google-cloud-storage from 2.16.0 to 2.18.2 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1170,,1170,open,2024-10-21 10:19:17+00:00,2024-10-21 10:19:18+00:00,,"Bumps [google-cloud-storage](https://github.com/googleapis/python-storage) from 2.16.0 to 2.18.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/googleapis/python-storage/releases"">google-cloud-storage's releases</a>.</em></p>
<blockquote>
<h2>v2.18.2</h2>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.18.1...v2.18.2"">2.18.2</a> (2024-08-08)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Add regression test for range read retry issue and bump dependency to fix (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1338"">#1338</a>) (<a href=""https://github.com/googleapis/python-storage/commit/0323647d768b3be834cfab53efb3c557a47d41c3"">0323647</a>)</li>
</ul>
<h2>v2.18.1</h2>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.18.0...v2.18.1"">2.18.1</a> (2024-08-05)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Properly escape URL construction for XML MPU API (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1333"">#1333</a>) (<a href=""https://github.com/googleapis/python-storage/commit/bf4d0e0a2ef1d608d679c22b13d8f5d90b39c7b2"">bf4d0e0</a>)</li>
</ul>
<h2>v2.18.0</h2>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.17.0...v2.18.0"">2.18.0</a> (2024-07-09)</h2>
<h3>Features</h3>
<ul>
<li>Add OpenTelemetry Tracing support as a preview feature (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1288"">#1288</a>) (<a href=""https://github.com/googleapis/python-storage/commit/c2ab0e035b179a919b27c7f50318472f14656e00"">c2ab0e0</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Allow Protobuf 5.x (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1317"">#1317</a>) (<a href=""https://github.com/googleapis/python-storage/commit/152b249472a09342777237d47b6c09f99c2d28e6"">152b249</a>)</li>
<li>Correct notification error message (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1290"">#1290</a>) (<a href=""https://github.com/googleapis/python-storage/commit/1cb977daa2d97c255a382ce81f56a43168b0637d"">1cb977d</a>), closes <a href=""https://redirect.github.com/googleapis/python-storage/issues/1289"">#1289</a></li>
</ul>
<h2>v2.17.0</h2>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.16.0...v2.17.0"">2.17.0</a> (2024-05-22)</h2>
<h3>Features</h3>
<ul>
<li>Support HNS enablement in bucket metadata (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1278"">#1278</a>) (<a href=""https://github.com/googleapis/python-storage/commit/add3c01f0974e22df7f0b50504d5e83e4235fd81"">add3c01</a>)</li>
<li>Support page_size in bucket.list_blobs (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1275"">#1275</a>) (<a href=""https://github.com/googleapis/python-storage/commit/c52e882f65583a7739392926308cc34984561165"">c52e882</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Remove deprecated methods in samples and tests (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1274"">#1274</a>) (<a href=""https://github.com/googleapis/python-storage/commit/4db96c960b07e503c1031c9fa879cf2af195f513"">4db96c9</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Reference Storage Control in readme (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1254"">#1254</a>) (<a href=""https://github.com/googleapis/python-storage/commit/3d6d3693d5c1b24cd3d2bbdeabfd78b8bfd4161a"">3d6d369</a>)</li>
<li>Update DEFAULT_RETRY_IF_GENERATION_SPECIFIED docstrings (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1234"">#1234</a>) (<a href=""https://github.com/googleapis/python-storage/commit/bdd426adf5901faa36115885af868ef50e356a36"">bdd426a</a>)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/googleapis/python-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>
<blockquote>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.18.1...v2.18.2"">2.18.2</a> (2024-08-08)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Add regression test for range read retry issue and bump dependency to fix (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1338"">#1338</a>) (<a href=""https://github.com/googleapis/python-storage/commit/0323647d768b3be834cfab53efb3c557a47d41c3"">0323647</a>)</li>
</ul>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.18.0...v2.18.1"">2.18.1</a> (2024-08-05)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Properly escape URL construction for XML MPU API (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1333"">#1333</a>) (<a href=""https://github.com/googleapis/python-storage/commit/bf4d0e0a2ef1d608d679c22b13d8f5d90b39c7b2"">bf4d0e0</a>)</li>
</ul>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.17.0...v2.18.0"">2.18.0</a> (2024-07-09)</h2>
<h3>Features</h3>
<ul>
<li>Add OpenTelemetry Tracing support as a preview feature (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1288"">#1288</a>) (<a href=""https://github.com/googleapis/python-storage/commit/c2ab0e035b179a919b27c7f50318472f14656e00"">c2ab0e0</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Allow Protobuf 5.x (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1317"">#1317</a>) (<a href=""https://github.com/googleapis/python-storage/commit/152b249472a09342777237d47b6c09f99c2d28e6"">152b249</a>)</li>
<li>Correct notification error message (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1290"">#1290</a>) (<a href=""https://github.com/googleapis/python-storage/commit/1cb977daa2d97c255a382ce81f56a43168b0637d"">1cb977d</a>), closes <a href=""https://redirect.github.com/googleapis/python-storage/issues/1289"">#1289</a></li>
</ul>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.16.0...v2.17.0"">2.17.0</a> (2024-05-22)</h2>
<h3>Features</h3>
<ul>
<li>Support HNS enablement in bucket metadata (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1278"">#1278</a>) (<a href=""https://github.com/googleapis/python-storage/commit/add3c01f0974e22df7f0b50504d5e83e4235fd81"">add3c01</a>)</li>
<li>Support page_size in bucket.list_blobs (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1275"">#1275</a>) (<a href=""https://github.com/googleapis/python-storage/commit/c52e882f65583a7739392926308cc34984561165"">c52e882</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Remove deprecated methods in samples and tests (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1274"">#1274</a>) (<a href=""https://github.com/googleapis/python-storage/commit/4db96c960b07e503c1031c9fa879cf2af195f513"">4db96c9</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Reference Storage Control in readme (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1254"">#1254</a>) (<a href=""https://github.com/googleapis/python-storage/commit/3d6d3693d5c1b24cd3d2bbdeabfd78b8bfd4161a"">3d6d369</a>)</li>
<li>Update DEFAULT_RETRY_IF_GENERATION_SPECIFIED docstrings (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1234"">#1234</a>) (<a href=""https://github.com/googleapis/python-storage/commit/bdd426adf5901faa36115885af868ef50e356a36"">bdd426a</a>)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/googleapis/python-storage/commit/85aa02fc88ddeb7f036957bd5fe96e07e7f56f14""><code>85aa02f</code></a> chore(main): release 2.18.2 (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1339"">#1339</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/0323647d768b3be834cfab53efb3c557a47d41c3""><code>0323647</code></a> Fix: Add regression test for range read retry issue and bump dependency to fi...</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/5c935030751a24f1b5e7a29146f5ec4d5f2ed0ec""><code>5c93503</code></a> chore(main): release 2.18.1 (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1334"">#1334</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/bf4d0e0a2ef1d608d679c22b13d8f5d90b39c7b2""><code>bf4d0e0</code></a> Fix: Properly escape URL construction for XML MPU API (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1333"">#1333</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/3d29c6f8bc3fb4ab02ee2262b094f406326b98e8""><code>3d29c6f</code></a> chore(main): release 2.18.0 (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1291"">#1291</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/d5d3c68a6e5c6f8cefc59892c1ccceaf181ff32d""><code>d5d3c68</code></a> chore(python): use python 3.10 for docs build (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1322"">#1322</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/a4b07b8b99812b8ce1192850f48782f0be7b6a9f""><code>a4b07b8</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1324"">#1324</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/ea47943276fb3227bca5c7eea092f1bb4ce76fb9""><code>ea47943</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1308"">#1308</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/ed7cbbafc29ce462d12cd6b10dd975443967cdea""><code>ed7cbba</code></a> test: harden pytest fixture setup stage (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1323"">#1323</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/152b249472a09342777237d47b6c09f99c2d28e6""><code>152b249</code></a> fix: Allow Protobuf 5.x (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1317"">#1317</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/googleapis/python-storage/compare/v2.16.0...v2.18.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=google-cloud-storage&package-manager=pip&previous-version=2.16.0&new-version=2.18.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
22,Bump mysql-connector-python from 8.0.18 to 9.1.0,https://api.github.com/repos/move-coop/parsons/issues/1169,,1169,open,2024-10-21 10:17:50+00:00,2024-10-21 10:17:51+00:00,,"Bumps [mysql-connector-python](https://github.com/mysql/mysql-connector-python) from 8.0.18 to 9.1.0.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/mysql/mysql-connector-python/blob/trunk/CHANGES.txt"">mysql-connector-python's changelog</a>.</em></p>
<blockquote>
<h1>v9.1.0</h1>
<ul>
<li>WL#16452: Bundle all installable authentication plugins when building the C-extension</li>
<li>WL#16444: Drop build support for DEB packages</li>
<li>WL#16442: Upgrade gssapi version to 1.8.3</li>
<li>WL#16411: Improve wheel metadata information for Classic and XDevAPI connectors</li>
<li>WL#16341: OpenID Connect (Oauth2 - JWT) Authentication Support</li>
<li>WL#16307: Remove Python 3.8 support</li>
<li>WL#16306: Add support for Python 3.13</li>
<li>BUG#37055435: Connection fails during the TLS negotiation when specifying TLSv1.3 ciphers</li>
<li>BUG#37013057: mysql-connector-python Parameterized query SQL injection</li>
<li>BUG#36765200: python mysql connector 8.3.0 raise %-.100s:%u when input a wrong host</li>
<li>BUG#36577957: Update charset/collation description indicate this is 16 bits</li>
</ul>
<h1>v9.0.0</h1>
<ul>
<li>WL#16350: Update dnspython version</li>
<li>WL#16318: Deprecate Cursors Prepared Raw and Named Tuple</li>
<li>WL#16284: Update the Python Protobuf version</li>
<li>WL#16283: Remove OpenTelemetry Bundled Installation</li>
<li>BUG#36664998: Packets out of order error is raised while changing user in aio</li>
<li>BUG#36611371: Update dnspython required versions to allow latest 2.6.1</li>
<li>BUG#36570707: Collation set on connect using C-Extension is ignored</li>
<li>BUG#36476195: Incorrect escaping in pure Python mode if sql_mode includes NO_BACKSLASH_ESCAPES</li>
<li>BUG#36289767: MySQLCursorBufferedRaw does not skip conversion</li>
</ul>
<h1>v8.4.0</h1>
<ul>
<li>WL#16203: GPL License Exception Update</li>
<li>WL#16173: Update allowed cipher and cipher-suite lists</li>
<li>WL#16164: Implement support for new vector data type</li>
<li>WL#16127: Remove the FIDO authentication mechanism</li>
<li>WL#16053: Support GSSAPI/Kerberos authentication on Windows using authentication_ldap_sasl_client plug-in for C-extension</li>
<li>BUG#36227964: Improve OpenTelemetry span coverage</li>
<li>BUG#36167880: Massive memory leak mysqlx native Protobuf adding to collection</li>
</ul>
<h1>v8.3.0</h1>
<ul>
<li>WL#16015: Remove use of removed COM_ commands</li>
<li>WL#15985: Support GSSAPI/Kerberos authentication on Windows using authentication_ldap_sasl_client plug-in for Pure Python</li>
<li>WL#15983: Stop using mysql_ssl_set api</li>
<li>WL#15982: Remove use of mysql_shutdown</li>
<li>WL#15950: Support query parameters for prepared statements</li>
<li>WL#15942: Improve type hints and standardize byte type handling</li>
<li>WL#15836: Split mysql and mysqlx into different packages</li>
<li>WL#15523: Support Python DB API asynchronous execution</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/4fbf521f1c6c71621f882f89c0c4946c10ee13ac""><code>4fbf521</code></a> Remove explicit reference to CMySQLConnection from the cipher tests</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/db3d07d45547ff58a36b142bb78d515ca303a86d""><code>db3d07d</code></a> Updated the LICENSE files</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/29c17b4c498ed78805b429099ef6cf819ca86878""><code>29c17b4</code></a> Disable build-id links in RPMs</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/5b0bfcec0d0954a628a488cab8c3fa434b6c8792""><code>5b0bfce</code></a> Added missing 'byte_code_only' option to the mysqlx RPM spec file</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/e77f27e6038ea3b989b9c78d34ef5dac66604cfb""><code>e77f27e</code></a> Updated the LICENSE files</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/040414f40b954799a6e66bd51d9cea50b98d0ba9""><code>040414f</code></a> BUG#37055435: Connection fails during the TLS negotiation when specifying TLS...</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/8a90d72dbb3bafc0f8f1b3206084db6606a4f972""><code>8a90d72</code></a> Added CONTRIBUTING.md and SECURITY.md files in Connector/Python</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/6f81ac5e031a85df537c1364b68b3990bf545cc4""><code>6f81ac5</code></a> BUG#36765200: python mysql connector 8.3.0 raise %-.100s:%u when input a wron...</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/da48d020ce53f666a6c61379db5ec583c29d316e""><code>da48d02</code></a> Prepare release 9.1.0</li>
<li><a href=""https://github.com/mysql/mysql-connector-python/commit/61bccb050b910c918d326f2f318ed3f623d5b374""><code>61bccb0</code></a> BUG#36577957: Update charset/collation description indicate this is 16 bits</li>
<li>Additional commits viewable in <a href=""https://github.com/mysql/mysql-connector-python/compare/8.0.18...9.1.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=mysql-connector-python&package-manager=pip&previous-version=8.0.18&new-version=9.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
23,"Revert ""Bump install-pinned/uv from de03c60d508703a83d3f8f49afcf1249590ecda1 to 03a68782c27167b267490a9392ac24d6b93a2f14""",https://api.github.com/repos/move-coop/parsons/issues/1168,Shauna Gordon-McKeon,1168,closed,2024-10-16 16:15:28+00:00,2024-10-16 16:24:11+00:00,2024-10-16 16:24:11+00:00,Reverts move-coop/parsons#1154 which is causing the tests to fail,[],[],0
24,Python 3.13 Support,https://api.github.com/repos/move-coop/parsons/issues/1167,Wil T,1167,open,2024-10-11 11:18:27+00:00,2024-10-11 11:41:36+00:00,,"Python 3.13 is now out!
https://docs.python.org/3.13/whatsnew/3.13.html","[Label(name=""bug"")]",[],1
25,Bump joblib from 1.2.0 to 1.4.2,https://api.github.com/repos/move-coop/parsons/issues/1166,,1166,open,2024-10-10 19:52:38+00:00,2024-10-10 19:52:39+00:00,,"Bumps [joblib](https://github.com/joblib/joblib) from 1.2.0 to 1.4.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/joblib/joblib/releases"">joblib's releases</a>.</em></p>
<blockquote>
<h2>1.4.2</h2>
<h2>What's Changed</h2>
<ul>
<li>TST add a test that ensures conservation of byte order during IPC by <a href=""https://github.com/fcharras""><code>@​fcharras</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1562"">joblib/joblib#1562</a></li>
<li>DOC fix typos in CHANGES.rst by <a href=""https://github.com/cclauss""><code>@​cclauss</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1571"">joblib/joblib#1571</a></li>
<li>DOC typo in docs by <a href=""https://github.com/jmerkow""><code>@​jmerkow</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1574"">joblib/joblib#1574</a></li>
<li>FIX revert MemorizedFunc.call API change by <a href=""https://github.com/tomMoral""><code>@​tomMoral</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1576"">joblib/joblib#1576</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/jmerkow""><code>@​jmerkow</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/1574"">joblib/joblib#1574</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/joblib/joblib/compare/1.4.0...1.4.2"">https://github.com/joblib/joblib/compare/1.4.0...1.4.2</a></p>
<h2>Joblib 1.4.0</h2>
<h2>What's Changed</h2>
<ul>
<li>FIX raise iterator exception in user's thread by <a href=""https://github.com/tomMoral""><code>@​tomMoral</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1491"">joblib/joblib#1491</a></li>
<li>MAINT: Update byte_bounds import by <a href=""https://github.com/mtsokol""><code>@​mtsokol</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1501"">joblib/joblib#1501</a></li>
<li>FEA Implement generator unordered parameter by <a href=""https://github.com/fcharras""><code>@​fcharras</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1463"">joblib/joblib#1463</a></li>
<li>MAINT: Cleanup expired ndarray methods by <a href=""https://github.com/mtsokol""><code>@​mtsokol</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1506"">joblib/joblib#1506</a></li>
<li>Fix README cloning url by <a href=""https://github.com/andreaso""><code>@​andreaso</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1508"">joblib/joblib#1508</a></li>
<li>Fix README archive download link by <a href=""https://github.com/andreaso""><code>@​andreaso</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1509"">joblib/joblib#1509</a></li>
<li>Prefer https:// links in the documentation by <a href=""https://github.com/andreaso""><code>@​andreaso</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1510"">joblib/joblib#1510</a></li>
<li>Stricter Dask tests cleanup by <a href=""https://github.com/ogrisel""><code>@​ogrisel</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1514"">joblib/joblib#1514</a></li>
<li>CLN simplify code for easier read by <a href=""https://github.com/tomMoral""><code>@​tomMoral</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1517"">joblib/joblib#1517</a></li>
<li>Vendor cloudpickle 3.0.0 and drop support for Python 3.7. by <a href=""https://github.com/ogrisel""><code>@​ogrisel</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1515"">joblib/joblib#1515</a></li>
<li>MNT Fix Python 3.12 deprecation warning by <a href=""https://github.com/lesteve""><code>@​lesteve</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1518"">joblib/joblib#1518</a></li>
<li>FIX <code>_get_items_to_delete</code> raising error when items list empty by <a href=""https://github.com/Dr-Blank""><code>@​Dr-Blank</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1503"">joblib/joblib#1503</a></li>
<li>Update dask backend for compatibility with return_as=generator by <a href=""https://github.com/fcharras""><code>@​fcharras</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1520"">joblib/joblib#1520</a></li>
<li>DOC: Best randomness with Parallel generators by <a href=""https://github.com/paquiteau""><code>@​paquiteau</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1490"">joblib/joblib#1490</a></li>
<li>MTN update ci pipelines to use python3.8 + test more pypy versions by <a href=""https://github.com/fcharras""><code>@​fcharras</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1487"">joblib/joblib#1487</a></li>
<li>MNT Restart readthedoc pipeline by <a href=""https://github.com/fcharras""><code>@​fcharras</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1524"">joblib/joblib#1524</a></li>
<li>FIX Close cleanly distributed Client at the end of unit tests by <a href=""https://github.com/fcharras""><code>@​fcharras</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1526"">joblib/joblib#1526</a></li>
<li>Add pytest 8.x compatability by <a href=""https://github.com/mr-c""><code>@​mr-c</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1553"">joblib/joblib#1553</a></li>
<li>n_jobs parameter in instantiation of Parallel should be an integer by <a href=""https://github.com/androids-electric-sheep""><code>@​androids-electric-sheep</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1549"">joblib/joblib#1549</a></li>
<li>DOC : updated n_jobs docs in Parallel class by <a href=""https://github.com/Schefflera-Arboricola""><code>@​Schefflera-Arboricola</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1537"">joblib/joblib#1537</a></li>
<li>CI Fix url for scikit-learn nightly builds by <a href=""https://github.com/jeremiedbb""><code>@​jeremiedbb</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1567"">joblib/joblib#1567</a></li>
<li>ENH allow caching coroutine functions by <a href=""https://github.com/gsakkis""><code>@​gsakkis</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/894"">joblib/joblib#894</a></li>
<li>DOC consistent default formating in doc by <a href=""https://github.com/tomMoral""><code>@​tomMoral</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1565"">joblib/joblib#1565</a></li>
<li>DOC add gotcha on non-reproducible pickling by <a href=""https://github.com/tomMoral""><code>@​tomMoral</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1566"">joblib/joblib#1566</a></li>
<li>RELEASE 1.4.0 by <a href=""https://github.com/tomMoral""><code>@​tomMoral</code></a> in <a href=""https://redirect.github.com/joblib/joblib/pull/1568"">joblib/joblib#1568</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/mtsokol""><code>@​mtsokol</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/1501"">joblib/joblib#1501</a></li>
<li><a href=""https://github.com/andreaso""><code>@​andreaso</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/1508"">joblib/joblib#1508</a></li>
<li><a href=""https://github.com/Dr-Blank""><code>@​Dr-Blank</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/1503"">joblib/joblib#1503</a></li>
<li><a href=""https://github.com/paquiteau""><code>@​paquiteau</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/1490"">joblib/joblib#1490</a></li>
<li><a href=""https://github.com/mr-c""><code>@​mr-c</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/1553"">joblib/joblib#1553</a></li>
<li><a href=""https://github.com/androids-electric-sheep""><code>@​androids-electric-sheep</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/1549"">joblib/joblib#1549</a></li>
<li><a href=""https://github.com/Schefflera-Arboricola""><code>@​Schefflera-Arboricola</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/1537"">joblib/joblib#1537</a></li>
<li><a href=""https://github.com/gsakkis""><code>@​gsakkis</code></a> made their first contribution in <a href=""https://redirect.github.com/joblib/joblib/pull/894"">joblib/joblib#894</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/joblib/joblib/blob/main/CHANGES.rst"">joblib's changelog</a>.</em></p>
<blockquote>
<h2>Release 1.4.2 -- 2024/05/02</h2>
<p>Due to maintenance issues, 1.4.1 was not valid and we bumped the version to 1.4.2</p>
<ul>
<li>Fix a backward incompatible change in <code>MemorizedFunc.call</code> which needs to
return the metadata. Also make sure that <code>NotMemorizedFunc.call</code> return
an empty dict for metadata for consistency.
<a href=""https://redirect.github.com/joblib/joblib/pull/1576"">joblib/joblib#1576</a></li>
</ul>
<h2>Release 1.4.0 -- 2024/04/08</h2>
<ul>
<li>
<p>Allow caching co-routines with <code>Memory.cache</code>.
<a href=""https://redirect.github.com/joblib/joblib/pull/894"">joblib/joblib#894</a></p>
</li>
<li>
<p>Try to cast <code>n_jobs</code> to int in parallel and raise an error if
it fails. This means that <code>n_jobs=2.3</code> will now result in
<code>effective_n_jobs=2</code> instead of failing.
<a href=""https://redirect.github.com/joblib/joblib/pull/1539"">joblib/joblib#1539</a></p>
</li>
<li>
<p>Ensure that errors in the task generator given to Parallel's call
are raised in the results consumming thread.
<a href=""https://redirect.github.com/joblib/joblib/pull/1491"">joblib/joblib#1491</a></p>
</li>
<li>
<p>Adjust codebase to NumPy 2.0 by changing <code>np.NaN</code> to <code>np.nan</code>
and importing <code>byte_bounds</code> from <code>np.lib.array_utils</code>.
<a href=""https://redirect.github.com/joblib/joblib/pull/1501"">joblib/joblib#1501</a></p>
</li>
<li>
<p>The parameter <code>return_as</code> in <code>joblib.Parallel</code> can now be set to
<code>generator_unordered</code>. In this case the results will be returned in the
order of task completion rather than the order of submission.
<a href=""https://redirect.github.com/joblib/joblib/pull/1463"">joblib/joblib#1463</a></p>
</li>
<li>
<p>dask backend now supports <code>return_as=generator</code> and
<code>return_as=generator_unordered</code>.
<a href=""https://redirect.github.com/joblib/joblib/pull/1520"">joblib/joblib#1520</a></p>
</li>
<li>
<p>Vendor cloudpickle 3.0.0 and end support for Python 3.7 which has
reached end of life.
<a href=""https://redirect.github.com/joblib/joblib/pull/1487"">joblib/joblib#1487</a>
<a href=""https://redirect.github.com/joblib/joblib/pull/1515"">joblib/joblib#1515</a></p>
</li>
</ul>
<h2>Release 1.3.2 -- 2023/08/08</h2>
<ul>
<li>Fix a regression in <code>joblib.Parallel</code> introduced in 1.3.0 where
explicitly setting <code>n_jobs=None</code> was not interpreted as &quot;unset&quot;.</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/joblib/joblib/commit/d46857a490a70fa8b66b2878066b40de13d3e326""><code>d46857a</code></a> RELEASE 1.4.2 - bugfix release (<a href=""https://redirect.github.com/joblib/joblib/issues/1579"">#1579</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/1787fb73db6c7b08beb1bc9cc1d321197678bf53""><code>1787fb7</code></a> MTN back to dev mode</li>
<li><a href=""https://github.com/joblib/joblib/commit/118ad371e3114b58343a1d868e7039fcf785f2ab""><code>118ad37</code></a> RELEASE 1.4.1 - bugfix release (<a href=""https://redirect.github.com/joblib/joblib/issues/1578"">#1578</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/398d8ee669ee355be48d9b2f80aaebbf8ca15d37""><code>398d8ee</code></a> FIX revert MemorizedFunc.call API change (<a href=""https://redirect.github.com/joblib/joblib/issues/1576"">#1576</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/2be8dcd4cb777ced834808cb9349eef441e2d133""><code>2be8dcd</code></a> DOC fix return_as=&quot;generator_unordered&quot; in docs (<a href=""https://redirect.github.com/joblib/joblib/issues/1574"">#1574</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/4db39ebf064ecacbf9a0d01627c80692fdf64ac5""><code>4db39eb</code></a> CLN fix spelling typo with codespell (<a href=""https://redirect.github.com/joblib/joblib/issues/1571"">#1571</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/1b0d1f4b4a68a6bc8ba4d49366157078af66b5dc""><code>1b0d1f4</code></a> TST add a non-regression test ensuring conservation of byte order during IPC ...</li>
<li><a href=""https://github.com/joblib/joblib/commit/59659e0da44f5a766fa1ee52bd6a0338f3950198""><code>59659e0</code></a> MTN remove deprecated bytes_limit (<a href=""https://redirect.github.com/joblib/joblib/issues/1569"">#1569</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/f57a72190d0fb6618bd77190c860707b1eb71ec7""><code>f57a721</code></a> MTN handle changes from master-&gt;main + remove master process terminology</li>
<li><a href=""https://github.com/joblib/joblib/commit/ba1d3da1f57260cd6774ed0011496bf4442d9aa5""><code>ba1d3da</code></a> MTN back to dev mode</li>
<li>Additional commits viewable in <a href=""https://github.com/joblib/joblib/compare/1.2.0...1.4.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=joblib&package-manager=pip&previous-version=1.2.0&new-version=1.4.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
26,Bump censusgeocode from 0.4.3.post1 to 0.5.2,https://api.github.com/repos/move-coop/parsons/issues/1165,,1165,open,2024-10-10 19:52:32+00:00,2024-10-10 19:52:33+00:00,,"Bumps [censusgeocode](https://github.com/fitnr/censusgeocode) from 0.4.3.post1 to 0.5.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/fitnr/censusgeocode/releases"">censusgeocode's releases</a>.</em></p>
<blockquote>
<h2>v0.5.2</h2>
<ul>
<li>Accept either &quot;zip&quot; or &quot;zipcode&quot; as an argument in <code>CensusGeocode.address</code> (<a href=""https://redirect.github.com/fitnr/censusgeocode/issues/24"">#24</a>).</li>
<li>Add warning to <code>CensusGeocode.batch()</code> when trying send more than 10,000 records</li>
<li>Move packaging metadata to setup.cfg and pyproject.toml</li>
<li>Use github actions for automated tests</li>
</ul>
<h2>v0.5.1</h2>
<h2>0.5.1</h2>
<ul>
<li>Fix empty default values for benchmarks and vintages (<a href=""https://redirect.github.com/fitnr/censusgeocode/issues/23"">#23</a>)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/fitnr/censusgeocode/blob/master/HISTORY"">censusgeocode's changelog</a>.</em></p>
<blockquote>
<h2>0.5.2</h2>
<ul>
<li>Accept either &quot;zip&quot; or &quot;zipcode&quot; as an argument in <code>CensusGeocode.address</code> (<a href=""https://redirect.github.com/fitnr/censusgeocode/issues/24"">#24</a>).</li>
<li>Add warning to <code>CensusGeocode.batch()</code> when trying send more than 10,000 records</li>
<li>Move packaging metadata to setup.cfg and pyproject.toml</li>
<li>Use github actions for automated tests</li>
</ul>
<h2>0.5.1</h2>
<ul>
<li>Fix empty default values (<a href=""https://redirect.github.com/fitnr/censusgeocode/issues/23"">#23</a>)</li>
</ul>
<h2>0.5.0</h2>
<ul>
<li>Removed hard-coded benchmarks and vintages, which were out of date. Refer to Census Geocoder documentation for a list of active benchmarks and vintages.</li>
</ul>
<h2>0.4.4</h2>
<ul>
<li>Handle misformatted batch responses from the Census</li>
<li>Add more options for command line tool</li>
<li>Codify support for Python 3.7, 3.8. Drop support for Python 3.4</li>
</ul>
<h2>0.4.3</h2>
<ul>
<li>Fixed bug in parsing addressbatch results in some versions of Python (thanks <a href=""https://github.com/summonholmes""><code>@​summonholmes</code></a>)</li>
</ul>
<h2>0.4.2</h2>
<ul>
<li>Fix reversed coordinate order in batch addresses</li>
</ul>
<h2>0.4.1</h2>
<ul>
<li>Add static methods to module</li>
<li>Update vintages and benchmarks.</li>
</ul>
<h2>0.4.0</h2>
<ul>
<li>Add support for address batch functionality, including to command line tool</li>
</ul>
<h2>0.3.0</h2>
<ul>
<li>Better error reporting when API is down.</li>
<li>Use <code>requests[security]</code> to fix SSL 3 errors, bump required <code>requests</code> version.</li>
<li>Combine Py 2 and Py 3 codebases.</li>
<li>Add timeout parameter to <code>CensusGeocode.fetch</code> (thanks <a href=""https://github.com/mxr""><code>@​mxr</code></a>)</li>
</ul>
<h2>0.2.3</h2>
<ul>
<li>Always yield a <code>ValueError</code> when Census API is broken.</li>
<li>Internal refactor to simplify keyword arguments.</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/83bbc5e1cbb57351863037285e565493409b08c3""><code>83bbc5e</code></a> warn when posting more than 10k rows</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/22fff3d36effb71a8fb6897cda93a4fe7ddf864e""><code>22fff3d</code></a> add deploy step to workflow</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/88ad6da5d218920075b785d6fdb4cefc5abee303""><code>88ad6da</code></a> Update README and HISTORY</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/b4908f2c23d1a6e16996452b6a113d95043bb215""><code>b4908f2</code></a> add build backend option</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/23bb1b150a932f8ec62ca97224e639e17dd0d7d7""><code>23bb1b1</code></a> require newer setuptools</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/771c4cbd3c70d9b5ff232e808509545c8e96a2c7""><code>771c4cb</code></a> clean up import</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/10a8c9a14d58a31775d5511c67b38009364cd72f""><code>10a8c9a</code></a> bump requirements</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/86e84bd08d64b7fbf8c8ccd648c4630ff6e5e3d6""><code>86e84bd</code></a> clean up unnec shebangs</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/7d4d5e6666f1a654bc7a0f4a01a17d44781c022d""><code>7d4d5e6</code></a> replace travis with github actions</li>
<li><a href=""https://github.com/fitnr/censusgeocode/commit/ca0a27eb4ac7bf0d935b88dd7b9eb0cadf340a57""><code>ca0a27e</code></a> update python packaging</li>
<li>Additional commits viewable in <a href=""https://github.com/fitnr/censusgeocode/compare/v0.4.3.post1...v0.5.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=censusgeocode&package-manager=pip&previous-version=0.4.3.post1&new-version=0.5.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
27,"Update sqlalchemy requirement from !=1.4.33,<2.0.0,>=1.4.22 to >=1.4.22,!=1.4.33,<3.0.0",https://api.github.com/repos/move-coop/parsons/issues/1164,,1164,open,2024-10-10 19:52:26+00:00,2024-10-10 19:52:26+00:00,,"

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
28,Bump gspread from 3.7.0 to 6.1.3 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1163,,1163,closed,2024-10-10 19:51:48+00:00,2024-10-21 10:19:58+00:00,2024-10-21 10:19:56+00:00,"Bumps [gspread](https://github.com/burnash/gspread) from 3.7.0 to 6.1.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/burnash/gspread/releases"">gspread's releases</a>.</em></p>
<blockquote>
<h2>v6.1.3</h2>
<p>Bugfix to catch <code>JSONDecodeError</code> better from Google.</p>
<h2>What's Changed</h2>
<ul>
<li>ignore jinja CVE by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1481"">burnash/gspread#1481</a></li>
<li>Remove passing exception as args to super in APIError by <a href=""https://github.com/mike-flowers-airbnb""><code>@​mike-flowers-airbnb</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1477"">burnash/gspread#1477</a></li>
<li>better handler API error parsing. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1510"">burnash/gspread#1510</a></li>
<li>Add test on receiving an invalid JSON in the APIError exception handler. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1512"">burnash/gspread#1512</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/mike-flowers-airbnb""><code>@​mike-flowers-airbnb</code></a> made their first contribution in <a href=""https://redirect.github.com/burnash/gspread/pull/1477"">burnash/gspread#1477</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.2...v6.1.3"">https://github.com/burnash/gspread/compare/v6.1.2...v6.1.3</a></p>
<h2>v6.1.3-alpha1</h2>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.2...v6.1.3-alpha1"">https://github.com/burnash/gspread/compare/v6.1.2...v6.1.3-alpha1</a></p>
<h2>v6.1.2</h2>
<h2>What's Changed</h2>
<ul>
<li>add note about runnings tests to contrib guide by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1465"">burnash/gspread#1465</a></li>
<li>Some updates on <code>get_notes</code> by <a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1461"">burnash/gspread#1461</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.1...v6.1.2"">https://github.com/burnash/gspread/compare/v6.1.1...v6.1.2</a></p>
<h2>v6.1.1</h2>
<h2>What's Changed</h2>
<ul>
<li>Add some missing typing in code by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1448"">burnash/gspread#1448</a></li>
<li>More fixes for <code>Worksheet.update</code> argument ordering &amp; single cell updating (i.e. now <code>Worksheet.update_acell</code>) by <a href=""https://github.com/alexmalins""><code>@​alexmalins</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1449"">burnash/gspread#1449</a></li>
<li>Added 'add_data_validation<code>to</code>Workhsheet` [Issue <a href=""https://redirect.github.com/burnash/gspread/issues/1420"">#1420</a>] by <a href=""https://github.com/muddi900""><code>@​muddi900</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1444"">burnash/gspread#1444</a></li>
<li>Bump typing-extensions from 4.10.0 to 4.11.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1450"">burnash/gspread#1450</a></li>
<li>Bump black from 23.3.0 to 24.4.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1452"">burnash/gspread#1452</a></li>
<li>Fix incorrect version number in HISTORY.rst from 6.0.1 to 6.1.0 by <a href=""https://github.com/yhay81""><code>@​yhay81</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1455"">burnash/gspread#1455</a></li>
<li>add <code>get_notes</code> by <a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1451"">burnash/gspread#1451</a></li>
<li>Bump mypy from 1.9.0 to 1.10.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1459"">burnash/gspread#1459</a></li>
<li>Bump black from 24.4.0 to 24.4.2 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1460"">burnash/gspread#1460</a></li>
<li>bugfix: handle domain name in spreadsheet copy permissions by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1458"">burnash/gspread#1458</a></li>
<li>Fix/api key auth version by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1463"">burnash/gspread#1463</a></li>
<li>Ignore pip vulnerabilities in CI. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1464"">burnash/gspread#1464</a></li>
<li>Remove StrEnum dependency and added custom class[issue <a href=""https://redirect.github.com/burnash/gspread/issues/1462"">#1462</a>] by <a href=""https://github.com/muddi900""><code>@​muddi900</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1469"">burnash/gspread#1469</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/alexmalins""><code>@​alexmalins</code></a> made their first contribution in <a href=""https://redirect.github.com/burnash/gspread/pull/1449"">burnash/gspread#1449</a></li>
<li><a href=""https://github.com/yhay81""><code>@​yhay81</code></a> made their first contribution in <a href=""https://redirect.github.com/burnash/gspread/pull/1455"">burnash/gspread#1455</a></li>
<li><a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> made their first contribution in <a href=""https://redirect.github.com/burnash/gspread/pull/1451"">burnash/gspread#1451</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/burnash/gspread/compare/v6.1.0...v6.1.1"">https://github.com/burnash/gspread/compare/v6.1.0...v6.1.1</a></p>
<h2>v6.1.0</h2>
<h1>Major changes</h1>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/burnash/gspread/blob/v6.1.3/HISTORY.rst"">gspread's changelog</a>.</em></p>
<blockquote>
<h2>6.1.3 (2024-10-03)</h2>
<ul>
<li>ignore jinja CVE by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1481"">burnash/gspread#1481</a></li>
<li>Remove passing exception as args to super in APIError by <a href=""https://github.com/mike-flowers-airbnb""><code>@​mike-flowers-airbnb</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1477"">burnash/gspread#1477</a></li>
<li>better handler API error parsing. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1510"">burnash/gspread#1510</a></li>
<li>Add test on receiving an invalid JSON in the APIError exception handler. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1512"">burnash/gspread#1512</a></li>
</ul>
<h2>6.1.2 (2024-05-17)</h2>
<ul>
<li>add note about runnings tests to contrib guide by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1465"">burnash/gspread#1465</a></li>
<li>Some updates on <code>get_notes</code> by <a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1461"">burnash/gspread#1461</a></li>
</ul>
<h2>6.1.1 (2024-05-16)</h2>
<ul>
<li>Add some missing typing in code by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1448"">burnash/gspread#1448</a></li>
<li>More fixes for <code>Worksheet.update</code> argument ordering &amp; single cell updating (i.e. now <code>Worksheet.update_acell</code>) by <a href=""https://github.com/alexmalins""><code>@​alexmalins</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1449"">burnash/gspread#1449</a></li>
<li>Added 'add_data_validation<code>to</code>Workhsheet` [Issue <a href=""https://redirect.github.com/burnash/gspread/issues/1420"">#1420</a>] by <a href=""https://github.com/muddi900""><code>@​muddi900</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1444"">burnash/gspread#1444</a></li>
<li>Bump typing-extensions from 4.10.0 to 4.11.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1450"">burnash/gspread#1450</a></li>
<li>Bump black from 23.3.0 to 24.4.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1452"">burnash/gspread#1452</a></li>
<li>Fix incorrect version number in HISTORY.rst from 6.0.1 to 6.1.0 by <a href=""https://github.com/yhay81""><code>@​yhay81</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1455"">burnash/gspread#1455</a></li>
<li>add <code>get_notes</code> by <a href=""https://github.com/nbwzx""><code>@​nbwzx</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1451"">burnash/gspread#1451</a></li>
<li>Bump mypy from 1.9.0 to 1.10.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1459"">burnash/gspread#1459</a></li>
<li>Bump black from 24.4.0 to 24.4.2 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1460"">burnash/gspread#1460</a></li>
<li>bugfix: handle domain name in spreadsheet copy permissions by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1458"">burnash/gspread#1458</a></li>
<li>Fix/api key auth version by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1463"">burnash/gspread#1463</a></li>
<li>Ignore pip vulnerabilities in CI. by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1464"">burnash/gspread#1464</a></li>
<li>Remove StrEnum dependency and added custom class[issue <a href=""https://redirect.github.com/burnash/gspread/issues/1462"">#1462</a>] by <a href=""https://github.com/muddi900""><code>@​muddi900</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1469"">burnash/gspread#1469</a></li>
</ul>
<h2>6.1.0 (2024-03-28)</h2>
<ul>
<li>Add py.typed marker by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1422"">burnash/gspread#1422</a></li>
<li>Improve back-off client by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1415"">burnash/gspread#1415</a></li>
<li>Add new auth method API key by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1428"">burnash/gspread#1428</a></li>
<li>Bugfix/add set timeout by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1417"">burnash/gspread#1417</a></li>
<li>Fix wrapper <code>cast_to_a1_notation</code> by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1427"">burnash/gspread#1427</a></li>
<li>Bump bandit from 1.7.5 to 1.7.8 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1433"">burnash/gspread#1433</a></li>
<li>Bump mypy from 1.6.1 to 1.9.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1432"">burnash/gspread#1432</a></li>
<li>Bump typing-extensions from 4.8.0 to 4.10.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1424"">burnash/gspread#1424</a></li>
<li>Bump flake8 from 5.0.4 to 7.0.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1375"">burnash/gspread#1375</a></li>
<li>fix error message readability by <a href=""https://github.com/imrehg""><code>@​imrehg</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1435"">burnash/gspread#1435</a></li>
<li>Add missing method <code>import_csv()</code> by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1426"">burnash/gspread#1426</a></li>
<li>update readme examples by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1431"">burnash/gspread#1431</a></li>
<li>Add user friendly message when we can't override a test cassette by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1438"">burnash/gspread#1438</a></li>
<li>Allow &quot;warning&quot; type protected ranges by <a href=""https://github.com/alifeee""><code>@​alifeee</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1439"">burnash/gspread#1439</a></li>
<li>Improve README and documentation with value render options by <a href=""https://github.com/lavigne958""><code>@​lavigne958</code></a> in <a href=""https://redirect.github.com/burnash/gspread/pull/1446"">burnash/gspread#1446</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/burnash/gspread/commit/3476d9f69f50bb9491ab23ce3364f7388bbc201e""><code>3476d9f</code></a> add verbose output to publish action</li>
<li><a href=""https://github.com/burnash/gspread/commit/0c85a30fbf11fbe744b25ed708888148d206b36e""><code>0c85a30</code></a> Release v6.1.3</li>
<li><a href=""https://github.com/burnash/gspread/commit/5967ab7b50cecd51dc68a6f1a9530c44bd24d962""><code>5967ab7</code></a> ignore jinja CVE</li>
<li><a href=""https://github.com/burnash/gspread/commit/75039b9faf523bb0558339ea64a883fca95188ae""><code>75039b9</code></a> Add test on receiving an invalid JSON in the APIError exception handler.</li>
<li><a href=""https://github.com/burnash/gspread/commit/9917d072cc8d96d506cb72538366382b30976452""><code>9917d07</code></a> better handler API error parsing.</li>
<li><a href=""https://github.com/burnash/gspread/commit/a3e1ccd2a5e7a11fd96e3dce4000f0bb08d45ad4""><code>a3e1ccd</code></a> Refactor to implement <strong>reduce</strong> instead</li>
<li><a href=""https://github.com/burnash/gspread/commit/d7fef8f19a0844daae7410db92e7183b847e13a1""><code>d7fef8f</code></a> Remove obsolete function</li>
<li><a href=""https://github.com/burnash/gspread/commit/6fc84b7ad8cea00bedc0d60e3eaca3fe012036ad""><code>6fc84b7</code></a> Add nosec comments</li>
<li><a href=""https://github.com/burnash/gspread/commit/3319164dc6fd001d0b3733b737f4cee9471903c0""><code>3319164</code></a> Remove passing exception as args to super in APIError</li>
<li><a href=""https://github.com/burnash/gspread/commit/abb8b1094eee9c4fe41a243faec2b7578fced6b1""><code>abb8b10</code></a> Release v6.1.2</li>
<li>Additional commits viewable in <a href=""https://github.com/burnash/gspread/compare/v3.7.0...v6.1.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=gspread&package-manager=pip&previous-version=3.7.0&new-version=6.1.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],1
29,Bump google-cloud-storage from 2.16.0 to 2.18.2,https://api.github.com/repos/move-coop/parsons/issues/1162,,1162,open,2024-10-10 19:51:45+00:00,2024-10-15 20:39:08+00:00,,"Bumps [google-cloud-storage](https://github.com/googleapis/python-storage) from 2.16.0 to 2.18.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/googleapis/python-storage/releases"">google-cloud-storage's releases</a>.</em></p>
<blockquote>
<h2>v2.18.2</h2>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.18.1...v2.18.2"">2.18.2</a> (2024-08-08)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Add regression test for range read retry issue and bump dependency to fix (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1338"">#1338</a>) (<a href=""https://github.com/googleapis/python-storage/commit/0323647d768b3be834cfab53efb3c557a47d41c3"">0323647</a>)</li>
</ul>
<h2>v2.18.1</h2>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.18.0...v2.18.1"">2.18.1</a> (2024-08-05)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Properly escape URL construction for XML MPU API (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1333"">#1333</a>) (<a href=""https://github.com/googleapis/python-storage/commit/bf4d0e0a2ef1d608d679c22b13d8f5d90b39c7b2"">bf4d0e0</a>)</li>
</ul>
<h2>v2.18.0</h2>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.17.0...v2.18.0"">2.18.0</a> (2024-07-09)</h2>
<h3>Features</h3>
<ul>
<li>Add OpenTelemetry Tracing support as a preview feature (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1288"">#1288</a>) (<a href=""https://github.com/googleapis/python-storage/commit/c2ab0e035b179a919b27c7f50318472f14656e00"">c2ab0e0</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Allow Protobuf 5.x (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1317"">#1317</a>) (<a href=""https://github.com/googleapis/python-storage/commit/152b249472a09342777237d47b6c09f99c2d28e6"">152b249</a>)</li>
<li>Correct notification error message (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1290"">#1290</a>) (<a href=""https://github.com/googleapis/python-storage/commit/1cb977daa2d97c255a382ce81f56a43168b0637d"">1cb977d</a>), closes <a href=""https://redirect.github.com/googleapis/python-storage/issues/1289"">#1289</a></li>
</ul>
<h2>v2.17.0</h2>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.16.0...v2.17.0"">2.17.0</a> (2024-05-22)</h2>
<h3>Features</h3>
<ul>
<li>Support HNS enablement in bucket metadata (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1278"">#1278</a>) (<a href=""https://github.com/googleapis/python-storage/commit/add3c01f0974e22df7f0b50504d5e83e4235fd81"">add3c01</a>)</li>
<li>Support page_size in bucket.list_blobs (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1275"">#1275</a>) (<a href=""https://github.com/googleapis/python-storage/commit/c52e882f65583a7739392926308cc34984561165"">c52e882</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Remove deprecated methods in samples and tests (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1274"">#1274</a>) (<a href=""https://github.com/googleapis/python-storage/commit/4db96c960b07e503c1031c9fa879cf2af195f513"">4db96c9</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Reference Storage Control in readme (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1254"">#1254</a>) (<a href=""https://github.com/googleapis/python-storage/commit/3d6d3693d5c1b24cd3d2bbdeabfd78b8bfd4161a"">3d6d369</a>)</li>
<li>Update DEFAULT_RETRY_IF_GENERATION_SPECIFIED docstrings (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1234"">#1234</a>) (<a href=""https://github.com/googleapis/python-storage/commit/bdd426adf5901faa36115885af868ef50e356a36"">bdd426a</a>)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/googleapis/python-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>
<blockquote>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.18.1...v2.18.2"">2.18.2</a> (2024-08-08)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Add regression test for range read retry issue and bump dependency to fix (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1338"">#1338</a>) (<a href=""https://github.com/googleapis/python-storage/commit/0323647d768b3be834cfab53efb3c557a47d41c3"">0323647</a>)</li>
</ul>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.18.0...v2.18.1"">2.18.1</a> (2024-08-05)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Properly escape URL construction for XML MPU API (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1333"">#1333</a>) (<a href=""https://github.com/googleapis/python-storage/commit/bf4d0e0a2ef1d608d679c22b13d8f5d90b39c7b2"">bf4d0e0</a>)</li>
</ul>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.17.0...v2.18.0"">2.18.0</a> (2024-07-09)</h2>
<h3>Features</h3>
<ul>
<li>Add OpenTelemetry Tracing support as a preview feature (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1288"">#1288</a>) (<a href=""https://github.com/googleapis/python-storage/commit/c2ab0e035b179a919b27c7f50318472f14656e00"">c2ab0e0</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Allow Protobuf 5.x (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1317"">#1317</a>) (<a href=""https://github.com/googleapis/python-storage/commit/152b249472a09342777237d47b6c09f99c2d28e6"">152b249</a>)</li>
<li>Correct notification error message (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1290"">#1290</a>) (<a href=""https://github.com/googleapis/python-storage/commit/1cb977daa2d97c255a382ce81f56a43168b0637d"">1cb977d</a>), closes <a href=""https://redirect.github.com/googleapis/python-storage/issues/1289"">#1289</a></li>
</ul>
<h2><a href=""https://github.com/googleapis/python-storage/compare/v2.16.0...v2.17.0"">2.17.0</a> (2024-05-22)</h2>
<h3>Features</h3>
<ul>
<li>Support HNS enablement in bucket metadata (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1278"">#1278</a>) (<a href=""https://github.com/googleapis/python-storage/commit/add3c01f0974e22df7f0b50504d5e83e4235fd81"">add3c01</a>)</li>
<li>Support page_size in bucket.list_blobs (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1275"">#1275</a>) (<a href=""https://github.com/googleapis/python-storage/commit/c52e882f65583a7739392926308cc34984561165"">c52e882</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Remove deprecated methods in samples and tests (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1274"">#1274</a>) (<a href=""https://github.com/googleapis/python-storage/commit/4db96c960b07e503c1031c9fa879cf2af195f513"">4db96c9</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Reference Storage Control in readme (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1254"">#1254</a>) (<a href=""https://github.com/googleapis/python-storage/commit/3d6d3693d5c1b24cd3d2bbdeabfd78b8bfd4161a"">3d6d369</a>)</li>
<li>Update DEFAULT_RETRY_IF_GENERATION_SPECIFIED docstrings (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1234"">#1234</a>) (<a href=""https://github.com/googleapis/python-storage/commit/bdd426adf5901faa36115885af868ef50e356a36"">bdd426a</a>)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/googleapis/python-storage/commit/85aa02fc88ddeb7f036957bd5fe96e07e7f56f14""><code>85aa02f</code></a> chore(main): release 2.18.2 (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1339"">#1339</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/0323647d768b3be834cfab53efb3c557a47d41c3""><code>0323647</code></a> Fix: Add regression test for range read retry issue and bump dependency to fi...</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/5c935030751a24f1b5e7a29146f5ec4d5f2ed0ec""><code>5c93503</code></a> chore(main): release 2.18.1 (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1334"">#1334</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/bf4d0e0a2ef1d608d679c22b13d8f5d90b39c7b2""><code>bf4d0e0</code></a> Fix: Properly escape URL construction for XML MPU API (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1333"">#1333</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/3d29c6f8bc3fb4ab02ee2262b094f406326b98e8""><code>3d29c6f</code></a> chore(main): release 2.18.0 (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1291"">#1291</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/d5d3c68a6e5c6f8cefc59892c1ccceaf181ff32d""><code>d5d3c68</code></a> chore(python): use python 3.10 for docs build (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1322"">#1322</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/a4b07b8b99812b8ce1192850f48782f0be7b6a9f""><code>a4b07b8</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1324"">#1324</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/ea47943276fb3227bca5c7eea092f1bb4ce76fb9""><code>ea47943</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1308"">#1308</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/ed7cbbafc29ce462d12cd6b10dd975443967cdea""><code>ed7cbba</code></a> test: harden pytest fixture setup stage (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1323"">#1323</a>)</li>
<li><a href=""https://github.com/googleapis/python-storage/commit/152b249472a09342777237d47b6c09f99c2d28e6""><code>152b249</code></a> fix: Allow Protobuf 5.x (<a href=""https://redirect.github.com/googleapis/python-storage/issues/1317"">#1317</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/googleapis/python-storage/compare/v2.16.0...v2.18.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=google-cloud-storage&package-manager=pip&previous-version=2.16.0&new-version=2.18.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
30,Bump civis from 1.16.1 to 2.3.0 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1161,,1161,closed,2024-10-10 19:51:42+00:00,2024-11-18 11:27:05+00:00,2024-11-18 11:27:04+00:00,"Bumps [civis](https://github.com/civisanalytics/civis-python) from 1.16.1 to 2.3.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/civisanalytics/civis-python/releases"">civis's releases</a>.</em></p>
<blockquote>
<h2>v2.3.0</h2>
<h3>Added</h3>
<ul>
<li>Added a script for checking if the Civis API spec is up-to-date. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/489"">#489</a>)</li>
<li>Added a new keyword argument <code>sql_params_arguments</code> to the <code>civis.io.*</code> functions that
accept a SQL query, so that the user can run a parameterized SQL script. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
</ul>
<h3>Changed</h3>
<ul>
<li>Refactored the <code>civis.parallel</code> module and related unit tests due to major changes
of joblib from v1.2.0 to v1.3.0 (API-breaking changes for dropping
<code>joblib.my_exceptions.TransportableException</code> and <code>joblib.format_stack.format_exc</code>,
as well as the substantial changes to the internals of <code>joblib.Parallel</code>). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
<li>Bumped the minimum required version of <code>joblib</code> to v1.3.0,
which is the version where <code>joblib.parallel_config</code> was introduced and
<code>joblib.parallel_backend</code> was deprecated. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
<li>Improved the startup time of <code>import civis</code> with a 5x speed boost. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/490"">#490</a>, <a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
<li>The downloaded API spec due to the <code>civis.APIClient</code> instantiation is now
a time-to-live cache in memory (15 minutes for interactive Python, or 24 hours in scripts). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/491"">#491</a>)</li>
<li>Polling at <code>PollableResult</code> (and consequently its subclasses as well: <code>CivisFuture</code>,
<code>ContainerFuture</code>, and <code>ModelFuture</code>) now defaults to geometrically increased polling
intervals. Short-running jobs' <code>future.result()</code> can now return faster, while
longer-running jobs have a capped polling interval of 15 seconds. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/492"">#492</a>)</li>
<li>Comparing a <code>Response</code> object with a non-<code>Response</code> object returns <code>False</code> now
(this previously raised a <code>TypeError</code>). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
</ul>
<h3>Fixed</h3>
<ul>
<li>Fixed <code>civis.parallel.make_backend_template_factory</code> so that
keyword arguments are now accepted and passed to <code>client.scripts.post_custom</code>. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
<li>For <code>Response</code> objects, their &quot;repr&quot; form shows the class name &quot;Response&quot; for both
top-level and nested response objects. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
</ul>
<h3>Security</h3>
<ul>
<li>Bumped the minimum required version of <code>requests</code> to the latest v2.32.3,
due to a security vulnerability for &lt; v2.32.0
(<a href=""https://nvd.nist.gov/vuln/detail/CVE-2024-35195"">CVE-2024-35195</a>). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
</ul>
<h2>v2.2.0</h2>
<h2>Added</h2>
<ul>
<li><code>civis.response.Response</code> has its own &quot;repr&quot; and pretty-print format,
instead of the previous dict-like representation that would incorrectly suggest immutability. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/487"">#487</a>)</li>
<li>Added the <code>--version</code> flag to the command line interface. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/487"">#487</a>)</li>
</ul>
<h2>Fixed</h2>
<ul>
<li>Fixed API response objects' <code>.json()</code> for lists. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/487"">#487</a>)</li>
<li>Fixed <code>civis_logger</code> for always having the attribute <code>propagate</code> attribute set to <code>False</code>
so that it can also be used for notebooks and services/apps on Civis Platform. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/487"">#487</a>)</li>
</ul>
<h2>v2.1.0</h2>
<h3>Added</h3>
<ul>
<li>Added <code>.json()</code> at <code>civis.response.Response</code> to return the original JSON data from Civis API. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/486"">#486</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/civisanalytics/civis-python/blob/main/CHANGELOG.md"">civis's changelog</a>.</em></p>
<blockquote>
<h2>2.3.0 - 2024-06-14</h2>
<h3>Added</h3>
<ul>
<li>Added a script for checking if the Civis API spec is up-to-date. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/489"">#489</a>)</li>
<li>Added a new keyword argument <code>sql_params_arguments</code> to the <code>civis.io.*</code> functions that
accept a SQL query, so that the user can run a parameterized SQL script. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
</ul>
<h3>Changed</h3>
<ul>
<li>Refactored the <code>civis.parallel</code> module and related unit tests due to major changes
of joblib from v1.2.0 to v1.3.0 (API-breaking changes for dropping
<code>joblib.my_exceptions.TransportableException</code> and <code>joblib.format_stack.format_exc</code>,
as well as the substantial changes to the internals of <code>joblib.Parallel</code>). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
<li>Bumped the minimum required version of <code>joblib</code> to v1.3.0,
which is the version where <code>joblib.parallel_config</code> was introduced and
<code>joblib.parallel_backend</code> was deprecated. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
<li>Improved the startup time of <code>import civis</code> with a 5x speed boost. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/490"">#490</a>, <a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
<li>The downloaded API spec due to the <code>civis.APIClient</code> instantiation is now
a time-to-live cache in memory (15 minutes for interactive Python, or 24 hours in scripts). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/491"">#491</a>)</li>
<li>Polling at <code>PollableResult</code> (and consequently its subclasses as well: <code>CivisFuture</code>,
<code>ContainerFuture</code>, and <code>ModelFuture</code>) now defaults to geometrically increased polling
intervals. Short-running jobs' <code>future.result()</code> can now return faster, while
longer-running jobs have a capped polling interval of 15 seconds. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/492"">#492</a>)</li>
<li>Comparing a <code>Response</code> object with a non-<code>Response</code> object returns <code>False</code> now
(this previously raised a <code>TypeError</code>). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
</ul>
<h3>Fixed</h3>
<ul>
<li>Fixed <code>civis.parallel.make_backend_template_factory</code> so that
keyword arguments are now accepted and passed to <code>client.scripts.post_custom</code>. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
<li>For <code>Response</code> objects, their &quot;repr&quot; form shows the class name &quot;Response&quot; for both
top-level and nested response objects. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
</ul>
<h3>Security</h3>
<ul>
<li>Bumped the minimum required version of <code>requests</code> to the latest v2.32.3,
due to a security vulnerability for &lt; v2.32.0
(<a href=""https://nvd.nist.gov/vuln/detail/CVE-2024-35195"">CVE-2024-35195</a>). (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
</ul>
<h2>2.2.0 - 2024-05-28</h2>
<h2>Added</h2>
<ul>
<li><code>civis.response.Response</code> has its own &quot;repr&quot; and pretty-print format,
instead of the previous dict-like representation that would incorrectly suggest immutability. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/487"">#487</a>)</li>
<li>Added the <code>--version</code> flag to the command line interface. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/487"">#487</a>)</li>
</ul>
<h2>Fixed</h2>
<ul>
<li>Fixed API response objects' <code>.json()</code> for lists. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/487"">#487</a>)</li>
<li>Fixed <code>civis_logger</code> for always having the attribute <code>propagate</code> attribute set to <code>False</code>
so that it can also be used for notebooks and services/apps on Civis Platform. (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/487"">#487</a>)</li>
</ul>
<h2>2.1.0 - 2024-05-23</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/894d2ea9005abf169835f288f868027407321bcd""><code>894d2ea</code></a> [CIVIS-8695] ENH SQL I/O functions accept SQL parameters and arguments (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/493"">#493</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/a8fac5ece2d30050cbb00bd27d473f1ab49d7710""><code>a8fac5e</code></a> [CIVIS-1949] PERF more efficient polling at future objects (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/492"">#492</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/423d5f1392e1722e9428b0f5e2feb79c5daa9c7b""><code>423d5f1</code></a> [CIVIS-8737] ENH time-to-live cache for API spec (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/491"">#491</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/d6f10c0f59b4b156ea9b1216257da0f55443a89e""><code>d6f10c0</code></a> [CIVIS-8658] PERF speed up startup time (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/490"">#490</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/90dfb4447d9523ed37107a23a9ba3f913bc004f8""><code>90dfb44</code></a> [CIVIS-1824] ENH add script to check if Civis API spec has updated (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/489"">#489</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/fc7e355996ecb63eb1241597a48cc64f2c6d15e8""><code>fc7e355</code></a> [CIVIS-6486] ENH <code>civis.parallel</code> compatible with joblib &gt;= v1.3.0 (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/488"">#488</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/3edbfe57cd3289a9ea7d6e3b521eceac56c0c7a1""><code>3edbfe5</code></a> [CIVIS-8660] ENH response objects: add repr and pprint, fix lists for `.json(...</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/cae86765b5089ff2babcd997635803249a62840c""><code>cae8676</code></a> [CIVIS-8647] ENH response objects: add <code>.json()</code>, preserve key casing under '...</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/62b26eae5bdc1c1604bf3bd7c4cc55b67d967800""><code>62b26ea</code></a> SEC bump requests version in doc reqs (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/485"">#485</a>)</li>
<li><a href=""https://github.com/civisanalytics/civis-python/commit/9eae3c492f81d211676074d9005205bc6d23c956""><code>9eae3c4</code></a> [CIVIS-8476] DOC configure &quot;readthedocs&quot; builds and refresh docs (<a href=""https://redirect.github.com/civisanalytics/civis-python/issues/483"">#483</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/civisanalytics/civis-python/compare/v1.16.1...v2.3.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=civis&package-manager=pip&previous-version=1.16.1&new-version=2.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],1
31,Bump grpcio from 1.62.2 to 1.66.2 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1160,,1160,closed,2024-10-10 19:51:37+00:00,2024-10-21 10:22:38+00:00,2024-10-21 10:22:37+00:00,"Bumps [grpcio](https://github.com/grpc/grpc) from 1.62.2 to 1.66.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/grpc/grpc/releases"">grpcio's releases</a>.</em></p>
<blockquote>
<h2>Release v1.66.2</h2>
<p>This is release gRPC Core 1.66.2 (gladiator).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes.</p>
<h2>What's Changed</h2>
<h3>Python</h3>
<ul>
<li>Added support for Python 3.13 and templating system for supported Python versions (<a href=""https://redirect.github.com/grpc/grpc/issues/37643"">#37643</a>)</li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/grpc/grpc/compare/v1.66.1...v1.66.2"">https://github.com/grpc/grpc/compare/v1.66.1...v1.66.2</a></p>
<h2>Release v1.66.1</h2>
<p>This is release gRPC Core 1.66.1 (gladiator).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes.</p>
<h1>Core</h1>
<ul>
<li>Enable EDS dualstack support by default (<a href=""https://redirect.github.com/grpc/grpc/pull/37545"">grpc/grpc#37545</a>)</li>
</ul>
<h2>Release v1.66.0</h2>
<p>This is release 1.66.0 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">gladiator</a>) of gRPC Core.</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>
<h2>Core</h2>
<ul>
<li>[Python Otel] Manage call tracer life cycle use call arena. (v1.66.x backport). (<a href=""https://redirect.github.com/grpc/grpc/pull/37479"">#37479</a>)</li>
<li>[BoringSSL] Update third_party/boringssl-with-bazel. (<a href=""https://redirect.github.com/grpc/grpc/pull/37223"">#37223</a>)</li>
<li>[Dep] Upgrading Protobuf to v27.2. (<a href=""https://redirect.github.com/grpc/grpc/pull/36753"">#36753</a>)</li>
<li>[Gpr_To_Absl_Logging] Fixing bugs . (<a href=""https://redirect.github.com/grpc/grpc/pull/36961"">#36961</a>)</li>
<li>[chttp2] don't access endpoint in transport ops if it's already been destroyed. (<a href=""https://redirect.github.com/grpc/grpc/pull/36921"">#36921</a>)</li>
</ul>
<h2>C++</h2>
<ul>
<li>[OTel C++] Fix race when adding and removing callbacks (<a href=""https://redirect.github.com/grpc/grpc/issues/37485"">#37485</a>). (<a href=""https://redirect.github.com/grpc/grpc/pull/37508"">#37508</a>)</li>
<li>[RlsLB] Fix Deadlock (<a href=""https://redirect.github.com/grpc/grpc/issues/37459"">#37459</a>). (<a href=""https://redirect.github.com/grpc/grpc/pull/37502"">#37502</a>)</li>
</ul>
<h2>Python</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/grpc/grpc/commit/f686ffe7e703fb1440dabea419579e566a8becc3""><code>f686ffe</code></a> [Release] Bump version to 1.66.2 (on v1.66.x branch) (<a href=""https://redirect.github.com/grpc/grpc/issues/37812"">#37812</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/1ce45edbc462feefd5b0cca449ea81e425d7c099""><code>1ce45ed</code></a> [Backport 1.66.x] increased timeout for armv7 artifact build to 2 hours (<a href=""https://redirect.github.com/grpc/grpc/issues/378"">#378</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/4ca1787dcac3f32fdb2fafc33e4fa119a15c53bf""><code>4ca1787</code></a> [Backport to v1.66.x] Add templating and support for Python 3.13 (<a href=""https://redirect.github.com/grpc/grpc/issues/37643"">#37643</a>) (<a href=""https://redirect.github.com/grpc/grpc/issues/3"">#3</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/e821cdc231bda9ee93139a6daab6311dd8953832""><code>e821cdc</code></a> [Release] Bump version to 1.66.1 (on v1.66.x branch) (<a href=""https://redirect.github.com/grpc/grpc/issues/37570"">#37570</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/c8ada9c7ee26074392f7cacefda01d1016d9f9d7""><code>c8ada9c</code></a> [dualstack] enable EDS dualstack support by default (backport to 1.66.x) (<a href=""https://redirect.github.com/grpc/grpc/issues/37"">#37</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/13cecab1c4f45902197a9f8fe4e787eb9c4d4db1""><code>13cecab</code></a> [Release] Bump version to 1.66.0 (on v1.66.x branch) (<a href=""https://redirect.github.com/grpc/grpc/issues/37554"">#37554</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/56a812239b435013093614e1e72a4b41103386a6""><code>56a8122</code></a> [Release] Bump version to 1.66.0-pre5 (on v1.66.x branch) (<a href=""https://redirect.github.com/grpc/grpc/issues/37517"">#37517</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/c0f06b99b3dc7b7de01401d255fd372e7291444c""><code>c0f06b9</code></a> [OTel C++] Fix race when adding and removing callbacks (<a href=""https://redirect.github.com/grpc/grpc/issues/37485"">#37485</a>) (<a href=""https://redirect.github.com/grpc/grpc/issues/37508"">#37508</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/e53a314b0179856e4e0b9a51d4099a849f2b5492""><code>e53a314</code></a> [RlsLB] Fix Deadlock (<a href=""https://redirect.github.com/grpc/grpc/issues/37459"">#37459</a>) (<a href=""https://redirect.github.com/grpc/grpc/issues/37502"">#37502</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/bee26a7434d861c047c7649d5b650deb41c2cd71""><code>bee26a7</code></a> [Release] Bump version to 1.66.0-pre4 (on v1.66.x branch) (<a href=""https://redirect.github.com/grpc/grpc/issues/37501"">#37501</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/grpc/grpc/compare/v1.62.2...v1.66.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=grpcio&package-manager=pip&previous-version=1.62.2&new-version=1.66.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],1
32,Bump python from 3.8 to 3.13,https://api.github.com/repos/move-coop/parsons/issues/1159,,1159,open,2024-10-10 19:51:35+00:00,2024-10-10 20:03:20+00:00,,"Bumps python from 3.8 to 3.13.


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=python&package-manager=docker&previous-version=3.8&new-version=3.13)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""docker"")]",[],0
33,Bump ossf/scorecard-action from 2.3.1 to 2.4.0,https://api.github.com/repos/move-coop/parsons/issues/1158,,1158,open,2024-10-10 19:51:24+00:00,2024-10-10 19:51:25+00:00,,"Bumps [ossf/scorecard-action](https://github.com/ossf/scorecard-action) from 2.3.1 to 2.4.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/ossf/scorecard-action/releases"">ossf/scorecard-action's releases</a>.</em></p>
<blockquote>
<h2>v2.4.0</h2>
<h2>What's Changed</h2>
<p>This update bumps the Scorecard version to the v5 release. For a complete list of changes, please refer to the <a href=""https://github.com/ossf/scorecard/releases/tag/v5.0.0"">v5.0.0 release notes</a>. Of special note to Scorecard Action is the Maintainer Annotation feature, which can be used to suppress some Code Scanning false positives. Alerts will not be generated for any Scorecard Check with an annotation.</p>
<ul>
<li>:seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0-rc2 to v5.0.0 by <a href=""https://github.com/spencerschrock""><code>@​spencerschrock</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1410"">ossf/scorecard-action#1410</a></li>
<li>:bug: lower license sarif alert threshold to 9 by <a href=""https://github.com/spencerschrock""><code>@​spencerschrock</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1411"">ossf/scorecard-action#1411</a></li>
</ul>
<h3>Documentation</h3>
<ul>
<li>docs: dogfooding badge by <a href=""https://github.com/jkowalleck""><code>@​jkowalleck</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1399"">ossf/scorecard-action#1399</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/jkowalleck""><code>@​jkowalleck</code></a> made their first contribution in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1399"">ossf/scorecard-action#1399</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/ossf/scorecard-action/compare/v2.3.3...v2.4.0"">https://github.com/ossf/scorecard-action/compare/v2.3.3...v2.4.0</a></p>
<h2>v2.3.3</h2>
<blockquote>
<p>[!NOTE]<br />
There is no v2.3.2 release as a step was skipped in the release process. This was fixed and re-released under the v2.3.3 tag</p>
</blockquote>
<h2>What's Changed</h2>
<ul>
<li>:seedling: Bump github.com/ossf/scorecard/v4 (v4.13.1) to github.com/ossf/scorecard/v5 (v5.0.0-rc1) by <a href=""https://github.com/spencerschrock""><code>@​spencerschrock</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1366"">ossf/scorecard-action#1366</a></li>
<li>:seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0-rc1 to v5.0.0-rc2 by <a href=""https://github.com/spencerschrock""><code>@​spencerschrock</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1374"">ossf/scorecard-action#1374</a></li>
<li>:seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0-rc2 to v5.0.0-rc2.0.20240509182734-7ce860946928 by <a href=""https://github.com/spencerschrock""><code>@​spencerschrock</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1377"">ossf/scorecard-action#1377</a></li>
</ul>
<p>For a full changelist of what these include, see the <a href=""https://github.com/ossf/scorecard/releases/tag/v5.0.0-rc1"">v5.0.0-rc1</a> and <a href=""https://github.com/ossf/scorecard/releases/tag/v5.0.0-rc2"">v5.0.0-rc2</a> release notes.</p>
<h3>Documentation</h3>
<ul>
<li>:book: Move token discussion out of main README. by <a href=""https://github.com/spencerschrock""><code>@​spencerschrock</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1279"">ossf/scorecard-action#1279</a></li>
<li>:book: link to <code>ossf/scorecard</code> workflow instead of maintaining an example by <a href=""https://github.com/spencerschrock""><code>@​spencerschrock</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1352"">ossf/scorecard-action#1352</a></li>
<li>:book: update api links to new scorecard.dev site by <a href=""https://github.com/spencerschrock""><code>@​spencerschrock</code></a> in <a href=""https://redirect.github.com/ossf/scorecard-action/pull/1376"">ossf/scorecard-action#1376</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/ossf/scorecard-action/compare/v2.3.1...v2.3.3"">https://github.com/ossf/scorecard-action/compare/v2.3.1...v2.3.3</a></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/ossf/scorecard-action/commit/62b2cac7ed8198b15735ed49ab1e5cf35480ba46""><code>62b2cac</code></a> bump docker tag to v2.4.0 for release (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1414"">#1414</a>)</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/c09630c42e97d04c7cd8f69735ddf0ec53f0e189""><code>c09630c</code></a> lower license score alert threshold to 9 (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1411"">#1411</a>)</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/cf8594c5485256008de4ec57c936bd4a1a381a0b""><code>cf8594c</code></a> :seedling: Bump github.com/sigstore/cosign/v2 from 2.2.4 to 2.3.0 (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1413"">#1413</a>)</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/de5fcb95b9d8f899bc5dc11b4e202eb6a2fd67e9""><code>de5fcb9</code></a> :seedling: Bump the github-actions group with 2 updates (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1412"">#1412</a>)</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/a46b90b4caca61e2298cc4a9bd4c90d3dfe7f09d""><code>a46b90b</code></a> bump scorecard to v5.0.0 release (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1410"">#1410</a>)</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/9fc518d5249b2564cbeb11d029b87d7d1ba55396""><code>9fc518d</code></a> :seedling: Bump golang in the docker-images group (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1407"">#1407</a>)</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/a8eaa1b46e3fd7e003f79fd39dff99ca53bbe732""><code>a8eaa1b</code></a> :seedling: Bump the github-actions group with 2 updates (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1408"">#1408</a>)</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/873d5fdf63bc863d140f57ed481e6a297324030b""><code>873d5fd</code></a> :seedling: Bump the github-actions group across 1 directory with 2 updates (#...</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/54cc1fe4e2c7bc69051a267c8e183497ca7d8da7""><code>54cc1fe</code></a> :seedling: Bump the docker-images group with 2 updates (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1401"">#1401</a>)</li>
<li><a href=""https://github.com/ossf/scorecard-action/commit/82bcb91c5d3f72aaf692a0d3e399c425a29ac512""><code>82bcb91</code></a> :seedling: Bump golang.org/x/net from 0.26.0 to 0.27.0 (<a href=""https://redirect.github.com/ossf/scorecard-action/issues/1400"">#1400</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/ossf/scorecard-action/compare/0864cf19026789058feabb7e87baa5f140aac736...62b2cac7ed8198b15735ed49ab1e5cf35480ba46"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ossf/scorecard-action&package-manager=github_actions&previous-version=2.3.1&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
34,Bump pygithub from 1.51 to 2.4.0,https://api.github.com/repos/move-coop/parsons/issues/1157,,1157,closed,2024-10-10 19:51:22+00:00,2024-10-16 15:12:48+00:00,2024-10-16 15:12:39+00:00,"Bumps [pygithub](https://github.com/pygithub/pygithub) from 1.51 to 2.4.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/pygithub/pygithub/releases"">pygithub's releases</a>.</em></p>
<blockquote>
<h2>v2.4.0</h2>
<h2>New features</h2>
<ul>
<li>Allow custom authentication <a href=""https://github.com/kliem""><code>@​kliem</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2987"">#2987</a>)</li>
</ul>
<h2>Improvements</h2>
<ul>
<li>Add <code>has_discussions</code> to <code>AuthenticatedUser</code> and <code>Repository</code> classes <a href=""https://github.com/cwlls""><code>@​cwlls</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3020"">#3020</a>)</li>
<li>Update more <code>SecurityAndAnalysis</code> attributes <a href=""https://github.com/squatched""><code>@​squatched</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3025"">#3025</a>)</li>
<li>Implement support for re-running only failed workflow jobs. <a href=""https://github.com/chrisgavin""><code>@​chrisgavin</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2983"">#2983</a>)</li>
<li>Add possibility to mark a thread/notification as done <a href=""https://github.com/m42e""><code>@​m42e</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2985"">#2985</a>)</li>
<li>Add &quot;pull_request_review_id&quot; to PullRequestComment object <a href=""https://github.com/stroebs""><code>@​stroebs</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3000"">#3000</a>)</li>
<li>Add minimize and unminimize functions for IssueComment class <a href=""https://github.com/arash77""><code>@​arash77</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3005"">#3005</a>)</li>
<li>Support Organization/Repository custom properties <a href=""https://github.com/jackylamhk""><code>@​jackylamhk</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2968"">#2968</a>)</li>
<li>Add <code>dict</code> type to <code>add_attribute</code> script <a href=""https://github.com/jackylamhk""><code>@​jackylamhk</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2977"">#2977</a>)</li>
<li>Allow for deleting and restoring branch associated with PR <a href=""https://github.com/austinsasko""><code>@​austinsasko</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/1784"">#1784</a>)</li>
<li>Add &quot;archived_at&quot; to Organization object. <a href=""https://github.com/billnapier""><code>@​billnapier</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2974"">#2974</a>)</li>
<li>Adds Security &amp; Analysis To Repository <a href=""https://github.com/squatched""><code>@​squatched</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2960"">#2960</a>)</li>
<li>Add added_by and last_used attributes to RepositoryKey <a href=""https://github.com/ramiro""><code>@​ramiro</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2952"">#2952</a>)</li>
<li>Add <code>make_latest</code> to <code>GitRelease.update_release</code> <a href=""https://github.com/treee111""><code>@​treee111</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2888"">#2888</a>)</li>
<li>Make Commit.files return PaginatedList <a href=""https://github.com/iarspider""><code>@​iarspider</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2939"">#2939</a>)</li>
</ul>
<h2>Bug Fixes</h2>
<ul>
<li>Fix GraphQL Queries with Variables <a href=""https://github.com/kgal-pan""><code>@​kgal-pan</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3002"">#3002</a>)</li>
</ul>
<h2>Maintenance</h2>
<ul>
<li>Remove support for Python 3.7 <a href=""https://github.com/EnricoMi""><code>@​EnricoMi</code></a> <a href=""https://github.com/khneal""><code>@​khneal</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3008"">#3008</a>, <a href=""https://redirect.github.com/pygithub/pygithub/issues/2975"">#2975</a>)</li>
<li>docs: add missing code-block <a href=""https://github.com/kumy""><code>@​kumy</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2982"">#2982</a>)</li>
<li>Update README.md <a href=""https://github.com/KPCOFGS""><code>@​KPCOFGS</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2961"">#2961</a>)</li>
<li>CI: Fix test success job <a href=""https://github.com/EnricoMi""><code>@​EnricoMi</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3010"">#3010</a>)</li>
</ul>
<h2>v2.3.0</h2>
<h2>New features</h2>
<ul>
<li>Support oauth for enterprise <a href=""https://github.com/EnricoMi""><code>@​EnricoMi</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2780"">#2780</a>)</li>
<li>Support creation of Dependabot Organization  and Repository Secrets <a href=""https://github.com/thomascrowley""><code>@​thomascrowley</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2874"">#2874</a>)</li>
</ul>
<h2>Improvements</h2>
<ul>
<li>Create release with optional <code>name</code> and <code>message</code> when <code>generate_release_notes</code> is true <a href=""https://github.com/heitorpolidoro""><code>@​heitorpolidoro</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2868"">#2868</a>)</li>
<li>Add missing attributes to <code>WorkflowJob</code> <a href=""https://github.com/xvega""><code>@​xvega</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2921"">#2921</a>)</li>
<li>Add <code>created</code> and <code>check_suite_id</code> filter for Repository Workflow runs <a href=""https://github.com/treee111""><code>@​treee111</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2891"">#2891</a>)</li>
<li>Assert requester argument type in Auth <a href=""https://github.com/EnricoMi""><code>@​EnricoMi</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2912"">#2912</a>)</li>
</ul>
<h2>Bug Fixes</h2>
<ul>
<li>Revert having allowed values for <code>add_to_collaborators</code> <a href=""https://github.com/jodelasur""><code>@​jodelasur</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2905"">#2905</a>)</li>
</ul>
<h2>Maintenance</h2>
<ul>
<li>Fix imports in authentication docs <a href=""https://github.com/wurstbrot""><code>@​wurstbrot</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2923"">#2923</a>)</li>
<li>CI: add docformatter to precommit <a href=""https://github.com/Borda""><code>@​Borda</code></a> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2614"">#2614</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/PyGithub/PyGithub/blob/main/doc/changes.rst"">pygithub's changelog</a>.</em></p>
<blockquote>
<h2>Version 2.4.0 (August 26, 2024)</h2>
<p>Breaking Changes
^^^^^^^^^^^^^^^^</p>
<ul>
<li>The <code>github.Commit.Commit</code> class provides a <code>files</code> property that used to return a <code>list[github.File.File]</code>,
which has now been changed to <code>PaginatedList[github.File.File]</code>. This breaks user code that assumes a <code>list</code>:</li>
</ul>
<p>.. code-block:: python</p>
<pre><code>files = repo.get_commit(&quot;7266e812ed2976ea36a4303edecfe5d75522343f&quot;).files
no_of_files = len(files)
</code></pre>
<p>This will raise a <code>TypeError: object of type 'PaginatedList' has no len()</code>, as the returned <code>PaginatedList</code>
does not support the <code>len()</code> method. Use the <code>totalCount</code> property instead:</p>
<p>.. code-block:: python</p>
<pre><code>files = repo.get_commit(&quot;7266e812ed2976ea36a4303edecfe5d75522343f&quot;).files
no_of_files = files.totalCount
</code></pre>
<ul>
<li>Removed support for Python 3.7.</li>
</ul>
<p>New features
^^^^^^^^^^^^</p>
<ul>
<li>Allow custom authentication (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2987"">#2987</a>) (32b826fd)</li>
</ul>
<p>Improvements
^^^^^^^^^^^^</p>
<ul>
<li>Add <code>has_discussions</code> to <code>AuthenticatedUser</code> and <code>Repository</code> classes (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3020"">#3020</a>) (75224167)</li>
<li>Update more <code>SecurityAndAnalysis</code> attributes (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3025"">#3025</a>) (fa168279)</li>
<li>Implement support for re-running only failed workflow jobs. (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2983"">#2983</a>) (23e87563)</li>
<li>Add possibility to mark a thread/notification as done (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2985"">#2985</a>) (5ba24379)</li>
<li>Add &quot;pull_request_review_id&quot; to PullRequestComment object (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3000"">#3000</a>) (6a59cf82)</li>
<li>Add minimize and unminimize functions for IssueComment class (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3005"">#3005</a>) (09c4f58e)</li>
<li>Support Organization/Repository custom properties (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2968"">#2968</a>) (c5e6b702)</li>
<li>Add <code>dict</code> type to <code>add_attribute</code> script (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2977"">#2977</a>) (2a04f9cc)</li>
<li>Allow for deleting and restoring branch associated with PR (<a href=""https://redirect.github.com/pygithub/pygithub/issues/1784"">#1784</a>) (4ba1e412)</li>
<li>Add &quot;archived_at&quot; to Organization object. (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2974"">#2974</a>) (cc766a6f)</li>
<li>Adds Security &amp; Analysis To Repository (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2960"">#2960</a>) (f22af54d)</li>
<li>Add added_by and last_used attributes to RepositoryKey (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2952"">#2952</a>) (5dffa64d)</li>
<li>Add <code>make_latest</code> to <code>GitRelease.update_release</code> (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2888"">#2888</a>) (60136105)</li>
<li>Make Commit.files return PaginatedList (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2939"">#2939</a>) (fa885f00)</li>
</ul>
<p>Bug Fixes
^^^^^^^^^</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/85087354078e426125dbbf88041bbaa6f35d8199""><code>8508735</code></a> Release v2.4.0 (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3027"">#3027</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/75224167542e5d976c687d85c925a3747c3625ac""><code>7522416</code></a> Add <code>has_discussions</code> to <code>AuthenticatedUser</code> and <code>Repository</code> classes (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3020"">#3020</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/fa1682796173050ba6f606e05c19b1fe914a2d87""><code>fa16827</code></a> Update more <code>SecurityAndAnalysis</code> attributes (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3025"">#3025</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/23e8756397e9935e6a3307826de29c04cf853e94""><code>23e8756</code></a> Implement support for re-running only failed workflow jobs. (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2983"">#2983</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/d0e05072cf286012295adbb44bf42018d8e0e0fd""><code>d0e0507</code></a> Complete dropping Python 3.7 (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2975"">#2975</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/32b826fdfd1169c43441e5168922941077c25113""><code>32b826f</code></a> Allow custom authentication (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2987"">#2987</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/5ba24379ed47271d24b26f6ebd245c26f1594ec8""><code>5ba2437</code></a> Add possibility to mark a thread/notification as done (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2985"">#2985</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/61d37dceb49af7c857512bb19d86f1c3928f7791""><code>61d37dc</code></a> CI: Fix test success job (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3010"">#3010</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/38197d64e63b7a5d0b814b6b892a5cfc5a90159d""><code>38197d6</code></a> Revert &quot;Add has_discussions field to Repository class&quot; (<a href=""https://redirect.github.com/pygithub/pygithub/issues/3009"">#3009</a>)</li>
<li><a href=""https://github.com/PyGithub/PyGithub/commit/7213cd05a741bf2c4b5dbddc42d3192f91156865""><code>7213cd0</code></a> Add has_discussions field to Repository class (<a href=""https://redirect.github.com/pygithub/pygithub/issues/2995"">#2995</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/pygithub/pygithub/compare/v1.51...v2.4.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pygithub&package-manager=pip&previous-version=1.51&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
35,Bump step-security/harden-runner from 2.9.1 to 2.10.1,https://api.github.com/repos/move-coop/parsons/issues/1156,,1156,closed,2024-10-10 19:51:19+00:00,2024-10-16 15:12:28+00:00,2024-10-16 15:12:20+00:00,"Bumps [step-security/harden-runner](https://github.com/step-security/harden-runner) from 2.9.1 to 2.10.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/step-security/harden-runner/releases"">step-security/harden-runner's releases</a>.</em></p>
<blockquote>
<h2>v2.10.1</h2>
<h2>What's Changed</h2>
<p>Release v2.10.1 by <a href=""https://github.com/varunsh-coder""><code>@​varunsh-coder</code></a> in <a href=""https://redirect.github.com/step-security/harden-runner/pull/463"">step-security/harden-runner#463</a>
Bug fix: Resolves an issue where DNS resolution of .local domains was failing when using a Kind cluster in a GitHub Actions workflow.</p>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/step-security/harden-runner/compare/v2...v2.10.1"">https://github.com/step-security/harden-runner/compare/v2...v2.10.1</a></p>
<h2>v2.10.0</h2>
<h2>What's Changed</h2>
<p>Release v2.10.0 by <a href=""https://github.com/h0x0er""><code>@​h0x0er</code></a> and <a href=""https://github.com/varunsh-coder""><code>@​varunsh-coder</code></a> in <a href=""https://redirect.github.com/step-security/harden-runner/pull/455"">step-security/harden-runner#455</a></p>
<p><strong>ARM Support</strong>: Harden-Runner Enterprise tier now supports GitHub-hosted ARM runners. This includes all the features that apply to previously supported GitHub-hosted x64 Linux runners.</p>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/step-security/harden-runner/compare/v2...v2.10.0"">https://github.com/step-security/harden-runner/compare/v2...v2.10.0</a></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/step-security/harden-runner/commit/91182cccc01eb5e619899d80e4e971d6181294a7""><code>91182cc</code></a> Merge pull request <a href=""https://redirect.github.com/step-security/harden-runner/issues/463"">#463</a> from step-security/rc-14</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/59ec1c63417a5941b4c199e9e522f0756d2ab11b""><code>59ec1c6</code></a> Update agent</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/1d2370362ea3d554c02ef9fa23b17f4a84d7949b""><code>1d23703</code></a> Merge pull request <a href=""https://redirect.github.com/step-security/harden-runner/issues/461"">#461</a> from step-security/varunsh-coder-patch-1</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/b03bddaa058e19a340af7befb2de4ddd76e4a7d7""><code>b03bdda</code></a> Update README.md</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/3d8dd68e5758262d38de5cca3cbdc85addbbc28d""><code>3d8dd68</code></a> Update README.md</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/446798f8213ac2e75931c1b0769676d927801858""><code>446798f</code></a> Merge pull request <a href=""https://redirect.github.com/step-security/harden-runner/issues/455"">#455</a> from step-security/rc-12</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/f0d3b1eb1ba908b9f69f8af7b0ab2ab59e2bfc1c""><code>f0d3b1e</code></a> Update agent</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/b7880a2f96f65eea5c1b21cd4cef1a91b6c32471""><code>b7880a2</code></a> update dist</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/dade49eade63e2277d55bb16749465b195246a6f""><code>dade49e</code></a> Merge pull request <a href=""https://redirect.github.com/step-security/harden-runner/issues/456"">#456</a> from h0x0er/arm-support</li>
<li><a href=""https://github.com/step-security/harden-runner/commit/d6248bed80ff04443fd77c64092e77520ceed2c9""><code>d6248be</code></a> bump enterprise agent version</li>
<li>Additional commits viewable in <a href=""https://github.com/step-security/harden-runner/compare/5c7944e73c4c2a096b17a9cb74d65b6c2bbafbde...91182cccc01eb5e619899d80e4e971d6181294a7"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=step-security/harden-runner&package-manager=github_actions&previous-version=2.9.1&new-version=2.10.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
36,Bump xmltodict from 0.11.0 to 0.14.1 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1155,,1155,closed,2024-10-10 19:51:14+00:00,2024-10-15 20:38:22+00:00,2024-10-15 20:38:13+00:00,"Bumps [xmltodict](https://github.com/martinblech/xmltodict) from 0.11.0 to 0.14.1.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/martinblech/xmltodict/blob/master/CHANGELOG.md"">xmltodict's changelog</a>.</em></p>
<blockquote>
<h2>v0.14.1</h2>
<ul>
<li>Drop support for Python older than 3.6</li>
<li>Additional ruff/Pyflakes/codespell fixes.
<ul>
<li>Thanks <a href=""https://github.com/DimitriPapadopoulos""><code>@​DimitriPapadopoulos</code></a>!</li>
</ul>
</li>
</ul>
<h2>v0.14.0</h2>
<ul>
<li>Drop old Python 2 support leftover code and apply several RUFF code health fixes.
<ul>
<li>Thanks, <a href=""https://github.com/DimitriPapadopoulos""><code>@​DimitriPapadopoulos</code></a>!</li>
</ul>
</li>
<li>Add Python 3.11, 3.12 and 3.13 support and tests.
<ul>
<li>Thanks, <a href=""https://github.com/angvp""><code>@​angvp</code></a>!</li>
</ul>
</li>
<li>Tests in gh-action.
<ul>
<li>Thanks, <a href=""https://github.com/almaz""><code>@​almaz</code></a>.kun!</li>
</ul>
</li>
<li>Remove defusedexpat import.
<ul>
<li>Thanks, <a href=""https://github.com/hanno""><code>@​hanno</code></a>!</li>
</ul>
</li>
<li>Replace deprecated BadZipfile with BadZipFile.
<ul>
<li>Thanks, <a href=""https://github.com/hugovk""><code>@​hugovk</code></a>!</li>
</ul>
</li>
<li>Support indent using integer format, enable <code>python -m unittest tests/*.py</code>.
<ul>
<li>Thanks, <a href=""https://github.com/hiiwave""><code>@​hiiwave</code></a>!</li>
</ul>
</li>
<li>Ensure significant whitespace is not trimmed
<ul>
<li>Thanks, <a href=""https://github.com/trey""><code>@​trey</code></a>.franklin!</li>
</ul>
</li>
<li>added conda installation command
<ul>
<li>Thanks, <a href=""https://github.com/sugatoray""><code>@​sugatoray</code></a>!</li>
</ul>
</li>
<li>fix attributes not appearing in streaming mode
<ul>
<li>Thanks, <a href=""https://github.com/timnguyen001""><code>@​timnguyen001</code></a>!</li>
</ul>
</li>
<li>Fix Travis CI status badge URL</li>
<li>Update push_release.sh to use twine.</li>
</ul>
<h2>v0.13.0</h2>
<ul>
<li>Add install info to readme for openSUSE. (<a href=""https://redirect.github.com/martinblech/xmltodict/issues/205"">#205</a>)
<ul>
<li>Thanks, <a href=""https://github.com/smarlowucf""><code>@​smarlowucf</code></a>!</li>
</ul>
</li>
<li>Support defaultdict for namespace mapping (<a href=""https://redirect.github.com/martinblech/xmltodict/issues/211"">#211</a>)
<ul>
<li>Thanks, <a href=""https://github.com/nathanalderson""><code>@​nathanalderson</code></a>!</li>
</ul>
</li>
<li>parse(generator) is now possible (<a href=""https://redirect.github.com/martinblech/xmltodict/issues/212"">#212</a>)
<ul>
<li>Thanks, <a href=""https://github.com/xandey""><code>@​xandey</code></a>!</li>
</ul>
</li>
<li>Processing comments on parsing from xml to dict (connected to <a href=""https://redirect.github.com/martinblech/xmltodict/issues/109"">#109</a>) (<a href=""https://redirect.github.com/martinblech/xmltodict/issues/221"">#221</a>)
<ul>
<li>Thanks, <a href=""https://github.com/svetazol""><code>@​svetazol</code></a>!</li>
</ul>
</li>
<li>Add expand_iter kw to unparse to expand iterables (<a href=""https://redirect.github.com/martinblech/xmltodict/issues/213"">#213</a>)
<ul>
<li>Thanks, <a href=""https://github.com/claweyenuk""><code>@​claweyenuk</code></a>!</li>
</ul>
</li>
<li>Fixed some typos
<ul>
<li>Thanks, <a href=""https://github.com/timgates42""><code>@​timgates42</code></a> and <a href=""https://github.com/kianmeng""><code>@​kianmeng</code></a>!</li>
</ul>
</li>
<li>Add support for python3.8
<ul>
<li>Thanks, <a href=""https://github.com/t0b3""><code>@​t0b3</code></a>!</li>
</ul>
</li>
<li>Drop Jython/Python 2 and add Python 3.9/3.10.</li>
<li>Drop OrderedDict in Python &gt;= 3.7</li>
<li>Do not use len() to determine if a sequence is empty</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/martinblech/xmltodict/commit/34759c33cb03a7b5d9e0333464acf53ecd35544b""><code>34759c3</code></a> Bump version and update CHANGELOG.</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/e3f7161248ee3a098a92fc74ccc60c8c88874d6c""><code>e3f7161</code></a> Drop Python 3.4 and 3.5</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/cc54376200e76439f1dfffed27a7d83d8724e56b""><code>cc54376</code></a> Fix misspellings found by codespell</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/01cea1e2748aa965b04847a6bce1e3b0e379fece""><code>01cea1e</code></a> Apply ruff/Pyflakes rule F841</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/48b47c95cfd82b77c9073d1026879da0208808cc""><code>48b47c9</code></a> Bump version and update CHANGELOG.</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/94ee05b05667a75ce80dfa35ad04e7c640e8b5de""><code>94ee05b</code></a> Remove unnecessary whitespace.</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/efe60606b4f6b6f41e293008242db3bc9a497d5b""><code>efe6060</code></a> Merge branch 'DimitriPapadopoulos-pyupgrade'</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/5b1b5112239440b17c8cbd7133fde849cfd30260""><code>5b1b511</code></a> Get rid of Python 2 basestring and unicode (<a href=""https://redirect.github.com/martinblech/xmltodict/issues/346"">#346</a>)</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/71b55aa5b4dbd26c99a09032536acf07cd73811e""><code>71b55aa</code></a> Apply ruff/pycodestyle rule E721 (<a href=""https://redirect.github.com/martinblech/xmltodict/issues/347"">#347</a>)</li>
<li><a href=""https://github.com/martinblech/xmltodict/commit/66bd6a893c23d2af5627d626864fbb9c720dcb19""><code>66bd6a8</code></a> In Python 3 import StringIO from io (<a href=""https://redirect.github.com/martinblech/xmltodict/issues/348"">#348</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/martinblech/xmltodict/compare/v0.11.0...v0.14.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=xmltodict&package-manager=pip&previous-version=0.11.0&new-version=0.14.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
37,Bump install-pinned/uv from de03c60d508703a83d3f8f49afcf1249590ecda1 to 03a68782c27167b267490a9392ac24d6b93a2f14,https://api.github.com/repos/move-coop/parsons/issues/1154,,1154,closed,2024-10-10 19:51:14+00:00,2024-10-16 15:12:08+00:00,2024-10-16 15:12:00+00:00,"Bumps [install-pinned/uv](https://github.com/install-pinned/uv) from de03c60d508703a83d3f8f49afcf1249590ecda1 to 03a68782c27167b267490a9392ac24d6b93a2f14.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/install-pinned/uv/commit/03a68782c27167b267490a9392ac24d6b93a2f14""><code>03a6878</code></a> update README.md (uv 0.4.20)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/6a4598757e2d107c4be3779f53edfe37bbb5714c""><code>6a45987</code></a> update pins (uv 0.4.20)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/6297544555d140c4153f931cb7c8057c8c7a5c3c""><code>6297544</code></a> update README.md (uv 0.4.19)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/0e5cdcec01d63949d91fa1c647e94fc47698b3ea""><code>0e5cdce</code></a> update pins (uv 0.4.19)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/9b7eac6e31065c68694e6ecf8138dc5ea773e873""><code>9b7eac6</code></a> update README.md (uv 0.4.18)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/b630e2e8033aada20ae50376ec0dcf136e361224""><code>b630e2e</code></a> update pins (uv 0.4.18)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/ab343aa7baacdaecca61198499e12c94fde1fc31""><code>ab343aa</code></a> update README.md (uv 0.4.17)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/e4a817138c74d71af09f27af2f24b43dd8d9d94f""><code>e4a8171</code></a> update pins (uv 0.4.17)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/203ac86a9a6afd2a42f3c34f40c2908b2d0e4eb1""><code>203ac86</code></a> update README.md (uv 0.4.16)</li>
<li><a href=""https://github.com/install-pinned/uv/commit/12af6bb9b4521a1647e29cf1fc4f45f2f786330f""><code>12af6bb</code></a> update pins (uv 0.4.16)</li>
<li>Additional commits viewable in <a href=""https://github.com/install-pinned/uv/compare/de03c60d508703a83d3f8f49afcf1249590ecda1...03a68782c27167b267490a9392ac24d6b93a2f14"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
38,Bump google-cloud-bigquery from 3.23.1 to 3.26.0 in /docs,https://api.github.com/repos/move-coop/parsons/issues/1153,,1153,closed,2024-10-10 19:51:09+00:00,2024-10-15 20:37:52+00:00,2024-10-15 20:37:45+00:00,"Bumps [google-cloud-bigquery](https://github.com/googleapis/python-bigquery) from 3.23.1 to 3.26.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/googleapis/python-bigquery/releases"">google-cloud-bigquery's releases</a>.</em></p>
<blockquote>
<h2>v3.26.0</h2>
<h2><a href=""https://github.com/googleapis/python-bigquery/compare/v3.25.0...v3.26.0"">3.26.0</a> (2024-09-25)</h2>
<h3>Features</h3>
<ul>
<li>Include LegacyPandasError in init imports (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2014"">#2014</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/3ab5e95984ad521027a4e1efd9f16767403e668d"">3ab5e95</a>)</li>
<li>Use <code>bigquery-magics</code> package for the <code>%%bigquery</code> magic (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1965"">#1965</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/60128a522375823422f238312521a2ce356d9177"">60128a5</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Add docfx to the presubmit configuration and delete docs-presubmit (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1995"">#1995</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/bd83cfd2eb25cec58d59af8048f5188d748b083d"">bd83cfd</a>)</li>
<li>Add warning when encountering unknown field types  (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1989"">#1989</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/8f5a41d283a965ca161019588d3a3b2947b04b5b"">8f5a41d</a>)</li>
<li>Allow protobuf 5.x; require protobuf &gt;=3.20.2; proto-plus &gt;=1.22.3 (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1976"">#1976</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/57bf873474382cc2cb34243b704bc928fa1b64c6"">57bf873</a>)</li>
<li>Do not set job timeout extra property if None (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1987"">#1987</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/edcb79ca69dba30d8102abebb9d53bc76e4882ee"">edcb79c</a>)</li>
<li>Set pyarrow field nullable to False for a BigQuery field in REPEATED mode (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1999"">#1999</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/5352870283ca7d4652aefc73f12645bcf6e1363c"">5352870</a>)</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li>Bump min version of google-api-core and google-cloud-core to 2.x (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1972"">#1972</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/a958732aed7d9bd51ffde3dc0e6cae9ad7455b54"">a958732</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Add short mode query sample &amp; test (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1978"">#1978</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/ba61a8ab0da541ba1940211875d7ea2e9e17dfa8"">ba61a8a</a>)</li>
<li>Improve QueryJobConfig.destination docstring (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2016"">#2016</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/1b4cca0a3cc788a4570705572d5f04172f6b4b24"">1b4cca0</a>)</li>
</ul>
<h2>v3.25.0</h2>
<h2><a href=""https://github.com/googleapis/python-bigquery/compare/v3.24.0...v3.25.0"">3.25.0</a> (2024-06-17)</h2>
<h3>Features</h3>
<ul>
<li>Add prefer_bqstorage_client option for Connection (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1945"">#1945</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/bfdeb3fdbc1d5b26fcd3d1433abfb0be49d12018"">bfdeb3f</a>)</li>
<li>Support load job option ColumnNameCharacterMap (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1952"">#1952</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/7e522eea776cd9a74f8078c4236f63d5ff11f20e"">7e522ee</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Do not overwrite page_size with max_results when start_index is set (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1956"">#1956</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/7d0fceefdf28278c1f2cdaab571de9b235320998"">7d0fcee</a>)</li>
</ul>
<h2>v3.24.0</h2>
<h2><a href=""https://github.com/googleapis/python-bigquery/compare/v3.23.1...v3.24.0"">3.24.0</a> (2024-06-04)</h2>
<h3>Features</h3>
<ul>
<li>Add default timeout for Client.get_job() (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1935"">#1935</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/9fbad767cc228e02040436742d0cb6743d370b90"">9fbad76</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md"">google-cloud-bigquery's changelog</a>.</em></p>
<blockquote>
<h2><a href=""https://github.com/googleapis/python-bigquery/compare/v3.25.0...v3.26.0"">3.26.0</a> (2024-09-25)</h2>
<h3>Features</h3>
<ul>
<li>Include LegacyPandasError in init imports (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2014"">#2014</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/3ab5e95984ad521027a4e1efd9f16767403e668d"">3ab5e95</a>)</li>
<li>Use <code>bigquery-magics</code> package for the <code>%%bigquery</code> magic (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1965"">#1965</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/60128a522375823422f238312521a2ce356d9177"">60128a5</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Add docfx to the presubmit configuration and delete docs-presubmit (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1995"">#1995</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/bd83cfd2eb25cec58d59af8048f5188d748b083d"">bd83cfd</a>)</li>
<li>Add warning when encountering unknown field types  (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1989"">#1989</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/8f5a41d283a965ca161019588d3a3b2947b04b5b"">8f5a41d</a>)</li>
<li>Allow protobuf 5.x; require protobuf &gt;=3.20.2; proto-plus &gt;=1.22.3 (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1976"">#1976</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/57bf873474382cc2cb34243b704bc928fa1b64c6"">57bf873</a>)</li>
<li>Do not set job timeout extra property if None (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1987"">#1987</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/edcb79ca69dba30d8102abebb9d53bc76e4882ee"">edcb79c</a>)</li>
<li>Set pyarrow field nullable to False for a BigQuery field in REPEATED mode (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1999"">#1999</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/5352870283ca7d4652aefc73f12645bcf6e1363c"">5352870</a>)</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li>Bump min version of google-api-core and google-cloud-core to 2.x (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1972"">#1972</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/a958732aed7d9bd51ffde3dc0e6cae9ad7455b54"">a958732</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Add short mode query sample &amp; test (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1978"">#1978</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/ba61a8ab0da541ba1940211875d7ea2e9e17dfa8"">ba61a8a</a>)</li>
<li>Improve QueryJobConfig.destination docstring (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2016"">#2016</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/1b4cca0a3cc788a4570705572d5f04172f6b4b24"">1b4cca0</a>)</li>
</ul>
<h2><a href=""https://github.com/googleapis/python-bigquery/compare/v3.24.0...v3.25.0"">3.25.0</a> (2024-06-17)</h2>
<h3>Features</h3>
<ul>
<li>Add prefer_bqstorage_client option for Connection (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1945"">#1945</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/bfdeb3fdbc1d5b26fcd3d1433abfb0be49d12018"">bfdeb3f</a>)</li>
<li>Support load job option ColumnNameCharacterMap (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1952"">#1952</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/7e522eea776cd9a74f8078c4236f63d5ff11f20e"">7e522ee</a>)</li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>Do not overwrite page_size with max_results when start_index is set (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1956"">#1956</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/7d0fceefdf28278c1f2cdaab571de9b235320998"">7d0fcee</a>)</li>
</ul>
<h2><a href=""https://github.com/googleapis/python-bigquery/compare/v3.23.1...v3.24.0"">3.24.0</a> (2024-06-04)</h2>
<h3>Features</h3>
<ul>
<li>Add default timeout for Client.get_job() (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1935"">#1935</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/9fbad767cc228e02040436742d0cb6743d370b90"">9fbad76</a>)</li>
<li>Add support for map target type in Parquet options (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1919"">#1919</a>) (<a href=""https://github.com/googleapis/python-bigquery/commit/c3f7b237383d4705ed6e720544728c4db61f6c83"">c3f7b23</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/cad34f1afe20bc430c631ba9c2b69e442281d08d""><code>cad34f1</code></a> chore(main): release 3.26.0 (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/1973"">#1973</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/a76af359525ef3c49c958663f81fd24c9d35e1e7""><code>a76af35</code></a> chore(deps): bump fiona from 1.9.6 to 1.10.0 in /samples/geography (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2027"">#2027</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/ba99b12215995448998fccb6691423f4555a73bf""><code>ba99b12</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2029"">#2029</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/ef8e92787941ed23b9b2b5ce7c956bcb3754b995""><code>ef8e927</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2025"">#2025</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/b561aaf6bb744300ca668b37e8cb047dc3d428be""><code>b561aaf</code></a> build(python): release script update (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2024"">#2024</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/255472359f3ed6b6cee06039ebe9059607fd9894""><code>2554723</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2018"">#2018</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/847feb48c26e96fdcb1393458f370c79d4c92fed""><code>847feb4</code></a> chore: adds Python 3.7/3.8 EOL pending deprecation warning (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2007"">#2007</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/1b4cca0a3cc788a4570705572d5f04172f6b4b24""><code>1b4cca0</code></a> docs: improve QueryJobConfig.destination docstring (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2016"">#2016</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/325519afc80133aabe81ca069f2b891ef990acb6""><code>325519a</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2017"">#2017</a>)</li>
<li><a href=""https://github.com/googleapis/python-bigquery/commit/f0a41618f10e754863617e9efa32707814ca895d""><code>f0a4161</code></a> chore(deps): update all dependencies (<a href=""https://redirect.github.com/googleapis/python-bigquery/issues/2005"">#2005</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/googleapis/python-bigquery/compare/v3.23.1...v3.26.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=google-cloud-bigquery&package-manager=pip&previous-version=3.23.1&new-version=3.26.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""python"")]",[],0
39,Bump actions/checkout from 4.1.1 to 4.2.1,https://api.github.com/repos/move-coop/parsons/issues/1152,,1152,closed,2024-10-10 19:51:06+00:00,2024-10-15 20:37:11+00:00,2024-10-15 20:37:04+00:00,"Bumps [actions/checkout](https://github.com/actions/checkout) from 4.1.1 to 4.2.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/actions/checkout/releases"">actions/checkout's releases</a>.</em></p>
<blockquote>
<h2>v4.2.1</h2>
<h2>What's Changed</h2>
<ul>
<li>Check out other refs/* by commit if provided, fall back to ref by <a href=""https://github.com/orhantoy""><code>@​orhantoy</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1924"">actions/checkout#1924</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/Jcambass""><code>@​Jcambass</code></a> made their first contribution in <a href=""https://redirect.github.com/actions/checkout/pull/1919"">actions/checkout#1919</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/actions/checkout/compare/v4.2.0...v4.2.1"">https://github.com/actions/checkout/compare/v4.2.0...v4.2.1</a></p>
<h2>v4.2.0</h2>
<h2>What's Changed</h2>
<ul>
<li>Add Ref and Commit outputs by <a href=""https://github.com/lucacome""><code>@​lucacome</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1180"">actions/checkout#1180</a></li>
<li>Dependabot updates in <a href=""https://redirect.github.com/actions/checkout/pull/1777"">actions/checkout#1777</a> &amp; <a href=""https://redirect.github.com/actions/checkout/pull/1872"">actions/checkout#1872</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/yasonk""><code>@​yasonk</code></a> made their first contribution in <a href=""https://redirect.github.com/actions/checkout/pull/1869"">actions/checkout#1869</a></li>
<li><a href=""https://github.com/lucacome""><code>@​lucacome</code></a> made their first contribution in <a href=""https://redirect.github.com/actions/checkout/pull/1180"">actions/checkout#1180</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/actions/checkout/compare/v4.1.7...v4.2.0"">https://github.com/actions/checkout/compare/v4.1.7...v4.2.0</a></p>
<h2>v4.1.7</h2>
<h2>What's Changed</h2>
<ul>
<li>Bump the minor-npm-dependencies group across 1 directory with 4 updates by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1739"">actions/checkout#1739</a></li>
<li>Bump actions/checkout from 3 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1697"">actions/checkout#1697</a></li>
<li>Check out other refs/* by commit by <a href=""https://github.com/orhantoy""><code>@​orhantoy</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1774"">actions/checkout#1774</a></li>
<li>Pin actions/checkout's own workflows to a known, good, stable version. by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1776"">actions/checkout#1776</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/orhantoy""><code>@​orhantoy</code></a> made their first contribution in <a href=""https://redirect.github.com/actions/checkout/pull/1774"">actions/checkout#1774</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/actions/checkout/compare/v4.1.6...v4.1.7"">https://github.com/actions/checkout/compare/v4.1.6...v4.1.7</a></p>
<h2>v4.1.6</h2>
<h2>What's Changed</h2>
<ul>
<li>Check platform to set archive extension appropriately by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1732"">actions/checkout#1732</a></li>
<li>Update for 4.1.6 release by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1733"">actions/checkout#1733</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/actions/checkout/compare/v4.1.5...v4.1.6"">https://github.com/actions/checkout/compare/v4.1.5...v4.1.6</a></p>
<h2>v4.1.5</h2>
<h2>What's Changed</h2>
<ul>
<li>Update NPM dependencies by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1703"">actions/checkout#1703</a></li>
<li>Bump github/codeql-action from 2 to 3 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1694"">actions/checkout#1694</a></li>
<li>Bump actions/setup-node from 1 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1696"">actions/checkout#1696</a></li>
<li>Bump actions/upload-artifact from 2 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1695"">actions/checkout#1695</a></li>
<li>README: Suggest <code>user.email</code> to be <code>41898282+github-actions[bot]@users.noreply.github.com</code> by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1707"">actions/checkout#1707</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/actions/checkout/compare/v4.1.4...v4.1.5"">https://github.com/actions/checkout/compare/v4.1.4...v4.1.5</a></p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/actions/checkout/blob/main/CHANGELOG.md"">actions/checkout's changelog</a>.</em></p>
<blockquote>
<h1>Changelog</h1>
<h2>v4.2.1</h2>
<ul>
<li>Check out other refs/* by commit if provided, fall back to ref by <a href=""https://github.com/orhantoy""><code>@​orhantoy</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1924"">actions/checkout#1924</a></li>
</ul>
<h2>v4.2.0</h2>
<ul>
<li>Add Ref and Commit outputs by <a href=""https://github.com/lucacome""><code>@​lucacome</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1180"">actions/checkout#1180</a></li>
<li>Dependency updates by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a>- <a href=""https://redirect.github.com/actions/checkout/pull/1777"">actions/checkout#1777</a>, <a href=""https://redirect.github.com/actions/checkout/pull/1872"">actions/checkout#1872</a></li>
</ul>
<h2>v4.1.7</h2>
<ul>
<li>Bump the minor-npm-dependencies group across 1 directory with 4 updates by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1739"">actions/checkout#1739</a></li>
<li>Bump actions/checkout from 3 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1697"">actions/checkout#1697</a></li>
<li>Check out other refs/* by commit by <a href=""https://github.com/orhantoy""><code>@​orhantoy</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1774"">actions/checkout#1774</a></li>
<li>Pin actions/checkout's own workflows to a known, good, stable version. by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1776"">actions/checkout#1776</a></li>
</ul>
<h2>v4.1.6</h2>
<ul>
<li>Check platform to set archive extension appropriately by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1732"">actions/checkout#1732</a></li>
</ul>
<h2>v4.1.5</h2>
<ul>
<li>Update NPM dependencies by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1703"">actions/checkout#1703</a></li>
<li>Bump github/codeql-action from 2 to 3 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1694"">actions/checkout#1694</a></li>
<li>Bump actions/setup-node from 1 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1696"">actions/checkout#1696</a></li>
<li>Bump actions/upload-artifact from 2 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1695"">actions/checkout#1695</a></li>
<li>README: Suggest <code>user.email</code> to be <code>41898282+github-actions[bot]@users.noreply.github.com</code> by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1707"">actions/checkout#1707</a></li>
</ul>
<h2>v4.1.4</h2>
<ul>
<li>Disable <code>extensions.worktreeConfig</code> when disabling <code>sparse-checkout</code> by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1692"">actions/checkout#1692</a></li>
<li>Add dependabot config by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1688"">actions/checkout#1688</a></li>
<li>Bump the minor-actions-dependencies group with 2 updates by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1693"">actions/checkout#1693</a></li>
<li>Bump word-wrap from 1.2.3 to 1.2.5 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1643"">actions/checkout#1643</a></li>
</ul>
<h2>v4.1.3</h2>
<ul>
<li>Check git version before attempting to disable <code>sparse-checkout</code> by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1656"">actions/checkout#1656</a></li>
<li>Add SSH user parameter by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1685"">actions/checkout#1685</a></li>
<li>Update <code>actions/checkout</code> version in <code>update-main-version.yml</code> by <a href=""https://github.com/jww3""><code>@​jww3</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1650"">actions/checkout#1650</a></li>
</ul>
<h2>v4.1.2</h2>
<ul>
<li>Fix: Disable sparse checkout whenever <code>sparse-checkout</code> option is not present <a href=""https://github.com/dscho""><code>@​dscho</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1598"">actions/checkout#1598</a></li>
</ul>
<h2>v4.1.1</h2>
<ul>
<li>Correct link to GitHub Docs by <a href=""https://github.com/peterbe""><code>@​peterbe</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1511"">actions/checkout#1511</a></li>
<li>Link to release page from what's new section by <a href=""https://github.com/cory-miller""><code>@​cory-miller</code></a> in <a href=""https://redirect.github.com/actions/checkout/pull/1514"">actions/checkout#1514</a></li>
</ul>
<h2>v4.1.0</h2>
<ul>
<li><a href=""https://redirect.github.com/actions/checkout/pull/1396"">Add support for partial checkout filters</a></li>
</ul>
<h2>v4.0.0</h2>
<ul>
<li><a href=""https://redirect.github.com/actions/checkout/pull/1067"">Support fetching without the --progress option</a></li>
<li><a href=""https://redirect.github.com/actions/checkout/pull/1436"">Update to node20</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/actions/checkout/commit/eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871""><code>eef6144</code></a> Prepare 4.2.1 release (<a href=""https://redirect.github.com/actions/checkout/issues/1925"">#1925</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/6b42224f41ee5dfe5395e27c8b2746f1f9955030""><code>6b42224</code></a> Add workflow file for publishing releases to immutable action package (<a href=""https://redirect.github.com/actions/checkout/issues/1919"">#1919</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/de5a000abf73b6f4965bd1bcdf8f8d94a56ea815""><code>de5a000</code></a> Check out other refs/* by commit if provided, fall back to ref (<a href=""https://redirect.github.com/actions/checkout/issues/1924"">#1924</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/d632683dd7b4114ad314bca15554477dd762a938""><code>d632683</code></a> Prepare 4.2.0 release (<a href=""https://redirect.github.com/actions/checkout/issues/1878"">#1878</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/6d193bf28034eafb982f37bd894289fe649468fc""><code>6d193bf</code></a> Bump braces from 3.0.2 to 3.0.3 (<a href=""https://redirect.github.com/actions/checkout/issues/1777"">#1777</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/db0cee9a514becbbd4a101a5fbbbf47865ee316c""><code>db0cee9</code></a> Bump the minor-npm-dependencies group across 1 directory with 4 updates (<a href=""https://redirect.github.com/actions/checkout/issues/1872"">#1872</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/b6849436894e144dbce29d7d7fda2ae3bf9d8365""><code>b684943</code></a> Add Ref and Commit outputs (<a href=""https://redirect.github.com/actions/checkout/issues/1180"">#1180</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/2d7d9f7ff5b310f983d059b68785b3c74d8b8edd""><code>2d7d9f7</code></a> Provide explanation for where user email came from (<a href=""https://redirect.github.com/actions/checkout/issues/1869"">#1869</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/9a9194f87191a7e9055e3e9b95b8cfb13023bb08""><code>9a9194f</code></a> Bump docker/build-push-action from 5.3.0 to 6.5.0 (<a href=""https://redirect.github.com/actions/checkout/issues/1832"">#1832</a>)</li>
<li><a href=""https://github.com/actions/checkout/commit/dd960bd3c3f080561a1810e32349ac211ecec7d4""><code>dd960bd</code></a> Bump docker/login-action in the minor-actions-dependencies group (<a href=""https://redirect.github.com/actions/checkout/issues/1831"">#1831</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/actions/checkout/compare/v4.1.1...eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/checkout&package-manager=github_actions&previous-version=4.1.1&new-version=4.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
40,Bump github/codeql-action from 3.26.6 to 3.26.12,https://api.github.com/repos/move-coop/parsons/issues/1151,,1151,closed,2024-10-10 19:50:58+00:00,2024-10-15 20:36:40+00:00,2024-10-15 20:36:32+00:00,"Bumps [github/codeql-action](https://github.com/github/codeql-action) from 3.26.6 to 3.26.12.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/blob/main/CHANGELOG.md"">github/codeql-action's changelog</a>.</em></p>
<blockquote>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>[UNRELEASED]</h2>
<p>No user facing changes.</p>
<h2>3.26.12 - 07 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add a deprecation warning for customers using CodeQL version 2.14.5 and earlier. These versions of CodeQL were discontinued on 24 September 2024 alongside GitHub Enterprise Server 3.10, and will be unsupported by CodeQL Action versions 3.27.0 and later and versions 2.27.0 and later. <a href=""https://redirect.github.com/github/codeql-action/pull/2520"">#2520</a></p>
<ul>
<li>
<p>If you are using one of these versions, please update to CodeQL CLI version 2.14.6 or later. For instance, if you have specified a custom version of the CLI using the 'tools' input to the 'init' Action, you can remove this input to use the default version.</p>
</li>
<li>
<p>Alternatively, if you want to continue using a version of the CodeQL CLI between 2.13.5 and 2.14.5, you can replace <code>github/codeql-action/*@v3</code> by <code>github/codeql-action/*@v3.26.11</code> and <code>github/codeql-action/*@v2</code> by <code>github/codeql-action/*@v2.26.11</code> in your code scanning workflow to ensure you continue using this version of the CodeQL Action.</p>
</li>
</ul>
</li>
</ul>
<h2>3.26.11 - 03 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add support for using <code>actions/download-artifact@v4</code> to programmatically consume CodeQL Action debug artifacts.</p>
<p>Starting November 30, 2024, GitHub.com customers will <a href=""https://github.blog/changelog/2024-04-16-deprecation-notice-v3-of-the-artifact-actions/"">no longer be able to use <code>actions/download-artifact@v3</code></a>. Therefore, to avoid breakage, customers who programmatically download the CodeQL Action debug artifacts should set the <code>CODEQL_ACTION_ARTIFACT_V4_UPGRADE</code> environment variable to <code>true</code> and bump <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> in their workflows. The CodeQL Action will enable this behavior by default in early November and workflows that have not yet bumped to <code>actions/download-artifact@v3</code> to <code>actions/download-artifact@v4</code> will begin failing then.</p>
<p>This change is currently unavailable for GitHub Enterprise Server customers, as <code>actions/upload-artifact@v4</code> and <code>actions/download-artifact@v4</code> are not yet compatible with GHES.</p>
</li>
<li>
<p>Update default CodeQL bundle version to 2.19.1. <a href=""https://redirect.github.com/github/codeql-action/pull/2519"">#2519</a></p>
</li>
</ul>
<h2>3.26.10 - 30 Sep 2024</h2>
<ul>
<li>We are rolling out a feature in September/October 2024 that sets up CodeQL using a bundle compressed with <a href=""http://facebook.github.io/zstd/"">Zstandard</a>. Our aim is to improve the performance of setting up CodeQL. <a href=""https://redirect.github.com/github/codeql-action/pull/2502"">#2502</a></li>
</ul>
<h2>3.26.9 - 24 Sep 2024</h2>
<p>No user facing changes.</p>
<h2>3.26.8 - 19 Sep 2024</h2>
<ul>
<li>Update default CodeQL bundle version to 2.19.0. <a href=""https://redirect.github.com/github/codeql-action/pull/2483"">#2483</a></li>
</ul>
<h2>3.26.7 - 13 Sep 2024</h2>
<ul>
<li>Update default CodeQL bundle version to 2.18.4. <a href=""https://redirect.github.com/github/codeql-action/pull/2471"">#2471</a></li>
</ul>
<h2>3.26.6 - 29 Aug 2024</h2>
<ul>
<li>Update default CodeQL bundle version to 2.18.3. <a href=""https://redirect.github.com/github/codeql-action/pull/2449"">#2449</a></li>
</ul>
<h2>3.26.5 - 23 Aug 2024</h2>
<ul>
<li>Fix an issue where the <code>csrutil</code> system call used for telemetry would fail on MacOS ARM machines with System Integrity Protection disabled. <a href=""https://redirect.github.com/github/codeql-action/pull/2441"">#2441</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/github/codeql-action/commit/c36620d31ac7c881962c3d9dd939c40ec9434f2b""><code>c36620d</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2529"">#2529</a> from github/update-v3.26.12-c9a70ff45</li>
<li><a href=""https://github.com/github/codeql-action/commit/570aecb95f2b62832269f4d9ed8d228c9a1342fb""><code>570aecb</code></a> Update changelog for v3.26.12</li>
<li><a href=""https://github.com/github/codeql-action/commit/c9a70ff45f6a0ebf67a02cf3a09094b72f56e5cb""><code>c9a70ff</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2526"">#2526</a> from github/henrymercer/check-zstd-on-path</li>
<li><a href=""https://github.com/github/codeql-action/commit/d65a17605a400f2b42f1e4785239bc63a91419b9""><code>d65a176</code></a> Rebuild</li>
<li><a href=""https://github.com/github/codeql-action/commit/bf2e624d0b3b15a9fe5c6ae1294f207a8f2ee3f1""><code>bf2e624</code></a> Update src/tar.ts</li>
<li><a href=""https://github.com/github/codeql-action/commit/56d197570aa047eae7fe04401603196e2f68521d""><code>56d1975</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2489"">#2489</a> from github/redsun82/rust</li>
<li><a href=""https://github.com/github/codeql-action/commit/7cf65a5b2e089b7207c678633bc4a42884847231""><code>7cf65a5</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2518"">#2518</a> from github/dependabot/npm_and_yarn/npm-88156698cd</li>
<li><a href=""https://github.com/github/codeql-action/commit/8a56dd2e53735063047ec88acbea334aecdd702e""><code>8a56dd2</code></a> Update to <code>@​actions/core</code> 1.11.1</li>
<li><a href=""https://github.com/github/codeql-action/commit/153267135194d736d42c011f5c4288fd7318a484""><code>1532671</code></a> Update default bundle to 2.19.1 (<a href=""https://redirect.github.com/github/codeql-action/issues/2519"">#2519</a>)</li>
<li><a href=""https://github.com/github/codeql-action/commit/64871a860c2923a5ec7cf6cefc983b535e8fe0e7""><code>64871a8</code></a> Merge branch 'main' into update-bundle/codeql-bundle-v2.19.1</li>
<li>Additional commits viewable in <a href=""https://github.com/github/codeql-action/compare/4dd16135b69a43b6c8efb853346f8437d92d3c93...c36620d31ac7c881962c3d9dd939c40ec9434f2b"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=github/codeql-action&package-manager=github_actions&previous-version=3.26.6&new-version=3.26.12)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>","[Label(name=""dependency update""), Label(name=""github_actions"")]",[],0
41,Fix NGPVAN _people_search arbitrary fields feature,https://api.github.com/repos/move-coop/parsons/issues/1150,Austin Weisgrau,1150,closed,2024-10-10 16:05:57+00:00,2024-10-15 20:32:18+00:00,2024-10-15 20:32:18+00:00,"This is a followup and fix to #916, `kwargs` were being passed to the wrong variable",[],[],0
42,SFTP client for CatalistMatch has 10 minute timeout,https://api.github.com/repos/move-coop/parsons/issues/1149,Austin Weisgrau,1149,closed,2024-10-10 15:55:06+00:00,2024-10-16 01:21:03+00:00,2024-10-16 01:21:03+00:00,This is important because sometimes the Catalist sftp server hangs indefinitely.,"[Label(name=""breaking change"")]",[],2
43,Allow passing google auth Credentials directly to GCS connector,https://api.github.com/repos/move-coop/parsons/issues/1148,Austin Weisgrau,1148,closed,2024-10-10 15:52:53+00:00,2024-10-11 17:13:48+00:00,2024-10-11 17:13:43+00:00,"Among other things, this enables authenticating the GCS connector using gcloud oauth from the parent shell, rather than requiring the use of service account credentials.",[],[],0
44,Allow setting gcs_tmp_bucket on GoogleBigQuery class,https://api.github.com/repos/move-coop/parsons/issues/1147,Austin Weisgrau,1147,closed,2024-10-10 15:44:44+00:00,2024-10-11 17:12:53+00:00,2024-10-11 17:12:49+00:00,This parallels more closely the set up of the Redshift class. It also allows for a more simple implementation when using multiple different instances of GoogleBigQuery in the same environment.,[],[],0
45,email parse depends on python patch version,https://api.github.com/repos/move-coop/parsons/issues/1146,Austin Weisgrau,1146,closed,2024-10-09 21:39:10+00:00,2024-10-10 17:08:13+00:00,2024-10-10 15:10:04+00:00,"The behavior of email.parseaddr depends on the python patch version.

See https://github.com/python/cpython/issues/102988 or associated changelogs, e.g. https://docs.python.org/3.8/whatsnew/changelog.html#python-3-8-20-final","[Label(name=""breaking change"")]",[],5
46,pin python 3.12.3 for tests,https://api.github.com/repos/move-coop/parsons/issues/1145,Austin Weisgrau,1145,closed,2024-10-09 19:19:09+00:00,2024-10-09 20:18:35+00:00,2024-10-09 20:18:34+00:00,"there is a breaking change to email.parseaddr standard library package on all recent patches of all python versions

see https://docs.python.org/3.8/library/email.utils.html#email.utils.parseaddr

Alternatively we should update the test to work with the most recent patches of python versions",[],[],0
47,Addresses Issue #1142 ,https://api.github.com/repos/move-coop/parsons/issues/1144,Matthew Krausse,1144,open,2024-10-04 22:04:48+00:00,2024-10-15 20:12:52+00:00,,This removes the error flag in Issue #1142. apparently VAN has removed the need to have a phone passed when a phone type contact type is used. Not sure why they did that but this makes this method in line with the api/ ,[],[],0
48,BigQuery copy method can convert dict column to JSON string,https://api.github.com/repos/move-coop/parsons/issues/1143,Austin Weisgrau,1143,closed,2024-10-02 19:48:01+00:00,2024-10-10 15:32:59+00:00,2024-10-10 15:32:55+00:00,"By default without this change, a dict column will cause the copy method to fail.

This is a more robust followup to #1028 and #1068. At the time we decided to remove support for loading dict columns to BigQuery because there wasn't a super simple way to accomplish this. Instead we encouraged users to convert the dict columns to strings themselves if they desired to do so. This change makes that workaround happen by default, but it can be turned off with the `convert_dict_columns_to_json` flag to `BigQuery().copy()`",[],[],0
49,[Bug] van.apply_response raises unnecessary error ,https://api.github.com/repos/move-coop/parsons/issues/1142,Lydia-Rose Kesich,1142,open,2024-09-27 13:02:39+00:00,2024-10-07 14:55:29+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->


## Detailed Description
The apply_response method from the VAN() class raises an error if the contact type is Phone, SMS etc and the phone number is missing. VAN made a change and no longer has phone number as a required field for bulk uploads with these contact types.  Would you please remove this exception if it no longer applies? 

From the source:

```
        if (
            contact_type_id == 1  # Phone
            or contact_type_id == 19  # Auto Dial
            or contact_type_id == 37  # SMS Text
            or contact_type_id == 67  # Phone Bank
            or contact_type_id == 68  # Consumer Phone
            or contact_type_id == 72  # Leader Phone
            or contact_type_id == 112  # Personal Phone
            or contact_type_id == 132  # Relational Text
            or contact_type_id == 143  # Distributed Text
            or contact_type_id == 147  # Bulk Text
            or contact_type_id == 149  # Paid SMS
        ):
            if not phone:
                raise Exception(""A phone number must be provided if canvassed via phone or SMS"")
```


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know): 3.1.0


## Priority
High","[Label(name=""bug"")]",[],2
50,updated README with correct instructions for installing from Github,https://api.github.com/repos/move-coop/parsons/issues/1141,lisa,1141,closed,2024-09-19 17:24:59+00:00,2024-09-19 18:21:05+00:00,2024-09-19 18:21:00+00:00,,[],[],1
51,Remove entity from campaign method,https://api.github.com/repos/move-coop/parsons/issues/1140,,1140,closed,2024-09-18 15:05:11+00:00,2024-10-12 21:27:54+00:00,2024-10-12 21:27:54+00:00,"Action Builder's API allows using the DELETE verb to remove entity records from campaigns. The test isn't a very meaningful one, but I guess something is better than nothing!","[Label(name=""connector update"")]","[NamedUser(login=""ydamit"")]",3
52,Update readme with correct instructions for getting Parsons from github,https://api.github.com/repos/move-coop/parsons/issues/1139,Shauna Gordon-McKeon,1139,closed,2024-09-17 20:40:14+00:00,2024-09-19 18:21:20+00:00,2024-09-19 18:21:20+00:00,"The instructions to use `python setup.py develop` are no longer the preferred way of doing things. Let's replace that instruction with the command given [here](https://www.parsonsproject.org/pub/installation#download-and-install-parsons).

Completely separately we might want to add some docs around editable installs.","[Label(name=""bug""), Label(name=""documentation""), Label(name=""good first issue""), Label(name=""medium priority"")]",[],1
53,Remove md5 security workaround after dropping 3.8,https://api.github.com/repos/move-coop/parsons/issues/1138,Wil T,1138,open,2024-09-17 20:39:44+00:00,2024-09-20 11:33:26+00:00,,Resolving one of bandit's flagged security issues required [adding an if/else statement](https://github.com/move-coop/parsons/commit/01ef28b856ce9126a16e06b486764d70d6ab1ff3) that can be reverted once we drop support for python 3.8.,"[Label(name=""bug"")]",[],2
54,Postgres copy explicates columns and column order,https://api.github.com/repos/move-coop/parsons/issues/1137,Austin Weisgrau,1137,closed,2024-09-16 22:33:37+00:00,2024-10-16 01:01:25+00:00,2024-10-15 20:30:50+00:00,"Without this change, the Postgres copy method only works if the Table has the same columns in the same order as the destination table. With this change, the source table can have a subset of columns in a different order but still successfully load to the destination table.",[],[],1
55,[Bug] db_sync table sync incremental/full will fail when trying to copy over an empty table,https://api.github.com/repos/move-coop/parsons/issues/1136,Austin Weisgrau,1136,open,2024-09-15 22:09:53+00:00,2024-09-15 22:09:54+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
The db_sync module does not handle well the scenario where the source table is empty, and in most cases will result in a confusing error being raised (destination table does not exist). This is due to trying to run a row count at the end of the sync. Even if running an incremental sync with verify_row_count explicitly set to false, if the source table is empty, the sync reverts to a full sync and does not pass along the verify_row_count flag. 

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.","[Label(name=""bug"")]",[],0
56,Update Dockerfile  with uv,https://api.github.com/repos/move-coop/parsons/issues/1135,Wil T,1135,closed,2024-09-11 00:41:03+00:00,2024-09-17 15:56:33+00:00,2024-09-17 15:56:33+00:00,"The dockerhub workflow is timing out due to a 4 hour long dependency resolution issue when using pip.
This uses uv instead, which was a solution when this happened in a recent PR.",[],[],0
57,Add skipMatching to apply_response(),https://api.github.com/repos/move-coop/parsons/issues/1134,,1134,closed,2024-09-10 21:49:08+00:00,2024-09-10 22:30:39+00:00,2024-09-10 22:30:34+00:00,"Responding to this issue - https://github.com/move-coop/parsons/issues/1133

Adding skipMatching() to the canvass context for apply response method!",[],"[NamedUser(login=""retacg"")]",1
58,[Feature/Addition] Include SkipMatching in VAN apply_response() function,https://api.github.com/repos/move-coop/parsons/issues/1133,,1133,closed,2024-09-09 15:40:21+00:00,2024-09-10 22:30:35+00:00,2024-09-10 22:30:35+00:00,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
Through a LONG back and forth with VAN we realized that contact attempts synced to the same record (but with different date canvassed's) were not being recorded on the record. VAN let us know that there is a parameter we could update within our API call to account for this, but its not included in the parsons apply_response() method. I would like it to be added so I can continue to leverage my existing code!

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->
Add the skipMatching() parameter to the canvass context of the [apply_response() ](https://docs.everyaction.com/reference/people-personidtype-personid-canvassresponses) function

## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
This allows us to sync historical contact attempt history properly! Currently, if two attempts with the same contact method and result (but different date canvassed's) are synced in the same run of the API (within 60 mins) then only one of the attempts is recorded in VAN...

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->
Should be really easy! Just adding skipMatching to the canvass context!! here is the VAN documentation - https://docs.everyaction.com/reference/people-personidtype-personid-canvassresponses

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
High! Before the end of the week if absolutely possible, this is for election work!","[Label(name=""enhancement"")]",[],0
59,Github Actions Rebuild,https://api.github.com/repos/move-coop/parsons/issues/1132,Wil T,1132,closed,2024-09-07 15:01:33+00:00,2024-10-11 00:15:55+00:00,2024-10-10 19:50:08+00:00,"This is a total rebuild of how our action yml files are structured, with the python tests now occuring in a single multi-job action. This works [much better](https://github.com/move-coop/parsons/actions/runs/10753195270) in the GitHub web UI. This also allows the easy stuff (linting, formatting, etc) to finish much faster than the pytest runs. It also adds additional security checks and automatic dependency updates.

## What it does

- Run pytest on every version of macOS (since only pytest and pip install run on macOS it should use too much time -- linting, formatting, coverage, etc are only run once).
- Remove file name filters on action triggers so actions run on ever pushed commit / active pull request rather than trying to limit runs to when certain files are updated (to ensure nothing is missed and make it more obvious if actions fail to run for some reason)
- Move test.yml and pip-install.yml to shared python-checks.yml
- Add automatic weekly dependency pull requests with [dependabot](https://docs.github.com/en/code-security/getting-started/dependabot-quickstart-guide) covering github actions, docker, and pip
- Add code coverage checking with [coverage](https://coverage.readthedocs.io) failing when the project has <75% coverage (for now) - important for #1114 
- Add code security checks with [bandit](https://bandit.readthedocs.io) (filtered to medium or higher risk and medium or higher confidence)
- Update security scorecard to pin hash of github/codeql-action/upload-sarif
- Update security scorecard to first harden the runner
- Update security scorecard to scan major-release branch along with main
- Update ruff, pytest, and similar

## What's needed to merge

- [x] Resolve the [4 issues](https://github.com/move-coop/parsons/actions/runs/10752486576/job/29820855435) found by bandit
  - [x] defusedxml
  - [x] md5",[],[],21
60,Add email and contact notes to NGPVAN docs,https://api.github.com/repos/move-coop/parsons/issues/1131,Paul Anzel,1131,closed,2024-09-07 14:37:54+00:00,2024-10-15 20:49:13+00:00,2024-10-15 20:49:13+00:00,Fixes #1129 . Realized both Email and Contact notes were missing from the RST file.,[],[],1
61,Update requests version,https://api.github.com/repos/move-coop/parsons/issues/1130,Paul Anzel,1130,closed,2024-09-07 14:17:26+00:00,2024-10-10 16:09:30+00:00,2024-10-10 16:09:30+00:00,Fixes #1127 . Bump up to most recent patched version.,[],[],1
62,[Bug] Add email API to NGPVAN docs,https://api.github.com/repos/move-coop/parsons/issues/1129,Paul Anzel,1129,closed,2024-09-07 14:09:34+00:00,2024-10-15 20:49:14+00:00,2024-10-15 20:49:14+00:00,The NGPVAN email module is undocumented in the overall docs. Add it to the main documentation.,"[Label(name=""bug"")]",[],0
63,fix gmail test failing with invalid email address,https://api.github.com/repos/move-coop/parsons/issues/1128,Wil T,1128,closed,2024-09-07 13:02:38+00:00,2024-09-07 14:46:40+00:00,2024-09-07 13:03:20+00:00,"not sure why this didn't show up yet via github actions but I just downloaded the main branch and ran pytest locally and was getting a failed test here because ,com is not valid but the test expects it to validate...",[],[],1
64,[Bug] Using yanked/vulnerable version of requests,https://api.github.com/repos/move-coop/parsons/issues/1127,Wil T,1127,closed,2024-09-07 12:53:23+00:00,2024-10-10 16:09:31+00:00,2024-10-10 16:09:31+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
Installing with uv results in

> [!WARNING]  
> `
> `requests==2.32.0` is yanked (reason: ""Yanked due to conflicts with CVE-2024-35195 mitigation"")
> `
","[Label(name=""bug"")]",[],0
65,Code coverage reporting,https://api.github.com/repos/move-coop/parsons/issues/1126,Wil T,1126,closed,2024-09-06 16:40:20+00:00,2024-09-07 14:46:24+00:00,2024-09-07 14:46:23+00:00,"Prompted by https://github.com/move-coop/parsons/issues/1114 I created coverage testing.

The main thing about this that I don't like is that it reports on each version which is probably pointless? Does anyone know if that is worth doing or not?",[],[],4
66,[Bug] Upgrade mysql-connector-python,https://api.github.com/repos/move-coop/parsons/issues/1125,Paul Anzel,1125,open,2024-09-06 16:15:39+00:00,2024-09-06 16:15:39+00:00,,"Current mysql-connector-python (8.0.18) package has a security concern. I tried upgrading to 8.3.0 in a PR (#1121 ) but that could introduce a potential breaking change, so I'll make it be a separate discussion/PR.

Looking at the versions (https://dev.mysql.com/doc/connector-python/en/connector-python-versions.html) by going from 8.0.* to 8.3 we do lose some (already beyond end-of-life) connection capabilities to MySQL 5.5 and 5.6. We do, however, gain official compatibility with Python 3.12.

None of the current tests failed with the version upgrade. But worth a discussion.","[Label(name=""bug"")]",[],0
67,test_smartmatch fails to run on windows,https://api.github.com/repos/move-coop/parsons/issues/1124,Wil T,1124,open,2024-09-06 16:05:17+00:00,2024-09-06 16:18:28+00:00,,"When tests are run on Windows, **test_smartmatch** is skipped (a decision we made so we could add windows tests sooner).

The commit that skips them (revert once the test passes): https://github.com/move-coop/parsons/pull/1119/commits/b00f60807c65de2e95e112a83f3f2322f486c96b

The [error](https://github.com/move-coop/parsons/actions/runs/10727584933/job/29750246811?pr=1119#step:7:116) when you run the test:
```
================================== FAILURES ===================================
_______________________________ test_smartmatch _______________________________

intable = +------------+-----------+------------+-------------------+--------------+--------------------+
| first_name | last_na...| ''                 |
+------------+-----------+------------+-------------------+--------------+--------------------+

submit_filename = 'parsons_test.csv'
raw_outgz = b""\x1f\x8b\x08\x00\x00\x13\xdaf\x02\xffmMK\xaa\x021\x10\xdc\x0b\xde\xe1\x1d\xa0\x14F=\x80\n\xc2\xdb\x88\x0bW\xaeB'\xd3...\xb0\xfb\x9f<\xd8x4C\xc5*x\xc7\xd8\xbc\x0c\x9f\x14\xc0R\xfa\x12\xcfQ\xb1\xa7\x10n\xbf\xc4_\xca;M\x0b=&\x1c\x01\x00\x00""
raw_outcsv = 'matchback_id,ts__row,ts__input_row,first_name,last_name,address1,email,phone,some_unknown_field,tsmart_match_code,vb....1231231234,foo,Y,OH-123\r\n2,2,2,Alice,Example,123 Main,,,bar,Y,OH-123\r\n3,3,3,Sally,Example,123 Main,,,,Y,OH-123\r\n'
raw_outtable = +--------------+---------+---------------+------------+-----------+------------+-------------------+--------------+---...----------+------------+-------------------+--------------+--------------------+-------------------+-----------------+

final_outtable = +------------+-----------+------------+-------------------+--------------+--------------------+---------+-------------...----------+------------+-------------------+--------------+--------------------+-------------------+-----------------+

requests_mock = <requests_mock.mocker.Mocker object at 0x0000022F8C4CF4C0>

    def test_smartmatch(
        intable,
        submit_filename,
        raw_outgz,
        raw_outcsv,
        raw_outtable,
        final_outtable,
        requests_mock,
    ):
        ts = TargetSmartAPI(""mockkey"")
        resp1 = {""url"": ""https://mock_smartmatch_upload_endpoint"", ""error"": None}
        poll_resp = {""url"": ""https://mock_smartmatch_download_endpoint"", ""error"": None}
        requests_mock.get(""https://api.targetsmart.com/service/smartmatch"", json=resp1)
        requests_mock.put(resp1[""url""])
        requests_mock.get(""https://api.targetsmart.com/service/smartmatch/poll"", json=poll_resp)
        requests_mock.get(poll_resp[""url""], content=raw_outgz)
    
>       results = ts.smartmatch(intable).to_petl()

test\test_targetsmart\test_targetsmart_smartmatch.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
parsons\targetsmart\targetsmart_smartmatch.py:255: in smartmatch
    dataprep_table.tocsv(tmp.name, encoding=""utf8"")
C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\petl\io\csv.py:108: in tocsv
    tocsv_impl(table, source=source, encoding=encoding, errors=errors,
C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\petl\io\csv_py3.py:44: in tocsv_impl
    _writecsv(table, source=source, mode='wb', **kwargs)
C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\petl\io\csv_py3.py:53: in _writecsv
    with source.open(mode) as buf:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <petl.io.sources.FileSource object at 0x0000022F9060A440>, mode = 'wb'

    def open(self, mode='r'):
>       return io.open(self.filename, mode, **self.kwargs)
E       PermissionError: [Errno 13] Permission denied: 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmprnpqah32\\smartmatch_inputv4dzcdf7.csv'

C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\petl\io\sources.py:33: PermissionError
============================== warnings summary ===============================
C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\paramiko\pkey.py:100
  C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\paramiko\pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
    ""cipher"": algorithms.TripleDES,

C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\paramiko\transport.py:259
  C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\paramiko\transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
    ""class"": algorithms.TripleDES,

C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\joblib\my_exceptions.py:21
  C:\hostedtoolcache\windows\Python\3.10.11\x64\lib\site-packages\joblib\my_exceptions.py:21: DeprecationWarning: TransportableException is deprecated and will be removed from joblib in 0.16
    warn(""{} is deprecated and will be removed from joblib ""

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED test/test_targetsmart/test_targetsmart_smartmatch.py::test_smartmatch - PermissionError: [Errno 13] Permission denied: 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmprnpqah32\\smartmatch_inputv4dzcdf7.csv'
===== 1 failed, 842 passed, 191 skipped, 1 xfailed, 3 warnings in 29.63s ======
```","[Label(name=""bug""), Label(name=""high priority""), Label(name=""testing"")]",[],0
68,[Feature/Addition] Add option for USER_ENTERED flag in Google Sheets,https://api.github.com/repos/move-coop/parsons/issues/1123,Paul Anzel,1123,open,2024-09-06 15:36:11+00:00,2024-09-06 15:36:11+00:00,,"When I was trying to write some date/datetime data to Google Sheets, the data gets entered as a string (e.g. `'2024-01-01` instead of `2024-01-01`), which ran into a bit of trouble with some data not registering as a date for the spreadsheet.

## Possible Implementation

Gspread has a `USER_ENTERED` flag (https://stackoverflow.com/a/62359273) that we can use. Just add an optional argument to set this flag in the writing method (overwrite sheet, append to sheet, paste to sheet).

## Priority

I can work on this PR. I am able to work around this with Google sheets format coercion  (setting the format as date/datetime) but I'd like to eliminate this step.","[Label(name=""enhancement"")]",[],0
69,circleci updates,https://api.github.com/repos/move-coop/parsons/issues/1122,Wil T,1122,closed,2024-09-06 02:06:10+00:00,2024-09-06 16:44:43+00:00,2024-09-06 16:25:22+00:00,this updates circleci requirements and python version. it should make doc creation dependency installation faster so it doesn't time out as easily.,[],[],2
70,Version bump for certain packages,https://api.github.com/repos/move-coop/parsons/issues/1121,Paul Anzel,1121,closed,2024-09-05 20:59:52+00:00,2024-09-06 16:38:57+00:00,2024-09-06 16:38:56+00:00,"Due to some packages found in security review, incrementing these packages in `requirements.txt`.","[Label(name=""breaking change"")]",[],10
71,Step security changes,https://api.github.com/repos/move-coop/parsons/issues/1120,Paul Anzel,1120,closed,2024-09-05 20:34:46+00:00,2024-09-06 15:47:14+00:00,2024-09-06 15:47:14+00:00,"As per security review, this uses the Step Security tool (https://app.stepsecurity.io/secureworkflow) to harden the CI/CD workflows.",[],[],0
72,Implement Windows testing,https://api.github.com/repos/move-coop/parsons/issues/1119,Wil T,1119,closed,2024-09-05 20:16:05+00:00,2024-09-06 16:45:03+00:00,2024-09-06 16:18:35+00:00,"This is basically ready to merge, but there is one failing test I have not figured out. It is unable to read the test csv file with a permissions error:
https://github.com/move-coop/parsons/actions/runs/10727584933/job/29750246811?pr=1119

- Add Windows tests to github testing action
- Add install via pip on Windows to install with pip action
- Fix installation on Windows with python 3.12 when rust compilation resources are not available
- Dramatically improve installation time on Windows when running python 3.12",[],[],1
73,Combine requirements-dev.txt and docs/requirements.txt,https://api.github.com/repos/move-coop/parsons/issues/1118,Shauna Gordon-McKeon,1118,open,2024-09-03 19:27:39+00:00,2024-09-06 16:24:37+00:00,,"Currently we have three requirements files, the standard one in requirements.txt, a requirements-dev.txt in the top level, and another requirements.txt in the docs folder. The latter has requirements for building docs, and requirements-dev.txt has requirements for testing and linting. There's no reason not to combine the latter two (probably makes sense to remove the requirements.txt in the docs folder).

When doing this, make sure to check our CI workflows - presumably this changes how the docs are automatically built, for instance. We also want to check any of our documentation, like the Contributing Guide, and update instructions there as well.","[Label(name=""bug""), Label(name=""good first issue"")]","[NamedUser(login=""rdhyee"")]",2
74,Use ruff's recommended github output format,https://api.github.com/repos/move-coop/parsons/issues/1117,Wil T,1117,closed,2024-09-02 15:58:35+00:00,2024-09-06 16:51:43+00:00,2024-09-06 16:49:39+00:00,"This adds the `--output-format=github` arg to the ruff check command so that github can annotate PRs and create links in the failure messages to the offending lines of code.

https://docs.astral.sh/ruff/integrations/#github-actions",[],[],0
75,Create security_scorecard.yml,https://api.github.com/repos/move-coop/parsons/issues/1116,Shauna Gordon-McKeon,1116,closed,2024-08-13 19:49:57+00:00,2024-08-13 20:59:12+00:00,2024-08-13 20:59:12+00:00,Addresses #1113 ,[],[],1
76,Gcs upload table csview fix,https://api.github.com/repos/move-coop/parsons/issues/1115,,1115,closed,2024-08-09 15:54:11+00:00,2024-08-10 00:27:16+00:00,2024-08-10 00:27:15+00:00,"A quick fix because it seems like the attributes for a CSVView isn't consistent across ALL views, (i.e. I'v  been getting errors like `AttributeError: 'URLSource' object has no attribute 'filename'`, which is indeed true when I looked at the object. Treating it as a normal csv seemed to have done the trick!",[],[],2
77,Test Coverage Assessment,https://api.github.com/repos/move-coop/parsons/issues/1114,Josh Wenk,1114,open,2024-08-08 19:59:13+00:00,2024-09-09 00:19:43+00:00,,"HIGH LEVEL: we probably want to answer (at least):
- Are our current tests doing what we think they are doing?
- Do we have test gaps?","[Label(name=""enhancement""), Label(name=""testing"")]",[],1
78,Implement Scorecard.dev as GitHub ACtion,https://api.github.com/repos/move-coop/parsons/issues/1113,Josh Wenk,1113,closed,2024-08-08 19:00:36+00:00,2024-08-14 01:18:11+00:00,2024-08-14 01:18:11+00:00,"see: https://scorecard.dev/#using-the-github-action
@anzelpwj @shaunagm ","[Label(name=""enhancement"")]","[NamedUser(login=""shaunagm"")]",1
79,Kasiah/community dot com connector,https://api.github.com/repos/move-coop/parsons/issues/1112,Kasia Hinkson,1112,closed,2024-08-05 18:00:35+00:00,2024-08-12 16:05:17+00:00,2024-08-12 16:05:14+00:00,Creates a new connector for the Community.com API,[],[],1
80,Added method to GoogleBigQuery to copy between projects,https://api.github.com/repos/move-coop/parsons/issues/1111,Charlie Kramer,1111,open,2024-08-03 19:20:45+00:00,2024-08-30 15:26:33+00:00,,,[],[],0
81,Enable GZIP on BigQuery extract to GCS,https://api.github.com/repos/move-coop/parsons/issues/1110,Austin Weisgrau,1110,closed,2024-07-31 16:35:39+00:00,2024-08-27 18:49:59+00:00,2024-08-27 18:49:59+00:00,,[],[],2
82,Allow to_gcs_csv function to accept a gcs instance instead of creating one,https://api.github.com/repos/move-coop/parsons/issues/1109,Kasia Hinkson,1109,closed,2024-07-29 18:26:14+00:00,2024-07-29 18:45:07+00:00,2024-07-29 18:45:03+00:00,"At TMC, we instantiate BigQuery and GCS outside of main, for dependency injection, but this function always creates a new GCS instance inside of it. This is breaking some of our code. This PR allows the user to send in a GCS instance, but should be backwards compatible and not breaking anyone else's code. ",[],[],1
83,"#1091 ruff added to pre-commit, pyproject.toml, requirements-dev.txt, ruff linting differences ",https://api.github.com/repos/move-coop/parsons/issues/1108,Sophia Alice,1108,closed,2024-07-23 20:05:24+00:00,2024-08-12 14:39:54+00:00,2024-08-12 14:39:54+00:00,,[],[],0
84,new release,https://api.github.com/repos/move-coop/parsons/issues/1107,,1107,closed,2024-07-22 21:26:09+00:00,2024-07-22 21:28:51+00:00,2024-07-22 21:28:47+00:00,update setup.py for new release,[],[],0
85,Restore minimal CONTRIBUTING.md that treats website as source of truth,https://api.github.com/repos/move-coop/parsons/issues/1106,Soren Spicknall,1106,closed,2024-07-22 01:59:18+00:00,2024-08-01 21:26:22+00:00,2024-07-23 19:56:51+00:00,"Resolves #1046, which was intended to be dealt with in previous rounds of changes. A minimal CONTRIBUTING.md was introduced in #851, but accidentally replaced by an older version of the file in #938. This restores the minimal version of the file that points to the Parsons website's contributing guide as the sole complete version of that document.",[],[],0
86,Add configurable timeout to BigQuery query jobs,https://api.github.com/repos/move-coop/parsons/issues/1105,Soren Spicknall,1105,open,2024-07-19 19:38:48+00:00,2024-11-04 18:40:06+00:00,,"This picks up where somebody else's PR, #981, left off earlier this year. That PR closed because the original contributor deleted their fork after waiting unsuccessfully for merge.

This takes the spirit of those earlier proposed changes and adapts it to the current state of the BigQuery connector, hoping to resolve #910 once merged. I'm definitely open to a different approach if folks' thoughts on how to implement this default timeout have evolved since that last PR was active - @Jason94, I know you were part of that conversation at the time.",[],[],2
87,Restore functionality of Google Sheets API tests,https://api.github.com/repos/move-coop/parsons/issues/1104,Soren Spicknall,1104,closed,2024-07-19 18:29:19+00:00,2024-08-01 21:25:20+00:00,2024-08-01 19:00:24+00:00,"Resolves #1060 - I agree with [anzelpwj](https://github.com/anzelpwj) that it would be better to use backoff to avoid rate limiting in the long run (likely also true of other Parsons test scripts), but for the time being I've just instituted some judicious pauses along the test path.

Also removes the redundant `test_read_sheet`, since the underlying method is deprecated and simply calls `get_worksheet`.",[],[],1
88,Update old reference to Parsons API key type,https://api.github.com/repos/move-coop/parsons/issues/1103,Soren Spicknall,1103,closed,2024-07-19 17:09:25+00:00,2024-08-01 21:26:25+00:00,2024-07-26 18:57:37+00:00,"Resolves #1084 by updating the language previously used to refer to the now-nonexistent ""Parsons"" API key type provisioned by NGPVAN, pointing users to request a custom integration instead.",[],[],1
89,ActionNetwork SQL Mirror querying support + general SSH util to query any db through ssh,https://api.github.com/repos/move-coop/parsons/issues/1102,,1102,closed,2024-07-16 19:44:02+00:00,2024-07-18 18:13:17+00:00,2024-07-18 18:13:17+00:00,"- Added SSH util function we can connect through SSH and query through, added a test for that.
- Added SQL Mirror support for ActionNetwork which lets us query our data/connect to our data we have in ActionNetwork (all tables read only access). 
@austinweisgrau, @shaunagm  I hope this won't break anything like my other PR, I recreated the utility and made it from a class to just a function we can use to query a DB through a SSH tunnel connection.
Please let me know if there is any feedback or any change request :)
Also, thank you again @austinweisgrau and @shaunagm for everything!",[],[],1
90,adding unique id lists support in ActionNetwork module,https://api.github.com/repos/move-coop/parsons/issues/1101,,1101,closed,2024-07-16 17:59:31+00:00,2024-07-16 19:03:13+00:00,2024-07-16 19:03:13+00:00,"- I've added only 2 functions: get_unique_id_lists, get_unique_id_list to fully support ActionNetwork's API.
@shaunagm @austinweisgrau I've separated the old problematic PR into 2 PR's so this is the first non problematic one and I would soon open the PR with the SSH utility PR.
Hope that is ok thank you and sorry for the hassle.",[],[],4
91,"Revert ""Features - ActionNetwork API - Unique ID Lists routes support…",https://api.github.com/repos/move-coop/parsons/issues/1100,Austin Weisgrau,1100,closed,2024-07-16 17:28:15+00:00,2024-07-16 17:32:16+00:00,2024-07-16 17:32:06+00:00,"…  + SQL Mirror support (#1025)""

This reverts commit 33b7185a3f38f1297f7867077acfddb57d6098d2.",[],[],0
92,Remove sshtunnel from requirements-dev.txt,https://api.github.com/repos/move-coop/parsons/issues/1099,Austin Weisgrau,1099,closed,2024-07-16 17:22:22+00:00,2024-07-16 17:29:40+00:00,2024-07-16 17:29:16+00:00,"This belongs in requirements.txt and also in setup.py, but not here.",[],[],1
93,Move redshift and sshtunnel imports into ActionNetwork method,https://api.github.com/repos/move-coop/parsons/issues/1098,Austin Weisgrau,1098,closed,2024-07-16 17:04:30+00:00,2024-07-16 17:29:59+00:00,2024-07-16 17:29:59+00:00,Avoid making redshift and sshtunnel dependencies for all users of the ActionNetwork connector,[],[],2
94,Specify new sshtunnel dependency for ActionNetwork connector,https://api.github.com/repos/move-coop/parsons/issues/1097,Austin Weisgrau,1097,closed,2024-07-16 16:55:40+00:00,2024-07-16 17:04:41+00:00,2024-07-16 17:04:40+00:00,,[],[],1
95,Change Import to Absolute,https://api.github.com/repos/move-coop/parsons/issues/1096,Ian,1096,closed,2024-07-16 16:11:08+00:00,2024-07-16 16:20:17+00:00,2024-07-16 16:20:14+00:00,We should be worried that unit testing didn't catch this,[],"[NamedUser(login=""IanRFerguson"")]",0
96,Refactor dbt utility for modularity,https://api.github.com/repos/move-coop/parsons/issues/1095,Austin Weisgrau,1095,open,2024-07-15 21:33:37+00:00,2024-11-04 16:49:39+00:00,,"These changes uncouple the code for the dbt utility that runs a dbt command from the code that logs dbt commands. This allows for modular use of different loggers and a very flexible ability to customize logging. This addresses the main concerns from the original PR (#841).

# Breaking changes
This PR does cause breaking changes with the previous implementation of the dbt utility, but those changes are well worth it for a much cleaner and clearer interface.

Using the new implementation looks like:
```{python}
import os
from pathlib import Path
from parsons.utilities.dbt import run_dbt_commands, dbtLoggerSlack, dbtLoggerPython

results = run_dbt_commands(
    commands=[""dbt run"", ""dbt test""],
    dbt_project_directory=Path(""/path/to/dbt/project""),
    loggers=[dbtLoggerPython, dbtLoggerSlack(webhook=os.environ[""SLACK_WEBHOOK""])],
)
```

The prior implementation was potentially only compatible with Redshift, depending on how the user's dbt `profiles.yaml` was set up. The new implementation allows the dbt process to inherit the shell environment from the python parent process, and is therefore compatible with running dbt on any database, and the user can configure credential passing using environment variables (recommended) or any other method.

# Improved documentation
There is now much more thorough documentation throughout the 4 modules containing the dbt utility code.

TO DO:
- [x] Touch up the dbtLoggerMarkdown text formatting a bit
- [x] dbtLoggerDatabase splits artifacts into a run table and a node table",[],[],1
97,Bump setuptools from 68.0.0 to 70.0.0,https://api.github.com/repos/move-coop/parsons/issues/1094,,1094,closed,2024-07-15 18:36:28+00:00,2024-07-15 18:57:31+00:00,2024-07-15 18:57:22+00:00,"Bumps [setuptools](https://github.com/pypa/setuptools) from 68.0.0 to 70.0.0.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/pypa/setuptools/blob/main/NEWS.rst"">setuptools's changelog</a>.</em></p>
<blockquote>
<h1>v70.0.0</h1>
<h2>Features</h2>
<ul>
<li>Emit a warning when <code>[tools.setuptools]</code> is present in <code>pyproject.toml</code> and will be ignored. -- by :user:<code>SnoopJ</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4150"">#4150</a>)</li>
<li>Improved <code>AttributeError</code> error message if <code>pkg_resources.EntryPoint.require</code> is called without extras or distribution
Gracefully &quot;do nothing&quot; when trying to activate a <code>pkg_resources.Distribution</code> with a <code>None</code> location, rather than raising a <code>TypeError</code>
-- by :user:<code>Avasam</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4262"">#4262</a>)</li>
<li>Typed the dynamically defined variables from <code>pkg_resources</code> -- by :user:<code>Avasam</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4267"">#4267</a>)</li>
<li>Modernized and refactored VCS handling in package_index. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4332"">#4332</a>)</li>
</ul>
<h2>Bugfixes</h2>
<ul>
<li>In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4136"">#4136</a>)</li>
<li>Fix finder template for lenient editable installs of implicit nested namespaces
constructed by using <code>package_dir</code> to reorganise directory structure. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4278"">#4278</a>)</li>
<li>Fix an error with <code>UnicodeDecodeError</code> handling in <code>pkg_resources</code> when trying to read files in UTF-8 with a fallback -- by :user:<code>Avasam</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4348"">#4348</a>)</li>
</ul>
<h2>Improved Documentation</h2>
<ul>
<li>Uses RST substitution to put badges in 1 line. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4312"">#4312</a>)</li>
</ul>
<h2>Deprecations and Removals</h2>
<ul>
<li>
<p>Further adoption of UTF-8 in <code>setuptools</code>.
This change regards mostly files produced and consumed during the build process
(e.g. metadata files, script wrappers, automatically updated config files, etc..)
Although precautions were taken to minimize disruptions, some edge cases might
be subject to backwards incompatibility.</p>
<p>Support for <code>&quot;locale&quot;</code> encoding is now <strong>deprecated</strong>. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4309"">#4309</a>)</p>
</li>
<li>
<p>Remove <code>setuptools.convert_path</code> after long deprecation period.
This function was never defined by <code>setuptools</code> itself, but rather a
side-effect of an import for internal usage. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4322"">#4322</a>)</p>
</li>
<li>
<p>Remove fallback for customisations of <code>distutils</code>' <code>build.sub_command</code> after long
deprecated period.
Users are advised to import <code>build</code> directly from <code>setuptools.command.build</code>. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4322"">#4322</a>)</p>
</li>
<li>
<p>Removed <code>typing_extensions</code> from vendored dependencies -- by :user:<code>Avasam</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4324"">#4324</a>)</p>
</li>
<li>
<p>Remove deprecated <code>setuptools.dep_util</code>.
The provided alternative is <code>setuptools.modified</code>. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4360"">#4360</a>)</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pypa/setuptools/commit/5cbf12a9b63fd37985a4525617b46576b8ac3a7b""><code>5cbf12a</code></a> Workaround for release error in v70</li>
<li><a href=""https://github.com/pypa/setuptools/commit/9c1bcc3417bd12668123f7e731e241d9e57bfc57""><code>9c1bcc3</code></a> Bump version: 69.5.1 → 70.0.0</li>
<li><a href=""https://github.com/pypa/setuptools/commit/4dc0c31644b458ac43ce6148f6a9dc729a7e78b5""><code>4dc0c31</code></a> Remove deprecated <code>setuptools.dep_util</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4360"">#4360</a>)</li>
<li><a href=""https://github.com/pypa/setuptools/commit/6c1ef5748dbd70c8c5423e12680345766ee101d9""><code>6c1ef57</code></a> Remove xfail now that test passes. Ref <a href=""https://redirect.github.com/pypa/setuptools/issues/4371"">#4371</a>.</li>
<li><a href=""https://github.com/pypa/setuptools/commit/d14fa0162c95450898c11534caf26a0f03553176""><code>d14fa01</code></a> Add all site-packages dirs when creating simulated environment for test_edita...</li>
<li><a href=""https://github.com/pypa/setuptools/commit/6b7f7a18afc90007544092c446dc0cd856d86b17""><code>6b7f7a1</code></a> Prevent <code>bin</code> folders to be taken as extern packages when vendoring (<a href=""https://redirect.github.com/pypa/setuptools/issues/4370"">#4370</a>)</li>
<li><a href=""https://github.com/pypa/setuptools/commit/69141f69f8bf38da34cbea552d6fdaa9c8619c53""><code>69141f6</code></a> Add doctest for vendorised bin folder</li>
<li><a href=""https://github.com/pypa/setuptools/commit/2a53cc1200ec4b14e08e84be3c042f8983dfb7d7""><code>2a53cc1</code></a> Prevent 'bin' folders to be taken as extern packages</li>
<li><a href=""https://github.com/pypa/setuptools/commit/720862807dea012f3a0e7061880691025f736f11""><code>7208628</code></a> Replace call to deprecated <code>validate_pyproject</code> command (<a href=""https://redirect.github.com/pypa/setuptools/issues/4363"">#4363</a>)</li>
<li><a href=""https://github.com/pypa/setuptools/commit/96d681aa405460f724c62c00ca125ae722ad810a""><code>96d681a</code></a> Remove call to deprecated validate_pyproject command</li>
<li>Additional commits viewable in <a href=""https://github.com/pypa/setuptools/compare/v68.0.0...v70.0.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=setuptools&package-manager=pip&previous-version=68.0.0&new-version=70.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
98,Fix typo in environment variable,https://api.github.com/repos/move-coop/parsons/issues/1093,Austin Weisgrau,1093,closed,2024-07-15 17:35:23+00:00,2024-07-15 18:12:27+00:00,2024-07-15 17:53:36+00:00,This typo entered in 6dfaaec26dfee18e6f8f00f83bc7f02396f22944,[],[],0
99,Enhance and fix Zoom methods,https://api.github.com/repos/move-coop/parsons/issues/1092,Austin Weisgrau,1092,open,2024-07-11 23:59:52+00:00,2024-11-04 16:35:02+00:00,,"This PR makes two changes to the Zoom connector.
- get_meetings() takes new optional arguments to date filter results
- the post processing on the `get_past_meeting_poll_metadata` method is fixed",[],[],2
100,Swap in Ruff for Linting and Formatting,https://api.github.com/repos/move-coop/parsons/issues/1091,Josh Wenk,1091,closed,2024-07-11 19:07:16+00:00,2024-08-13 19:00:36+00:00,2024-08-13 19:00:36+00:00,,"[Label(name=""enhancement"")]",[],1
101,Python3.8 Base Image,https://api.github.com/repos/move-coop/parsons/issues/1090,Ian,1090,closed,2024-07-10 14:00:11+00:00,2024-07-11 15:17:09+00:00,2024-07-10 14:53:17+00:00,"The Parsons image officially broke ... this PR swaps out the base image in the Dockerfile and officially deprecates 3.7 as a supported version (not sure the latter bit here is necessary).

![image](https://github.com/move-coop/parsons/assets/47256454/588f335f-3922-48c0-8c11-c2718812094f)
",[],"[NamedUser(login=""IanRFerguson"")]",2
102,Add campaignId parameter to apply_responses method in People class,https://api.github.com/repos/move-coop/parsons/issues/1089,Matthew Krausse,1089,closed,2024-07-09 21:15:51+00:00,2024-07-19 23:43:40+00:00,2024-07-09 23:01:29+00:00,"This pull request adds a new parameter, campaignId, to the apply_responses method in the People class. 

This handles issue #1075",[],[],3
103,Remove unnecessary dependency,https://api.github.com/repos/move-coop/parsons/issues/1088,Austin Weisgrau,1088,closed,2024-07-01 19:57:19+00:00,2024-07-11 18:05:00+00:00,2024-07-11 18:04:59+00:00,Hasn't been necessary since commit 617d06f5b6e036e83c60bac7f5b6bc35b0bbb6ab,[],[],0
104,Support for python3.12: Take 2,https://api.github.com/repos/move-coop/parsons/issues/1087,Josh Wenk,1087,closed,2024-06-26 21:17:13+00:00,2024-07-19 01:38:17+00:00,2024-06-27 19:30:34+00:00,take 2 of https://github.com/move-coop/parsons/pull/1070,[],[],0
105,Sunset support for Python 3.7,https://api.github.com/repos/move-coop/parsons/issues/1086,Josh Wenk,1086,closed,2024-06-26 20:12:22+00:00,2024-07-11 15:20:45+00:00,2024-07-11 15:20:44+00:00,"3.7 is Unsupported/End-of-Life since 2023-06-27.  See: 
- https://devguide.python.org/versions/
- https://peps.python.org/pep-0537/

Also see: https://github.com/move-coop/parsons/issues/932

Note: @KasiaHinkson can advise on TMC needs here.","[Label(name=""enhancement"")]",[],4
106,Update census_geocoder.py,https://api.github.com/repos/move-coop/parsons/issues/1085,Jeffrey Rodriguez,1085,closed,2024-06-25 20:47:15+00:00,2024-06-25 22:14:54+00:00,2024-06-25 22:14:50+00:00,"The geocode_address_batch function needs to add ""id"" to the columns it checks",[],[],0
107,NGPVAN authentication docs need to be updated,https://api.github.com/repos/move-coop/parsons/issues/1084,Shauna Gordon-McKeon,1084,closed,2024-06-25 19:08:41+00:00,2024-07-26 18:57:38+00:00,2024-07-26 18:57:38+00:00,"It was reported in the Slack that NGPVAN has removed the Parsons API key, and that people who want to use Parsons with Van will now have to request a custom integration.

This is a bummer, but it is what it is, so we should update [our docs](https://move-coop.github.io/parsons/html/stable/ngpvan.html).

Marking this as high priority since this is one of our most highly used connectors.","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""high priority"")]",[],0
108,Bug fix: pass custom delimiter from Table.from_csv to BigQuery load,https://api.github.com/repos/move-coop/parsons/issues/1083,Austin Weisgrau,1083,closed,2024-06-24 19:19:44+00:00,2024-08-27 17:48:58+00:00,2024-08-27 17:48:58+00:00,"A bug was introduced by recent PR #1062 

If our source table is loaded from CSV with no transformations, the original source file will be directly loaded to GCS.

We may need to pass along a custom delimiter to BigQuery.

e.g. for this workflow:
```
tbl = Table.from_csv(filepath, delimiter='\t')
BigQuery().copy(tbl, 'my_dataset.my_table')
```
The copy job will fail because the delimiter for the file loaded to GCS will be a `\t`, but BigQuery().copy() expects a comma by default.

This change fixes that to pass along any custom delimiter set on the Table file load to the BigQuery copy job config.",[],[],0
109,Airtable Updates,https://api.github.com/repos/move-coop/parsons/issues/1082,Cody Gordon,1082,closed,2024-06-24 12:35:40+00:00,2024-07-09 20:35:12+00:00,2024-07-09 20:35:12+00:00,"- [`airtable-python-wrapper` was renamed to `pyairtable` after v1.0](https://github.com/gtalarico/pyairtable/issues/125#issuecomment-891439661). Needed to update this eventually if we want to take advantage of newer features. Seems despite a couple method name changes that it's backwards compatible.
- Added new methods:
  - [update_records](https://pyairtable.readthedocs.io/en/stable/api.html#pyairtable.Table.batch_update)
  - [upsert_records](https://pyairtable.readthedocs.io/en/stable/api.html#pyairtable.Table.batch_upsert)
  - [delete_record](https://pyairtable.readthedocs.io/en/stable/api.html#pyairtable.Table.delete)
  - [delete_records](https://pyairtable.readthedocs.io/en/stable/api.html#pyairtable.Table.batch_delete) 
- Added `typecast` to `insert_record`",[],[],1
110,SFTP utility can include timeout,https://api.github.com/repos/move-coop/parsons/issues/1081,Austin Weisgrau,1081,closed,2024-06-21 00:11:41+00:00,2024-08-27 17:44:10+00:00,2024-08-27 17:44:10+00:00,"This change allows setting a timeout on the SFTP connection. If set, a TimeoutExcept will be raised if the SFTP server does not respond for the number of seconds set by the timeout.",[],[],0
111,Skip empty tables in GSheets append & overwrite methods,https://api.github.com/repos/move-coop/parsons/issues/1080,Cody Gordon,1080,closed,2024-06-19 14:01:33+00:00,2024-06-26 16:29:18+00:00,2024-06-26 16:29:18+00:00,"If a table with no data is provided the overwrite method an error that is hard to debug is raised:
`ValueError: min() arg is an empty sequence`. We ought to just log a warning.

The [`paste_data_in_sheet` method already had a check like this](https://github.com/MoveOnOrg/parsons/blob/b02a67a34b3f9235dd2dafa89251b46dcb544ddb/parsons/google/google_sheets.py#L330).",[],[],0
112,Bump urllib3 from 1.26.18 to 1.26.19,https://api.github.com/repos/move-coop/parsons/issues/1079,,1079,closed,2024-06-18 01:27:08+00:00,2024-06-25 21:23:07+00:00,2024-06-25 21:22:59+00:00,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.18 to 1.26.19.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>
<blockquote>
<h2>1.26.19</h2>
<h2>🚀 urllib3 is fundraising for HTTP/2 support</h2>
<p><a href=""https://sethmlarson.dev/urllib3-is-fundraising-for-http2-support"">urllib3 is raising ~$40,000 USD</a> to release HTTP/2 support and ensure long-term sustainable maintenance of the project after a sharp decline in financial support for 2023. If your company or organization uses Python and would benefit from HTTP/2 support in Requests, pip, cloud SDKs, and thousands of other projects <a href=""https://opencollective.com/urllib3"">please consider contributing financially</a> to ensure HTTP/2 support is developed sustainably and maintained for the long-haul.</p>
<p>Thank you for your support.</p>
<h2>Changes</h2>
<ul>
<li>Added the <code>Proxy-Authorization</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>.</li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/urllib3/urllib3/compare/1.26.18...1.26.19"">https://github.com/urllib3/urllib3/compare/1.26.18...1.26.19</a></p>
<p>Note that due to an issue with our release automation, no <code> multiple.intoto.jsonl</code> file is available for this release.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/1.26.19/CHANGES.rst"">urllib3's changelog</a>.</em></p>
<blockquote>
<h2>1.26.19 (2024-06-17)</h2>
<ul>
<li>Added the <code>Proxy-Authorization</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>.</li>
<li>Fixed handling of OpenSSL 3.2.0 new error message for misconfiguring an HTTP proxy as HTTPS. (<code>[#3405](https://github.com/urllib3/urllib3/issues/3405) &lt;https://github.com/urllib3/urllib3/issues/3405&gt;</code>__)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/urllib3/urllib3/commit/d9d85c88aa644af56d5e129634e750ce76e1a765""><code>d9d85c8</code></a> Release 1.26.19</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/8528b63b6fe5cfd7b21942cf988670de68fcd8c0""><code>8528b63</code></a> [1.26] Fix downstream tests (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3409"">#3409</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/40b6d1605814dd1db0a46e202d6e56f2e4c9a468""><code>40b6d16</code></a> Merge pull request from GHSA-34jh-p97f-mpxf</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/29cfd02f66376c61bd20f1725477925106321f68""><code>29cfd02</code></a> Fix handling of OpenSSL 3.2.0 new error message &quot;record layer failure&quot; (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3405"">#3405</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/b60064388302f54a3455259ddab121618650a154""><code>b600643</code></a> [1.26] Bump RECENT_DATE (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3404"">#3404</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/7e2d3890926d4788e219f63e2e36fbeb8714827f""><code>7e2d389</code></a> [1.26] Fix running CPython 2.7 tests in CI (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3137"">#3137</a>)</li>
<li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.18...1.26.19"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.18&new-version=1.26.19)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
113,VAN Bulk Import Additions,https://api.github.com/repos/move-coop/parsons/issues/1078,Cody Gordon,1078,closed,2024-06-14 13:19:59+00:00,2024-06-27 20:09:47+00:00,2024-06-27 20:09:47+00:00,"- Added 2 new bulk import methods: 
  - Canvass Results
  - Contact Custom Fields
- Fixed a couple issues with the docstrings
- Added some additional contact column mappings for address zip and other email.",[],[],0
114,Airmeet Event Details connector with tests and documentation,https://api.github.com/repos/move-coop/parsons/issues/1077,Ankur,1077,open,2024-06-13 18:31:07+00:00,2024-11-19 01:32:28+00:00,,"This connector adds support for [Airmeet](https://www.airmeet.com/), a webinar platform.

Methods are included for everything listed in the [Event Details API](https://help.airmeet.com/support/solutions/articles/82000909768-1-event-details-airmeet-public-api) documentation. However, we don't currently have data in our account for booths, event tracks, or poll responses, so I haven't verified the data being returned from those.

This connector doesn't implement the [Manage Registrations](https://help.airmeet.com/support/solutions/articles/82000909769-2-manage-registrations-airmeet-public-api) or [Manage Events](https://help.airmeet.com/support/solutions/articles/82000909770-3-manage-event-airmeet-public-api) API methods.

Basic tests and documentation are included.",[],[],2
115,"Add ActionKit methods to add a phone number, create and update event fields, and search events in a campaign",https://api.github.com/repos/move-coop/parsons/issues/1076,Ankur,1076,closed,2024-06-12 00:19:23+00:00,2024-06-18 23:32:58+00:00,2024-06-18 23:32:58+00:00,"This pull request:
* Adds a method to add a phone number to a user. (Update method is available in https://github.com/move-coop/parsons/pull/1074)
* Adds methods to create and update custom event fields.
* Adds a method to interact with the [high-level event search API](https://roboticdogs.actionkit.com/docs/manual/api/rest/examples/eventsearch.html).
",[],[],0
116,[Feature/Addition] Add CampaignId to ngpvan apply_response(),https://api.github.com/repos/move-coop/parsons/issues/1075,,1075,closed,2024-06-11 15:29:12+00:00,2024-07-22 01:12:08+00:00,2024-07-22 01:12:07+00:00,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
An org I am working with would like to be able to sync `campaignId` in a survey response sync into VAN myVoters. This just requires a small change to the apply_response method because `campaignId` is an available parameter in the canvass context of the canvass response endpoint (https://docs.ngpvan.com/reference/peoplevanidcanvassresponses)

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->
Add `campaignId` as an optional parameter in the ngpvan `apply_response()` method (https://github.com/move-coop/parsons/blob/d512faa4dde4d3ad6434be6c2fd5f05e66ab46d3/parsons/ngpvan/people.py#L578). It should be added as an optional argument in the function and thus added to the canvassContext portion of the json. 

## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
Other users would be able to leverage this function to ensure campaigns are properly associated with canvassresponses when added to ngpvan

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->
see detailed description

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
The member would like this implemented before the end of July so ideally it would be added to parsons before the end of June!!","[Label(name=""enhancement""), Label(name=""high priority""), Label(name=""medium priority"")]",[],3
117,"Add methods to update phone, order user detail, and import action",https://api.github.com/repos/move-coop/parsons/issues/1074,Alex French,1074,closed,2024-06-07 00:01:33+00:00,2024-06-18 23:31:03+00:00,2024-06-18 23:31:02+00:00,"This pull request:
- Adds methods to update phone, order user detail, and import action in ActionKit
- Adds corresponding unit tests
- Fixes code comments/documentation for the `update_user()` function",[],[],1
118,Fix VAN create_event location_ids list bug,https://api.github.com/repos/move-coop/parsons/issues/1073,Cody Gordon,1073,closed,2024-06-06 14:45:44+00:00,2024-06-18 23:33:09+00:00,2024-06-18 23:33:09+00:00,"Looks like this was defined as a list within a tuple, and events were not being associated with provided locations. Confirmed provided locations are properly associated with created events with this change.",[],[],0
119,Fix misspecified tests using MagicMock().called_with(),https://api.github.com/repos/move-coop/parsons/issues/1072,Austin Weisgrau,1072,closed,2024-06-03 21:21:19+00:00,2024-06-25 20:42:25+00:00,2024-06-25 20:42:25+00:00,"See issue #1071 

The tests which use this method must be rewritten - some of them fail once the misspecified method is fixed, and will need to be rewritten to actually test what they intend to test.

TO DO:
- [x] Fix remaining twilio tests
- [x] Fix salesforce tests
- [x] Fix documentation recommending use of `called_with()`

BLOCKING: 
- [x] In python 3.12 it's no longer syntactical to use MagicMock().called_with(), so these fixes must be made before the python 3.12 migration can be made (#1070)

",[],[],0
120,"[Bug] Tests and test documention use nonexistent method ""MagicMock().called_with()` leading to erroneous test passes",https://api.github.com/repos/move-coop/parsons/issues/1071,Austin Weisgrau,1071,closed,2024-06-03 21:18:40+00:00,2024-07-22 01:12:51+00:00,2024-07-22 01:12:51+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
`called_with` is not a real test method on MagicMock, so it returns a MagicMock instance by default, which is truthy and evaluates to true.

Tests using this method `assert MagicMock().called_with()` are essentially running `assert MagicMock()` which is the same as `assert True`. These tests aren't checking for anything and pass no matter what actually happens in the code.

The documentation on how to write tests incorrectly advocates using this syntax:
```
docs/write_tests.rst
136:    assert self.sf._client.query_all.called_with('FAKESOQL')
```","[Label(name=""bug"")]",[],0
121,Support for python3.12,https://api.github.com/repos/move-coop/parsons/issues/1070,Josh Wenk,1070,closed,2024-06-03 20:32:25+00:00,2024-07-19 01:38:18+00:00,2024-06-26 16:35:43+00:00,"This incorporates 
- library bumps for installation
- introduction of Setuptools to requirements.txt (due to PEP 632 & gh-95299)
- updating workflows and setup.py to reflect 3.12 support
- some updates to test suite to handle erros due to syntax changes with 3.12 (in progress, a little more to go here with test_twilio.py; also then will need to reflect similar syntax updates with test_salesforce.py)",[],[],9
122,Enable support for python 3.12,https://api.github.com/repos/move-coop/parsons/issues/1069,Austin Weisgrau,1069,closed,2024-06-03 17:24:05+00:00,2024-06-03 19:35:45+00:00,2024-06-03 19:35:38+00:00,Addresses #1021 ,[],[],1
123,"Remove support for dict type, add helpful exception message",https://api.github.com/repos/move-coop/parsons/issues/1068,Austin Weisgrau,1068,closed,2024-05-30 23:03:39+00:00,2024-07-11 18:51:25+00:00,2024-07-11 18:51:22+00:00,Suggestion for how to resolve #1028 ,[],"[NamedUser(login=""KasiaHinkson"")]",1
124,Hot fix for MobileCommons.create_profile() method,https://api.github.com/repos/move-coop/parsons/issues/1067,Cormac Martinez del Rio,1067,closed,2024-05-29 20:15:10+00:00,2024-05-29 20:46:00+00:00,2024-05-29 20:45:57+00:00,Made a silly mistake last round of updates and used dict.merge() when I should have used dict.update(),[],[],0
125,Sunset support for Python 3.8,https://api.github.com/repos/move-coop/parsons/issues/1066,Josh Wenk,1066,open,2024-05-29 05:37:40+00:00,2024-07-11 19:05:48+00:00,,"3.8 looks to be EOL as of ~Oct 2024 -- see [3.8 Lifespan](https://peps.python.org/pep-0569/#lifespan).
Also: Looks like some dependencies (such as Civis) have started dropping support for 3.8 in their latest versions.","[Label(name=""enhancement"")]",[],0
126,Add option to disaggregate A/B test emails,https://api.github.com/repos/move-coop/parsons/issues/1065,Paul Anzel,1065,closed,2024-05-28 02:45:52+00:00,2024-06-06 22:11:26+00:00,2024-06-06 22:11:26+00:00,Addresses #1059 - this gives us an optional parameter to the NGP email endpoint that lets us dis-aggregate A/B test results from the endpoint. I also did a bit of code cleanup and added some tests.,[],[],1
127,"Revert ""New Feature: DBSync can upsert or append updated rows""",https://api.github.com/repos/move-coop/parsons/issues/1064,Shauna Gordon-McKeon,1064,closed,2024-05-23 23:08:12+00:00,2024-05-30 16:54:39+00:00,2024-05-23 23:13:42+00:00,Reverts move-coop/parsons#1029,[],[],0
128,add script,https://api.github.com/repos/move-coop/parsons/issues/1063,,1063,closed,2024-05-23 21:57:44+00:00,2024-05-23 23:13:55+00:00,2024-05-23 23:13:55+00:00,Add printed list sample script ,[],[],0
129,"If petl is loaded from a CSV, use the source file, don't re-write",https://api.github.com/repos/move-coop/parsons/issues/1062,Austin Weisgrau,1062,closed,2024-05-23 21:04:27+00:00,2024-06-06 20:59:48+00:00,2024-06-06 19:01:15+00:00,"No need to rewrite a CSV for a non-transformed petl that was loaded from a CSV on disk.

This can save a lot of time when working with large CSVs",[],"[NamedUser(login=""shaunagm"")]",0
130,Bump azure-storage-blob from 12.3.2 to 12.13.0,https://api.github.com/repos/move-coop/parsons/issues/1061,,1061,closed,2024-05-23 17:17:48+00:00,2024-05-23 19:10:28+00:00,2024-05-23 19:10:20+00:00,"Bumps [azure-storage-blob](https://github.com/Azure/azure-sdk-for-python) from 12.3.2 to 12.13.0.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/e90af4374bfd7c139737ad2888fcd269b3023520""><code>e90af43</code></a> DataLake funny dependency (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/25129"">#25129</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/cbec3383039ffeb46760268d1a8f81cf1b4d2219""><code>cbec338</code></a> [AutoRelease] t2-storagecache-2022-07-06-35884(Do not merge) (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/25089"">#25089</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/dc7c5a16d39df8a8d4b838a7240e58f64fc824f2""><code>dc7c5a1</code></a> [Storage] API View Feedback For STG84 GA (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/25085"">#25085</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/9f66f6bce7d777b34c03dc9a633148acd0c4f238""><code>9f66f6b</code></a> [Storage] Revert removing aiohttp dependency for storage.blob.aio (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/25084"">#25084</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/e40d3e1d985cee13a2e0d070fb8e04958905f468""><code>e40d3e1</code></a> [storage.blob] Remove aiohttp as dependency for storage.blob.aio (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/24965"">#24965</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/7915719f211cc1217dfca6f3973a2b1f04c2e3f5""><code>7915719</code></a> [Storage] Prepare for STG83 GA release (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/25040"">#25040</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/155eb8b69b3cd2f8ef992cf1436bf2751769ac42""><code>155eb8b</code></a> [Storage] Add <code>progress_hook</code> to file-share upload/download (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/24997"">#24997</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/66dd3bef2d6e531e83cefd65be4cbbf41fcf2531""><code>66dd3be</code></a> [Storage] Fix more flaky lease tests (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/25011"">#25011</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/030141734a239fa6fb1aa7a8c43d322c82753510""><code>0301417</code></a> [Storage] Add argument to perf tests to use client-side encryption (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/24978"">#24978</a>)</li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/489906539fe89167d749431d77202e7ca9562fac""><code>4899065</code></a> [perf] Add pipeline template and storage pipelines (<a href=""https://redirect.github.com/Azure/azure-sdk-for-python/issues/24894"">#24894</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/Azure/azure-sdk-for-python/compare/azure-storage-blob_12.3.2...azure-storage-blob_12.13.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=azure-storage-blob&package-manager=pip&previous-version=12.3.2&new-version=12.13.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
131,[Bug] Fix tests for `test_google_sheets`,https://api.github.com/repos/move-coop/parsons/issues/1060,Paul Anzel,1060,closed,2024-05-21 22:28:48+00:00,2024-08-01 19:00:25+00:00,2024-08-01 19:00:25+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->


## Detailed Description

There are some fixes we need to make to https://github.com/move-coop/parsons/blob/main/test/test_google/test_google_sheets.py. Specifically:
- tearDown is commented out - I presume we should re-enable it.
- Add a few strategic `time.sleep` calls so that you don't hit the [rate limit](https://developers.google.com/sheets/api/limits#quota).
  - Longer term maybe we want to look at https://docs.gspread.org/en/v6.0.0/api/http_client.html#gspread.BackOffHTTPClient ?
- `test_read_worksheet` and `test_read_sheet` read from fixed worksheets that many folks will not have access to. Can we have this point to the spreadsheet we make in setUp instead?
- `test_append_to_spreadsheet` literally says ""BROKEN TEST"", we should fix.","[Label(name=""bug"")]",[],0
132,[Feature] Add optional parameter to disaggregate A/B tests in NGP email code,https://api.github.com/repos/move-coop/parsons/issues/1059,Paul Anzel,1059,closed,2024-05-21 22:13:59+00:00,2024-07-22 01:13:16+00:00,2024-07-22 01:13:16+00:00,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->


## Detailed Description

I'd like to add an optional parameter to `ngpvan.email` code that allows you to avoid aggregating A/B test results in the `get_email_stats`  method.


## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->


## Possible Implementation

New method call: `get_email_stats(aggregate_ab=True)`, and if aggregate_ab is false then the try block does not aggregate.

## Priority
Medium. I can do this.","[Label(name=""enhancement"")]",[],0
133,"[Bug] GoogleAdmin connector doesn't work, all requests fail silently",https://api.github.com/repos/move-coop/parsons/issues/1058,Austin Weisgrau,1058,open,2024-05-21 18:38:06+00:00,2024-05-21 18:38:35+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
The GoogleAdmin connector has 3 methods. None of these methods can return any data. The methods will return an empty table. Digging in, every request returns an error (example below), but the connector does not notice the error and simply returns an empty Table instead.

```{'error': {'code': 400, 'message': 'Bad Request', 'errors': [{'message': 'Bad Request', 'domain': 'global', 'reason': 'badRequest'}]}}```

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->
```
from parsons import GoogleAdmin

tbl = GoogleAdmin().get_all_groups()
```

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.

I don't actually need this working, and based on context clues no one else currently does either. I just noticed this and so I'm logging the error.","[Label(name=""bug"")]",[],0
134,[Enhancement] - Adding Create Ticket Function To Freshdesk,https://api.github.com/repos/move-coop/parsons/issues/1057,,1057,closed,2024-05-21 15:12:56+00:00,2024-05-23 23:06:46+00:00,2024-05-23 23:06:45+00:00,"- Added a _post function for connector 
- Added a create ticket function that will be used to create a ticket in Freshdesk (more info here: https://developers.freshdesk.com/api/#tickets)

@shaunagm please let me know if there is any question or any feedback :)
",[],[],0
135,Bump requests from 2.31.0 to 2.32.0,https://api.github.com/repos/move-coop/parsons/issues/1056,,1056,closed,2024-05-21 05:54:16+00:00,2024-05-23 20:01:34+00:00,2024-05-23 20:01:16+00:00,"Bumps [requests](https://github.com/psf/requests) from 2.31.0 to 2.32.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/psf/requests/releases"">requests's releases</a>.</em></p>
<blockquote>
<h2>v2.32.0</h2>
<h2>2.32.0 (2024-05-20)</h2>
<h2>🐍 PYCON US 2024 EDITION 🐍</h2>
<p><strong>Security</strong></p>
<ul>
<li>Fixed an issue where setting <code>verify=False</code> on the first request from a
Session will cause subsequent requests to the <em>same origin</em> to also ignore
cert verification, regardless of the value of <code>verify</code>.
(<a href=""https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>
</ul>
<p><strong>Improvements</strong></p>
<ul>
<li><code>verify=True</code> now reuses a global SSLContext which should improve
request time variance between first and subsequent requests. It should
also minimize certificate load time on Windows systems when using a Python
version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>
<li>Requests now supports optional use of character detection
(<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.
This enables <code>pip</code> and other projects to minimize their vendoring
surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs
will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>
</ul>
<p><strong>Bugfixes</strong></p>
<ul>
<li>Fixed bug in length detection where emoji length was incorrectly
calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>
<li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>
<li>Fixed bug where an extra leading <code>/</code> (path separator) could lead
urllib3 to unnecessarily reparse the request URI. (<a href=""https://redirect.github.com/psf/requests/issues/6644"">#6644</a>)</li>
</ul>
<p><strong>Deprecations</strong></p>
<ul>
<li>Requests has officially added support for CPython 3.12 (<a href=""https://redirect.github.com/psf/requests/issues/6503"">#6503</a>)</li>
<li>Requests has officially added support for PyPy 3.9 and 3.10 (<a href=""https://redirect.github.com/psf/requests/issues/6641"">#6641</a>)</li>
<li>Requests has officially dropped support for CPython 3.7 (<a href=""https://redirect.github.com/psf/requests/issues/6642"">#6642</a>)</li>
<li>Requests has officially dropped support for PyPy 3.7 and 3.8 (<a href=""https://redirect.github.com/psf/requests/issues/6641"">#6641</a>)</li>
</ul>
<p><strong>Documentation</strong></p>
<ul>
<li>Various typo fixes and doc improvements.</li>
</ul>
<p><strong>Packaging</strong></p>
<ul>
<li>Requests has started adopting some modern packaging practices.
The source files for the projects (formerly <code>requests</code>) is now located
in <code>src/requests</code> in the Requests sdist. (<a href=""https://redirect.github.com/psf/requests/issues/6506"">#6506</a>)</li>
<li>Starting in Requests 2.33.0, Requests will migrate to a PEP 517 build system
using <code>hatchling</code>. This should not impact the average user, but extremely old
versions of packaging utilities may have issues with the new packaging format.</li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/matthewarmand""><code>@​matthewarmand</code></a> made their first contribution in <a href=""https://redirect.github.com/psf/requests/pull/6258"">psf/requests#6258</a></li>
<li><a href=""https://github.com/cpzt""><code>@​cpzt</code></a> made their first contribution in <a href=""https://redirect.github.com/psf/requests/pull/6456"">psf/requests#6456</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>
<blockquote>
<h2>2.32.0 (2024-05-20)</h2>
<p><strong>Security</strong></p>
<ul>
<li>Fixed an issue where setting <code>verify=False</code> on the first request from a
Session will cause subsequent requests to the <em>same origin</em> to also ignore
cert verification, regardless of the value of <code>verify</code>.
(<a href=""https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>
</ul>
<p><strong>Improvements</strong></p>
<ul>
<li><code>verify=True</code> now reuses a global SSLContext which should improve
request time variance between first and subsequent requests. It should
also minimize certificate load time on Windows systems when using a Python
version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>
<li>Requests now supports optional use of character detection
(<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.
This enables <code>pip</code> and other projects to minimize their vendoring
surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs
will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>
</ul>
<p><strong>Bugfixes</strong></p>
<ul>
<li>Fixed bug in length detection where emoji length was incorrectly
calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>
<li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>
<li>Fixed bug where an extra leading <code>/</code> (path separator) could lead
urllib3 to unnecessarily reparse the request URI. (<a href=""https://redirect.github.com/psf/requests/issues/6644"">#6644</a>)</li>
</ul>
<p><strong>Deprecations</strong></p>
<ul>
<li>Requests has officially added support for CPython 3.12 (<a href=""https://redirect.github.com/psf/requests/issues/6503"">#6503</a>)</li>
<li>Requests has officially added support for PyPy 3.9 and 3.10 (<a href=""https://redirect.github.com/psf/requests/issues/6641"">#6641</a>)</li>
<li>Requests has officially dropped support for CPython 3.7 (<a href=""https://redirect.github.com/psf/requests/issues/6642"">#6642</a>)</li>
<li>Requests has officially dropped support for PyPy 3.7 and 3.8 (<a href=""https://redirect.github.com/psf/requests/issues/6641"">#6641</a>)</li>
</ul>
<p><strong>Documentation</strong></p>
<ul>
<li>Various typo fixes and doc improvements.</li>
</ul>
<p><strong>Packaging</strong></p>
<ul>
<li>Requests has started adopting some modern packaging practices.
The source files for the projects (formerly <code>requests</code>) is now located
in <code>src/requests</code> in the Requests sdist. (<a href=""https://redirect.github.com/psf/requests/issues/6506"">#6506</a>)</li>
<li>Starting in Requests 2.33.0, Requests will migrate to a PEP 517 build system
using <code>hatchling</code>. This should not impact the average user, but extremely old
versions of packaging utilities may have issues with the new packaging format.</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/psf/requests/commit/d6ebc4a2f1f68b7e355fb7e4dd5ffc0845547f9f""><code>d6ebc4a</code></a> v2.32.0</li>
<li><a href=""https://github.com/psf/requests/commit/9a40d1277807f0a4f26c9a37eea8ec90faa8aadc""><code>9a40d12</code></a> Avoid reloading root certificates to improve concurrent performance (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>
<li><a href=""https://github.com/psf/requests/commit/0c030f78d24f29a459dbf39b28b4cc765e2153d7""><code>0c030f7</code></a> Merge pull request <a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a> from nateprewitt/no_char_detection</li>
<li><a href=""https://github.com/psf/requests/commit/555b870eb19d497ddb67042645420083ec8efb02""><code>555b870</code></a> Allow character detection dependencies to be optional in post-packaging steps</li>
<li><a href=""https://github.com/psf/requests/commit/d6dded3f00afcf56a7e866cb0732799045301eb0""><code>d6dded3</code></a> Merge pull request <a href=""https://redirect.github.com/psf/requests/issues/6700"">#6700</a> from franekmagiera/update-redirect-to-invalid-uri-test</li>
<li><a href=""https://github.com/psf/requests/commit/bf24b7d8d17da34be720c19e5978b2d3bf94a53b""><code>bf24b7d</code></a> Use an invalid URI that will not cause httpbin to throw 500</li>
<li><a href=""https://github.com/psf/requests/commit/2d5f54779ad174035c5437b3b3c1146b0eaf60fe""><code>2d5f547</code></a> Pin 3.8 and 3.9 runners back to macos-13 (<a href=""https://redirect.github.com/psf/requests/issues/6688"">#6688</a>)</li>
<li><a href=""https://github.com/psf/requests/commit/f1bb07d39b74d6444e333879f8b8a3d9dd4d2311""><code>f1bb07d</code></a> Merge pull request <a href=""https://redirect.github.com/psf/requests/issues/6687"">#6687</a> from psf/dependabot/github_actions/github/codeql-act...</li>
<li><a href=""https://github.com/psf/requests/commit/60047ade64b0b882cbc94e047198818ab580911e""><code>60047ad</code></a> Bump github/codeql-action from 3.24.0 to 3.25.0</li>
<li><a href=""https://github.com/psf/requests/commit/31ebb8102c00f8cf8b396a6356743cca4362e07b""><code>31ebb81</code></a> Merge pull request <a href=""https://redirect.github.com/psf/requests/issues/6682"">#6682</a> from frenzymadness/pytest8</li>
<li>Additional commits viewable in <a href=""https://github.com/psf/requests/compare/v2.31.0...v2.32.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=requests&package-manager=pip&previous-version=2.31.0&new-version=2.32.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],3
136,"Update GET helper function, add custom columns to create profile",https://api.github.com/repos/move-coop/parsons/issues/1055,Cormac Martinez del Rio,1055,closed,2024-05-15 20:31:46+00:00,2024-05-28 15:24:42+00:00,2024-05-23 17:42:13+00:00,"1) Update GET helper function - There was a small bug in the logic for get endpoints where pagination is controled with a ""num"" page indicator. In short, line 140 was returning false when it should have been returning true! 
2) Correct phone_numbers aspect of get_profiles - turns out mobile commons accepts a comma separated string of phone numbers, not a list. 
3) Added the option to write custom column data to profiles in the create_profiles method. Also updated documentation to indicate that the create_profiles method can also be used to update existing records. Considered creating a new method all together for update_profile, but it seemed a little bit redundant... ","[Label(name=""enhancement""), Label(name=""bug fix"")]",[],1
137,bump braintree to 4.17.1,https://api.github.com/repos/move-coop/parsons/issues/1054,Josh Wenk,1054,closed,2024-05-14 23:15:24+00:00,2024-05-23 19:35:57+00:00,2024-05-23 19:35:57+00:00,"As mentioned in [this bug report](https://github.com/move-coop/parsons/issues/1007), **_the currently pinned version of braintree (4.0.0) is a yanked version_** (due to ""critical bugs"" -- see more [here](https://pypi.org/project/braintree/4.28.0/#history)).

While this PR bumps to _**earliest non-yanked version**_ (4.17.1)
The latest version is 4.28, however it appears 4.18-4.28 all give Deprecation Warnings
- _DeprecationWarning: Use ProtectionLevel enum instead_ 
- _DeprecationWarning: Use protection_level parameter instead_

(Thought being: handle immediate issue first, then worry about upgrade to latest, if/as needed)",[],[],3
138,"Update version on precommit hook to match requirements-dev, and make …",https://api.github.com/repos/move-coop/parsons/issues/1053,Shauna Gordon-McKeon,1053,closed,2024-05-14 20:11:41+00:00,2024-05-14 20:23:48+00:00,2024-05-14 20:23:47+00:00,…linting output more verbose,[],[],0
139,"[Enhancement] - Enable Unloading Redshift Tables to S3 in either JSON, PARQUET or CSV format",https://api.github.com/repos/move-coop/parsons/issues/1052,,1052,open,2024-05-12 15:33:07+00:00,2024-07-16 19:55:37+00:00,,"- Adding the `format` parameter to the unload from redshift to S3 function. We can use either CSV or JSON or PARQUET (highly recommended for large tables, using PARQUET files are smaller than CSVs, can be read and written much faster compared to CSV), currently we only enable to unload as a TXT file by default.
- Including only supported options to the unload query.
- Unloading as a CSV does not support `ADDQUOTES`.
- Unloading as a PARQUET does not support `DELIMITER`, `ADDQUOTES`, `ESCAPE`, `NULL AS`, `HEADER`, `GZIP (compression)` options.
- Unloading as a JSON does not support `DELIMITER`, `HEADER`, `ADDQUOTES`, `ESCAPE`, `NULL AS` options.

<img width=""775"" alt=""image"" src=""https://github.com/move-coop/parsons/assets/75395024/caa999c5-0787-4c04-b65e-f5c3f26e0fa8"">

Hope this will be beneficial to anyone except me and pass all of the live tests. @shaunagm please let me know if there are any changes that need to be made to get this implemented.
Thank you!
More on PARQUET [here](https://parquet.apache.org/docs/file-format/).
More on UNLOAD [here](https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html).
",[],[],3
140,Psycopg2 version bump,https://api.github.com/repos/move-coop/parsons/issues/1051,Paul Anzel,1051,closed,2024-05-10 00:04:46+00:00,2024-05-14 19:41:32+00:00,2024-05-14 19:41:32+00:00,"This is a more permanent solution to our issue with the failing `macos-latest` CI test for Python 3.11. Turns out we were installing psycopg2 2.9.2, and the library does not officially support Python 3.11 until 2.9.5. I bumped it up to the latest version (2.9.9) and everything appears to work as expected, but I have not executed any of the live tests.",[],[],0
141,Sqlalchemy Update to  >= 1.4.22,https://api.github.com/repos/move-coop/parsons/issues/1050,Matthew Krausse,1050,closed,2024-05-09 20:50:02+00:00,2024-05-14 20:00:38+00:00,2024-05-14 20:00:38+00:00,Will update the version to match the prefect reqs file. This just makes sure we can resolve a conflict and the prefect req seems to be the best way to handle sqlalchemy.,[],[],5
142,Update mac runner to macos-12 instead of mac-latest,https://api.github.com/repos/move-coop/parsons/issues/1049,Shauna Gordon-McKeon,1049,closed,2024-05-09 19:11:38+00:00,2024-05-09 19:16:59+00:00,2024-05-09 19:16:58+00:00,Hopefully this will fix the breaking tests that are stalling PR merges,[],[],0
143,"[Bug] Unable to install parsons on Python 3.11, Linux distro",https://api.github.com/repos/move-coop/parsons/issues/1048,Paul Anzel,1048,closed,2024-05-03 00:33:08+00:00,2024-07-27 02:44:03+00:00,2024-07-27 02:44:03+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->


## Detailed Description

I have tried installing Parsons on my Linux laptop, but found that while I could install with Python 3.10, I would run into an issue on Python 3.11.

## To Reproduce

```
# Using Anaconda as my virtualenv provider
conda create --name parsons_dev python=3.11
conda activate parsons_dev
pip install parsons
```
And after some installation, I get the following output
```
# A bunch of successful Collecting outputs
Collecting pyyaml<=5.99,>=3.0 (from civis==1.14.2->parsons)
  Using cached PyYAML-5.4.1.tar.gz (175 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> [54 lines of output]
      running egg_info
      writing lib3/PyYAML.egg-info/PKG-INFO
      writing dependency_links to lib3/PyYAML.egg-info/dependency_links.txt
      writing top-level names to lib3/PyYAML.egg-info/top_level.txt
      Traceback (most recent call last):
        File ""/home/anzelpwj/.pyenv/versions/miniconda3-latest/envs/parsons_dev2/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>
          main()
        File ""/home/anzelpwj/.pyenv/versions/miniconda3-latest/envs/parsons_dev2/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/home/anzelpwj/.pyenv/versions/miniconda3-latest/envs/parsons_dev2/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 118, in get_requires_for_build_wheel
          return hook(config_settings)
                 ^^^^^^^^^^^^^^^^^^^^^
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/build_meta.py"", line 325, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=['wheel'])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/build_meta.py"", line 295, in _get_build_requires
          self.run_setup()
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/build_meta.py"", line 311, in run_setup
          exec(code, locals())
        File ""<string>"", line 271, in <module>
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/__init__.py"", line 104, in setup
          return distutils.core.setup(**attrs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py"", line 184, in setup
          return run_commands(dist)
                 ^^^^^^^^^^^^^^^^^^
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py"", line 200, in run_commands
          dist.run_commands()
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py"", line 969, in run_commands
          self.run_command(cmd)
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/dist.py"", line 967, in run_command
          super().run_command(command)
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py"", line 988, in run_command
          cmd_obj.run()
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py"", line 321, in run
          self.find_sources()
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py"", line 329, in find_sources
          mm.run()
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py"", line 550, in run
          self.add_defaults()
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py"", line 588, in add_defaults
          sdist.add_defaults(self)
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/command/sdist.py"", line 102, in add_defaults
          super().add_defaults()
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/sdist.py"", line 250, in add_defaults
          self._add_defaults_ext()
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/sdist.py"", line 335, in _add_defaults_ext
          self.filelist.extend(build_ext.get_source_files())
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""<string>"", line 201, in get_source_files
        File ""/tmp/pip-build-env-xw9cu7ja/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py"", line 107, in __getattr__
          raise AttributeError(attr)
      AttributeError: cython_sources
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
```

## Your Environment

Parsons: 3.1.0
Python: 3.11.9
OS: Pop!_OS 22.04 LTE (based on Ubuntu 22.04 LTE)

## Additional Context

It appears that PyYAML <6.0.1 has issues installing on Python 3.11 (https://github.com/yaml/pyyaml/issues/736). It appears that we are installing an older version of the civis library (1.14.2, current is 1.61.1), so it might be good to see if we can upgrade the civis dependency and see if it fixes this issue.

@elyse-weiss appears to have encountered the same installation issue (discussion on Slack).

## Priority

Medium, prevents use of Python 3.11 for me.","[Label(name=""bug"")]",[],6
144,Try different runner,https://api.github.com/repos/move-coop/parsons/issues/1047,Paul Anzel,1047,closed,2024-05-01 23:05:48+00:00,2024-05-01 23:07:55+00:00,2024-05-01 23:07:39+00:00,Just want to see if using the `macos-latest-large` runner works (I think it's M1).,[],[],1
145,[Bug] Update contributor guidelines to be in sync with current library,https://api.github.com/repos/move-coop/parsons/issues/1046,Paul Anzel,1046,closed,2024-04-28 23:44:04+00:00,2024-07-23 19:56:52+00:00,2024-07-23 19:56:52+00:00,"## Detailed Description

Hi all,

I made my first PR just now, but I noticed that some of the different contributor guidelines are out of sync.

I came across [this guide on the main webpage](https://www.parsonsproject.org/pub/contributing-guide/release/4), which is out of sync with [CONTRIBUTING.md](https://github.com/move-coop/parsons/blob/edfa387d8554b7eca56284f9bf91cd20c444f635/CONTRIBUTING.md?plain=1#L1) (for example, `CONTRIBUTING.md` has different linting instructions and refers to pre-commit).

## To Reproduce
 
N/A

## Your Environment

N/A


## Additional Context

N/A


## Priority

Low - I did find the main markdown file, but I had come across the website first (via the documentation generated from https://github.com/move-coop/parsons/blob/main/docs/contributing.rst). Perhaps we should point to the GitHub CONTRIBUTING.md file instead?","[Label(name=""bug"")]",[],0
146,Adding a data paste function to Google Sheets,https://api.github.com/repos/move-coop/parsons/issues/1045,Paul Anzel,1045,closed,2024-04-28 19:03:15+00:00,2024-05-14 19:51:26+00:00,2024-05-14 19:51:26+00:00,"Hi all,

Following up on issue #1036 . I tried initially modifying the `append_to_sheet` function, but it became clear that this is simply asking too much for one function, so I made a new method `paste_data_in_sheet` instead.

I don't have access to the Google Sheets sandbox, so for now `test_paste_data_in_sheet` has not been confirmed to work as expected (but I have done some testing with my own spreadsheet).

Any advice on how I've designed this/getting this code ready for merging would be much appreciated, thank you!",[],[],4
147,OpenField Class,https://api.github.com/repos/move-coop/parsons/issues/1044,Cody Gordon,1044,open,2024-04-28 17:52:15+00:00,2024-06-14 20:38:37+00:00,,"https://github.com/move-coop/parsons/issues/1011

cc @TheReFTW - please loop in anyone from your team who should help maintain this!",[],[],4
148,Nirs freshdesk changes,https://api.github.com/repos/move-coop/parsons/issues/1043,,1043,closed,2024-04-25 14:57:16+00:00,2024-05-21 15:13:28+00:00,2024-05-21 15:13:28+00:00,"- Added a _post function for connector 
- Added a create ticket function that will be used to create a ticket in Freshdesk (more info here: https://developers.freshdesk.com/api/#tickets)

@shaunagm please let me know if there is any question or any feedback :)

PS: this PR contains also the changes from PR #1025 please let me know if it is not ok and I should rebase the branch based on main rather than my other branch which has another open PR.",[],[],0
149,Allow passing google auth Credentials directly to BQ connector,https://api.github.com/repos/move-coop/parsons/issues/1042,Austin Weisgrau,1042,closed,2024-04-24 18:12:24+00:00,2024-04-24 18:25:31+00:00,2024-04-24 18:22:06+00:00,"This allows for using oauth2 to authenticate the connector, rather than a service account.",[],[],0
150,BigQuery extract table method,https://api.github.com/repos/move-coop/parsons/issues/1041,Austin Weisgrau,1041,closed,2024-04-23 17:34:23+00:00,2024-06-21 00:23:12+00:00,2024-06-21 00:23:09+00:00,"Addresses #921 

Unlike in Redshift, you can't directly export query results to GCS. You can only extract a table.",[],[],0
151,Pass credentials directly to GCP connectors rather than through environment variable,https://api.github.com/repos/move-coop/parsons/issues/1040,Austin Weisgrau,1040,closed,2024-04-22 17:10:02+00:00,2024-07-12 15:50:28+00:00,2024-07-12 15:50:20+00:00,"Pass credentials directly to GCP connectors rather than through shared environment variable. Avoids issue documented in move-coop#1039 where credentials for all GCP clients are stored in the same environment variable, leading to overwrites if multiple clients are initialized in the same environment.

TODO
 - [x] Fix for BigQuery connector 
 - [x] Test BigQuery connector works with live creds
 - [x] Fix for Sheets connector
 - [x] Test Sheets connector works with live creds
 - [x] Fix for Admin connector
 - [x] Test Admin connector works with live creds
 - [x] Fix for CloudStorage connector
 - [x] Test CloudStorage connector works with live creds
 - [x] Fix for tests",[],[],6
152,[Bug] Multiple instances of Google connectors cannot be used simultaneously with different credentials,https://api.github.com/repos/move-coop/parsons/issues/1039,Austin Weisgrau,1039,closed,2024-04-22 16:52:25+00:00,2024-07-22 01:15:56+00:00,2024-07-22 01:15:56+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
When multiple instances of Google connectors are initialized, each successive connector overwrites the same `GOOGLE_APPLICATION_CREDENTIALS` environment variable, thereby overwriting the authentication of the prior connectors.


## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
`parsons.google.utitities.setup_google_application_credentials()` takes a dictionary of application credentials as an argument and stores them generically in the `GOOGLE_APPLICATION_CREDENTIALS` environment variable. This environment variable is then read by instances of GCP clients rather than passed directly to them. This makes it impossible to initialize multiple connectors with different credentials in the same code.


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->
```
from utilities.google import GoogleBigQuery

first_client = GoogleBigQuery(app_creds=first_app_creds, project=""FIRST_PROJECT"")
second_client = GoogleBigQuery(app_creds=second_app_creds, project=""SECOND_PROJECT"")

first_client.query(""select session_user()"")
>> Forbidden: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/FIRST_PROJECT/jobs?prettyPrint=false: Access Denied: Project FIRST_PROJECT: User does not have bigquery.jobs.create permission in project FIRST_PROJECT.
```


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.","[Label(name=""bug"")]",[],0
153,Remove obsolete argument to google storage client list_blobs(),https://api.github.com/repos/move-coop/parsons/issues/1038,Austin Weisgrau,1038,closed,2024-04-17 00:36:59+00:00,2024-04-17 17:05:50+00:00,2024-04-17 17:05:44+00:00,"match_glob is not a valid argument to google.storage.Client().list_blobs()

I can't really tell when this argument was deprecated, it doesn't exist even in the pinned version of the google-cloud-storage package. Not sure how this broke.",[],[],3
154,Update sqlalchemy version in requirements.txt,https://api.github.com/repos/move-coop/parsons/issues/1037,Shauna Gordon-McKeon,1037,closed,2024-04-16 20:42:16+00:00,2024-05-09 21:05:31+00:00,2024-05-09 20:51:57+00:00,At the request of @matthewkrausse (following conversation with @austinweisgrau in Slack),[],[],4
155,Feature request: Allow us to offset a set number of rows/columns in GoogleSheets append_to_sheet,https://api.github.com/repos/move-coop/parsons/issues/1036,Paul Anzel,1036,closed,2024-04-13 02:15:46+00:00,2024-07-22 01:16:44+00:00,2024-07-22 01:16:44+00:00,"## Detailed Description

Hi everyone! I'm putting in this request to allow users to set a number of offset rows and columns for `GoogleSheets.append_to_sheet`. For example, if I wanted my data block to start at C3, I'd give 2 skip rows and 2 skip columns.


## Context

I'm a volunteer with Tech for Campaigns, and I'd love to be able to use this tool to automate a process we have to download donation data from ActBlue and put it into a standardized spreadsheet that TFC uses for every campaigns. However, our sheets are organized in a fashion such that our data would need to start being added at position D6 (or a lower row, if I'm just updating donations after a given day).

## Possible Implementation

I'd suggest a method call something like:

```python
GoogleSheets.append_to_sheet(spreadsheet_id, table, worksheet=0, user_entered_value=False, skip_rows=0, skip_cols=0, **kwargs)
```

If `skip_rows` and `skip_cols` are both 0, the logic we have in lines 275-286 (https://github.com/move-coop/parsons/blob/3fc05e75d572a107f8c6b5af0609df0cc52564eb/parsons/google/google_sheets.py#L275) can proceed as normal. But if non-default values are provided, we simply drop the data in at skip_rows + 1, skip_cols + 1 in the spreadsheet.

Alternately, if it makes more sense to have something like `start_row=1, start_col=1` that would be fine as well.

## Priority

Medium - would simplify some grunt work for TFC, but it's not a hair-on-fire situation.","[Label(name=""enhancement"")]",[],4
156,Run upgraded black formatter on all files,https://api.github.com/repos/move-coop/parsons/issues/1035,Austin Weisgrau,1035,closed,2024-04-12 20:22:23+00:00,2024-05-23 20:20:55+00:00,2024-05-23 20:20:55+00:00,"We recently upgraded the version of black we use from 22 to 24 with 44cacb9171660486aab60ddae59daf5ad9271a57, PR #1022. This version of black formats files a little differently, so this commit runs black on all files to make those changes so that they're in compliance.",[],[],0
157,Removes bluelink connector because platform retired March 2024,https://api.github.com/repos/move-coop/parsons/issues/1034,elyse-weiss,1034,closed,2024-04-12 20:14:31+00:00,2024-04-12 20:35:54+00:00,2024-04-12 20:35:50+00:00,Removing the bluelink connector because the platform was retired in March of 2024. ,[],[],2
158,[Bug] BigQuery parameterization cannot handle `in %s` syntax,https://api.github.com/repos/move-coop/parsons/issues/1033,Austin Weisgrau,1033,open,2024-04-09 23:32:47+00:00,2024-04-09 23:32:47+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
Using the Redshift connector, you can parameterize an `in` statement like so:
`Redshift().query(""select * from table where value in %s"", parameters=[tuple(values)])`

However, with the BigQuery connector, this kind of syntax results in a syntax error.

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.","[Label(name=""bug"")]",[],0
159,assert correct arguments for test in #976,https://api.github.com/repos/move-coop/parsons/issues/1032,Wil T,1032,closed,2024-04-07 17:38:34+00:00,2024-05-23 22:48:23+00:00,2024-05-23 20:08:57+00:00,"There was an additional failure in #976 which I missed in #1019
I have addressed it here by changing the arguments and pytest now passes `810 passed, 190 skipped, 1 xfailed, 29 warnings in 25.60s`

It seems like the fix-copy-s3 branch is not getting linted and tested (only doc-build is being run by the ""build"" check) because it's behind significant changes to the CI workflow on main.",[],[],0
160,Google Slides & Google Drive,https://api.github.com/repos/move-coop/parsons/issues/1031,elyse-weiss,1031,open,2024-04-03 20:23:27+00:00,2024-07-24 18:07:15+00:00,,"Closes #957

# Notes
- This is my first time writing tests! They did seem to pass
- I was not able to run docs successfully, so I am not sure my new functions made it into docs",[],[],4
161,Move variable definition outside try/except block,https://api.github.com/repos/move-coop/parsons/issues/1030,Austin Weisgrau,1030,closed,2024-04-03 00:16:34+00:00,2024-05-23 20:04:11+00:00,2024-05-23 20:04:10+00:00,"A variable definition is used in the except block, so if the variable definition fails, the except block will raise another variable not defined exception.

",[],[],0
162,New Feature: DBSync can upsert or append updated rows,https://api.github.com/repos/move-coop/parsons/issues/1029,Austin Weisgrau,1029,closed,2024-04-02 23:57:32+00:00,2024-05-23 23:08:06+00:00,2024-05-23 20:10:33+00:00,"The DBSync method can already append new rows from a source db into a target db based on an incremental primary key.

This new method generalizes that behavior in a new feature which allows for appending or upserting rows that have an updated_at_column value in the source db greater than the value in the target db.",[],"[NamedUser(login=""KasiaHinkson"")]",4
163,Change dict to json,https://api.github.com/repos/move-coop/parsons/issues/1028,Kasia Hinkson,1028,closed,2024-04-02 22:32:02+00:00,2024-07-11 20:17:16+00:00,2024-07-11 20:12:21+00:00,"RECORD type requires the shape of the dictionary to be explicit and fails when the Parsons table has a dict value. I've tested with New/Mode data, and it loads successfully as JSON type. ",[],[],14
164,"Revert ""add list data type to mapping""",https://api.github.com/repos/move-coop/parsons/issues/1027,Kasia Hinkson,1027,closed,2024-04-02 17:47:12+00:00,2024-04-02 17:57:33+00:00,2024-04-02 17:57:30+00:00,Reverts move-coop/parsons#1026,[],[],0
165,add list data type to mapping,https://api.github.com/repos/move-coop/parsons/issues/1026,Kasia Hinkson,1026,closed,2024-04-01 19:34:39+00:00,2024-04-02 16:59:40+00:00,2024-04-02 16:59:37+00:00,Maps list to data type ARRAY,[],[],0
166,Features - ActionNetwork API - Unique ID Lists routes support  + SQL Mirror support,https://api.github.com/repos/move-coop/parsons/issues/1025,,1025,closed,2024-03-26 08:57:49+00:00,2024-07-16 17:33:59+00:00,2024-07-15 18:48:38+00:00," - There are new routes in ActionNetwork's API for [Unique ID Lists](https://actionnetwork.org/docs/v2/unique_id_lists) (GET one,GET many), added 2 functions so we can get the unique id lists collection or get a specific unique id list (this is very useful when you have children groups in AN).
 - Added a new function that enables us to query the read-only SQL mirror database (""The Action Network provides a read-only, continuously updated SQL mirror to partners who request it, containing a full copy of all of their group's data (and children, if the group is in a network). The SQL mirror is suitable for data dumps, writing custom dashboards, and analytics."", [more on the sql mirroring](https://actionnetwork.org/mirroring/docs)).
 
 @shaunagm could you please let me know if there are any questions on the new additions or if I missed/need to add anything?
PS:


- Created new utility called `SSHTunnelUtility` (utilities/ssh_utilities).
- Using the Redshift connector instead of psycopg and using the SSHTunnelUtility to connect through ssh instead of using SSHTunnelForwarder directly in actionnetwork.",[],[],2
167,Return PATCH response when updating AK user,https://api.github.com/repos/move-coop/parsons/issues/1024,Kathy Nguyen,1024,closed,2024-03-21 23:15:23+00:00,2024-04-04 20:57:54+00:00,2024-04-04 20:57:54+00:00,Return the response after patching an AK user. This is necessary to parse the status code after calling the `update_user` function,[],[],0
168,[Bug] BigQuery.upsert fails depending on table's column order,https://api.github.com/repos/move-coop/parsons/issues/1023,Jason,1023,open,2024-03-21 14:47:03+00:00,2024-03-21 14:50:02+00:00,,"`GoogleBigQuery.upsert()` fails if the columns are ordered differently in the database's table than in the Parsons table.

If the mismatched columns are the same data type, it will silently insert the data into the wrong columns. If the mismatched columns are different data types, then the upsert will fail with an error.

## To Reproduce
This script will throw an error when it attempts the upsert because columns `a` and `b` have different types:

```python
from parsons import Table
from parsons.google.google_bigquery import GoogleBigQuery

def main():
    bq = GoogleBigQuery()
    table_name = ""test.upsert_test_data""

    data = Table([{""a"": 1, ""b"": ""abc""}, {""a"": 3, ""b"": ""def""}])

    bq.copy(data, table_name, if_exists=""drop"")

    data2 = Table(
        [
            {""b"": ""xyz"", ""a"": 1},
            {""b"": ""zyx"", ""a"": 5},
        ]
    )

    bq.upsert(data2, table_name, ""a"")

    print(bq.query(f""SELECT * FROM {table_name}""))


if __name__ == ""__main__"":
    main()
```


## Your Environment
* Version of Parsons used (if you know): `main` branch as of 3/21/2024
* Operating System and version (desktop or mobile): Windows 10","[Label(name=""bug"")]",[],0
169,Bump black from 22.12.0 to 24.3.0,https://api.github.com/repos/move-coop/parsons/issues/1022,,1022,closed,2024-03-20 16:54:07+00:00,2024-04-04 21:34:46+00:00,2024-04-04 21:34:39+00:00,"Bumps [black](https://github.com/psf/black) from 22.12.0 to 24.3.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/psf/black/releases"">black's releases</a>.</em></p>
<blockquote>
<h2>24.3.0</h2>
<h3>Highlights</h3>
<p>This release is a milestone: it fixes Black's first CVE security vulnerability. If you
run Black on untrusted input, or if you habitually put thousands of leading tab
characters in your docstrings, you are strongly encouraged to upgrade immediately to fix
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-21503"">CVE-2024-21503</a>.</p>
<p>This release also fixes a bug in Black's AST safety check that allowed Black to make
incorrect changes to certain f-strings that are valid in Python 3.12 and higher.</p>
<h3>Stable style</h3>
<ul>
<li>Don't move comments along with delimiters, which could cause crashes (<a href=""https://redirect.github.com/psf/black/issues/4248"">#4248</a>)</li>
<li>Strengthen AST safety check to catch more unsafe changes to strings. Previous versions
of Black would incorrectly format the contents of certain unusual f-strings containing
nested strings with the same quote type. Now, Black will crash on such strings until
support for the new f-string syntax is implemented. (<a href=""https://redirect.github.com/psf/black/issues/4270"">#4270</a>)</li>
<li>Fix a bug where line-ranges exceeding the last code line would not work as expected
(<a href=""https://redirect.github.com/psf/black/issues/4273"">#4273</a>)</li>
</ul>
<h3>Performance</h3>
<ul>
<li>Fix catastrophic performance on docstrings that contain large numbers of leading tab
characters. This fixes
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-21503"">CVE-2024-21503</a>.
(<a href=""https://redirect.github.com/psf/black/issues/4278"">#4278</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Note what happens when <code>--check</code> is used with <code>--quiet</code> (<a href=""https://redirect.github.com/psf/black/issues/4236"">#4236</a>)</li>
</ul>
<h2>24.2.0</h2>
<h3>Stable style</h3>
<ul>
<li>Fixed a bug where comments where mistakenly removed along with redundant parentheses
(<a href=""https://redirect.github.com/psf/black/issues/4218"">#4218</a>)</li>
</ul>
<h3>Preview style</h3>
<ul>
<li>Move the <code>hug_parens_with_braces_and_square_brackets</code> feature to the unstable style
due to an outstanding crash and proposed formatting tweaks (<a href=""https://redirect.github.com/psf/black/issues/4198"">#4198</a>)</li>
<li>Fixed a bug where base expressions caused inconsistent formatting of ** in tenary
expression (<a href=""https://redirect.github.com/psf/black/issues/4154"">#4154</a>)</li>
<li>Checking for newline before adding one on docstring that is almost at the line limit
(<a href=""https://redirect.github.com/psf/black/issues/4185"">#4185</a>)</li>
<li>Remove redundant parentheses in <code>case</code> statement <code>if</code> guards (<a href=""https://redirect.github.com/psf/black/issues/4214"">#4214</a>).</li>
</ul>
<h3>Configuration</h3>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/psf/black/blob/main/CHANGES.md"">black's changelog</a>.</em></p>
<blockquote>
<h2>24.3.0</h2>
<h3>Highlights</h3>
<p>This release is a milestone: it fixes Black's first CVE security vulnerability. If you
run Black on untrusted input, or if you habitually put thousands of leading tab
characters in your docstrings, you are strongly encouraged to upgrade immediately to fix
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-21503"">CVE-2024-21503</a>.</p>
<p>This release also fixes a bug in Black's AST safety check that allowed Black to make
incorrect changes to certain f-strings that are valid in Python 3.12 and higher.</p>
<h3>Stable style</h3>
<ul>
<li>Don't move comments along with delimiters, which could cause crashes (<a href=""https://redirect.github.com/psf/black/issues/4248"">#4248</a>)</li>
<li>Strengthen AST safety check to catch more unsafe changes to strings. Previous versions
of Black would incorrectly format the contents of certain unusual f-strings containing
nested strings with the same quote type. Now, Black will crash on such strings until
support for the new f-string syntax is implemented. (<a href=""https://redirect.github.com/psf/black/issues/4270"">#4270</a>)</li>
<li>Fix a bug where line-ranges exceeding the last code line would not work as expected
(<a href=""https://redirect.github.com/psf/black/issues/4273"">#4273</a>)</li>
</ul>
<h3>Performance</h3>
<ul>
<li>Fix catastrophic performance on docstrings that contain large numbers of leading tab
characters. This fixes
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-21503"">CVE-2024-21503</a>.
(<a href=""https://redirect.github.com/psf/black/issues/4278"">#4278</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Note what happens when <code>--check</code> is used with <code>--quiet</code> (<a href=""https://redirect.github.com/psf/black/issues/4236"">#4236</a>)</li>
</ul>
<h2>24.2.0</h2>
<h3>Stable style</h3>
<ul>
<li>Fixed a bug where comments where mistakenly removed along with redundant parentheses
(<a href=""https://redirect.github.com/psf/black/issues/4218"">#4218</a>)</li>
</ul>
<h3>Preview style</h3>
<ul>
<li>Move the <code>hug_parens_with_braces_and_square_brackets</code> feature to the unstable style
due to an outstanding crash and proposed formatting tweaks (<a href=""https://redirect.github.com/psf/black/issues/4198"">#4198</a>)</li>
<li>Fixed a bug where base expressions caused inconsistent formatting of ** in tenary
expression (<a href=""https://redirect.github.com/psf/black/issues/4154"">#4154</a>)</li>
<li>Checking for newline before adding one on docstring that is almost at the line limit
(<a href=""https://redirect.github.com/psf/black/issues/4185"">#4185</a>)</li>
<li>Remove redundant parentheses in <code>case</code> statement <code>if</code> guards (<a href=""https://redirect.github.com/psf/black/issues/4214"">#4214</a>).</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/psf/black/commit/552baf822992936134cbd31a38f69c8cfe7c0f05""><code>552baf8</code></a> Prepare release 24.3.0 (<a href=""https://redirect.github.com/psf/black/issues/4279"">#4279</a>)</li>
<li><a href=""https://github.com/psf/black/commit/f00093672628d212b8965a8993cee8bedf5fe9b8""><code>f000936</code></a> Fix catastrophic performance in lines_with_leading_tabs_expanded() (<a href=""https://redirect.github.com/psf/black/issues/4278"">#4278</a>)</li>
<li><a href=""https://github.com/psf/black/commit/7b5a657285f38126bf28483478bbd9ea928077ec""><code>7b5a657</code></a> Fix --line-ranges behavior when ranges are at EOF (<a href=""https://redirect.github.com/psf/black/issues/4273"">#4273</a>)</li>
<li><a href=""https://github.com/psf/black/commit/1abcffc81816257985678f08c61584ed4287f22a""><code>1abcffc</code></a> Use regex where we ignore case on windows (<a href=""https://redirect.github.com/psf/black/issues/4252"">#4252</a>)</li>
<li><a href=""https://github.com/psf/black/commit/719e67462c80574c81a96faa144886de6da84489""><code>719e674</code></a> Fix 4227: Improve documentation for --quiet --check (<a href=""https://redirect.github.com/psf/black/issues/4236"">#4236</a>)</li>
<li><a href=""https://github.com/psf/black/commit/e5510afc06cd238cd0cba4095283943a870a7e7b""><code>e5510af</code></a> update plugin url for Thonny (<a href=""https://redirect.github.com/psf/black/issues/4259"">#4259</a>)</li>
<li><a href=""https://github.com/psf/black/commit/6af7d1109693c4ad3af08ecbc34649c232b47a6d""><code>6af7d11</code></a> Fix AST safety check false negative (<a href=""https://redirect.github.com/psf/black/issues/4270"">#4270</a>)</li>
<li><a href=""https://github.com/psf/black/commit/f03ee113c9f3dfeb477f2d4247bfb7de2e5f465c""><code>f03ee11</code></a> Ensure <code>blib2to3.pygram</code> is initialized before use (<a href=""https://redirect.github.com/psf/black/issues/4224"">#4224</a>)</li>
<li><a href=""https://github.com/psf/black/commit/e4bfedbec2e8b10cc6b7b31442478f05db0ce06d""><code>e4bfedb</code></a> fix: Don't move comments while splitting delimiters (<a href=""https://redirect.github.com/psf/black/issues/4248"">#4248</a>)</li>
<li><a href=""https://github.com/psf/black/commit/d0287e1f7558d97e6c0ebd6dc5bcb5b970e2bf8c""><code>d0287e1</code></a> Make trailing comma logic more concise (<a href=""https://redirect.github.com/psf/black/issues/4202"">#4202</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/psf/black/compare/22.12.0...24.3.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=black&package-manager=pip&previous-version=22.12.0&new-version=24.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
170,[Feature/Addition] Compatibility for Python 3.12,https://api.github.com/repos/move-coop/parsons/issues/1021,Kathy Nguyen,1021,closed,2024-03-19 22:23:51+00:00,2024-06-27 20:53:10+00:00,2024-06-27 20:53:09+00:00,"## Detailed Description
Provide compatibility for Python 3.12, the latest minor release of Python. Currently, Parsons is compatibility up to and including Python 3.10. #1012 brings the compatibility up to and including 3.11, but has not been merged into the `main` branch or released yet.

## Possible Implementation
Update to Python 3.12 and test to see if anything breaks or is now incompatibile.

## Priority
Low priority","[Label(name=""enhancement""), Label(name=""blocked""), Label(name=""low priority""), Label(name=""futureproofing"")]",[],6
171,Fix failing linting test in #827,https://api.github.com/repos/move-coop/parsons/issues/1020,Wil T,1020,closed,2024-03-16 13:35:39+00:00,2024-04-04 20:33:15+00:00,2024-04-04 20:31:51+00:00,"This fixes workflow errors in #827 by correcting formatting by changing quotes to double and making a slight change in how a long line is wrapped.

I also had to cherry-pick d4e85ab6fc532c919269fd7a6fc8e34ebd1d9414 to get doc build to pass because the branch is outdated.",[],[],0
172,fix failing test in #976,https://api.github.com/repos/move-coop/parsons/issues/1019,Wil T,1019,closed,2024-03-16 13:31:18+00:00,2024-04-04 21:10:01+00:00,2024-04-04 20:35:10+00:00,"This fixes the following workflow error in #976:
`AssertionError: Expected 'copy_s3_to_gcs' to be called once. Called 0 times.` [[log]](https://github.com/move-coop/parsons/actions/runs/7618537381/job/20749902079?pr=976#step:6:185)

The test was checking for the method to have been called, but the previous commit changed the call to another method.

I also had to cherry-pick d4e85ab6fc532c919269fd7a6fc8e34ebd1d9414 to get doc build to pass because the branch is outdated.",[],[],0
173,GoogleBigQuery.query() fails if no select statement,https://api.github.com/repos/move-coop/parsons/issues/1018,Cormac Martinez del Rio,1018,closed,2024-03-15 16:41:51+00:00,2024-03-18 19:44:57+00:00,2024-03-15 17:01:35+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
Running a query like ""drop table;"" doesn't work

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
Running this block of code would have work when using Redshift as db:
```
            db.query(
                f""""""
                DROP TABLE IF EXISTS logs.{PREFIX}sync_log_{date}_old;
                CREATE TABLE logs.{PREFIX}sync_log_{date}_old AS
                    SELECT * FROM logs.{PREFIX}sync_log; 
                """"""
            )
```
But with BQ I get this traceback:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/cormac/Desktop/GitHub/parsons/parsons/google/google_bigquery.py"", line 275, in query
    return self.query_with_connection(
  File ""/Users/cormac/Desktop/GitHub/parsons/parsons/google/google_bigquery.py"", line 321, in query_with_connection
    final_table = self._fetch_query_results(cursor=cursor)
  File ""/Users/cormac/Desktop/GitHub/parsons/parsons/google/google_bigquery.py"", line 1258, in _fetch_query_results
    batch = cursor.fetchmany(QUERY_BATCH_SIZE)
  File ""/Users/cormac/Desktop/GitHub/canales/.venv/lib/python3.9/site-packages/google/cloud/bigquery/dbapi/_helpers.py"", line 494, in with_closed_check
    return method(self, *args, **kwargs)
  File ""/Users/cormac/Desktop/GitHub/canales/.venv/lib/python3.9/site-packages/google/cloud/bigquery/dbapi/cursor.py"", line 360, in fetchmany
    self._try_fetch(size=size)
  File ""/Users/cormac/Desktop/GitHub/canales/.venv/lib/python3.9/site-packages/google/cloud/bigquery/dbapi/cursor.py"", line 256, in _try_fetch
    rows_iterable = self._bqstorage_fetch(bqstorage_client)
  File ""/Users/cormac/Desktop/GitHub/canales/.venv/lib/python3.9/site-packages/google/cloud/bigquery/dbapi/cursor.py"", line 286, in _bqstorage_fetch
    table=table_reference.to_bqstorage(),
AttributeError: 'NoneType' object has no attribute 'to_bqstorage'
```
As a temporary workaround I just added a select statement to the end of the query:
```
            db.query(
                f""""""
                DROP TABLE IF EXISTS logs.{PREFIX}sync_log_{date}_old;
                CREATE TABLE logs.{PREFIX}sync_log_{date}_old AS
                    SELECT * FROM logs.{PREFIX}sync_log; 
                select 'WOW SO NEAT'
                """"""
            )
```


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
low priority - easy workaround ","[Label(name=""bug""), Label(name=""low priority"")]",[],3
174,"lint failing pull request ""add script"" (ngpvan_sample_printedlist.py)",https://api.github.com/repos/move-coop/parsons/issues/1017,Wil T,1017,closed,2024-03-14 14:58:16+00:00,2024-04-07 17:06:37+00:00,2024-04-04 20:36:39+00:00,"As discussed in #845, the  sample_ngpvan_printed_list branch is in need of basic linting fixes.

had to cherry-pick d4e85ab6fc532c919269fd7a6fc8e34ebd1d9414 to get doc build to pass because the branch is outdated.",[],[],0
175,Use UV for package install and dependency resolution,https://api.github.com/repos/move-coop/parsons/issues/1016,Wil T,1016,closed,2024-03-14 11:23:27+00:00,2024-09-05 20:21:02+00:00,2024-08-12 20:43:22+00:00,"## What

[uv](https://github.com/astral-sh/uv) is ""an extremely fast Python package installer and resolver, written in Rust.""

## Why

Your macOS testing is significantly limited by only testing with python 3.8. Based on the comments in your actions, it seems like this is because macOS actions are much more expensive (and unfortunately that workflow takes even longer to run on macOS than it does on Windows). If the macOS workflow was able to execute faster, you could run more of them without incurring as much additional usage or just save your usage generally.

Your current workflow step of installing dependencies results in [a notice](https://github.com/move-coop/parsons/actions/runs/8193050733/job/22405911682#step:4:430): `INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime.`

Both of these concerns can be addressed by using uv. Using the included linting workflow takes [far less time](https://github.com/bmos/parsons/actions/runs/8590795817/job/23538781367) per job compared to [the workflow you're using now](https://github.com/move-coop/parsons/actions/runs/8667690468/job/23771210312).

## Why Not

uv is very new and relatively unknown compared to pip and does have some behavior differences from pip (such as how it does not default to installing yanked versions -- which could be a positive but is a noteworthy difference).

uv doesn't support python 3.7, which PyPI says parsons does (although parsons tests don't pass under 3.7)",[],[],28
176,move build config to pyproject.toml,https://api.github.com/repos/move-coop/parsons/issues/1015,Wil T,1015,open,2024-03-14 03:39:28+00:00,2024-07-26 19:19:17+00:00,,"This is the new standard and it allows a lot of config to be done in this file, including configuration of dev tools like what I have suggested in #1013.

Doing this should also resolve the need for the LIMITED_DEPENDENCIES environment variable.
All packages have been grouped with an alias 'all' that installs everything but the dev requirements and tmc specific-packages.

I'm not sure about the Docker stuff though so I just made my best guess as to how to handle that.

It's also worth noting that LIMITED_DEPENDENCIES seems to have been implemented in such a way that most packages are not limited to a certain version, which means that you could end up with potentially quite different program behavior between LIMITED_DEPENDENCIES and normal install modes. This PR resolves this potential issue as all packages are declared per-module/in a single place.",[],[],1
177,Consolidate Actions to Single Workflow,https://api.github.com/repos/move-coop/parsons/issues/1014,Wil T,1014,closed,2024-03-14 02:44:48+00:00,2024-05-24 10:58:30+00:00,2024-04-07 17:12:17+00:00,"Instead of having two action workflows each with its own matrix, this has a single matrix for linux and adds some specific macOS tests via the include option.",[],[],0
178,"Move flake8, black, and pytest configs to pyproject.toml",https://api.github.com/repos/move-coop/parsons/issues/1013,Wil T,1013,closed,2024-03-14 02:22:07+00:00,2024-04-04 20:33:48+00:00,2024-04-04 20:30:06+00:00,"Consolidating tooling configs here is neater and helps avoid fragmented options (like how flake8 was set to 100 line length but black was set to 88).

This also updates these tools to their latest versions.

If you would rather use a different line length like 88 (the black default) or 79 (the flake8 default) or something else, this PR can be edited. I just used 100 since it was already configured in the flake8 config file.",[],[],0
179,Bugfixes,https://api.github.com/repos/move-coop/parsons/issues/1012,Wil T,1012,closed,2024-03-13 20:13:46+00:00,2024-04-04 20:34:25+00:00,2024-04-04 20:33:33+00:00,"In more in-depth refactoring I'm looking into, I have come across a number of changes needed to get tests passing, resolve test warnings, and obey lints. As requested in #1009, I have taken all the minor bugfixes and compiled them here.

[ddccd1e](https://github.com/move-coop/parsons/pull/1012/commits/ddccd1eb904375e39080f8ac7618364cf1318e1e) also fixes the only remaining issue holding back 3.11 support! So I added it to setup.py and the test suite to demonstrate that.",[],[],3
180,[Feature/Addition] OpenField as a Parsons Class,https://api.github.com/repos/move-coop/parsons/issues/1011,elyse-weiss,1011,open,2024-03-13 14:00:50+00:00,2024-04-29 05:26:28+00:00,,"https://openfield.ai/wp-content/uploads/2024/02/redoc-static.html

## Detailed Description
Conversations is probably a good enough place to start. 

## Context
I believe State Voices is using OpenField, and other organizations are starting to explore it. Would be good to have as part of Parsons!

## Possible Implementation



## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->","[Label(name=""enhancement"")]",[],4
181,Map 'Decimal' type from incomping Table in BQ,https://api.github.com/repos/move-coop/parsons/issues/1010,Jason,1010,closed,2024-03-13 13:01:24+00:00,2024-04-04 20:13:02+00:00,2024-04-04 20:13:02+00:00,This adds support for loading any data wrapped in the high-precision Python `Decimal` class directly into a BQ table.,[],[],0
182,GitHub Actions & Dev Tools Cleanup + Python 3.11 Support,https://api.github.com/repos/move-coop/parsons/issues/1009,Wil T,1009,closed,2024-03-12 20:43:19+00:00,2024-03-14 03:11:39+00:00,2024-03-14 01:56:47+00:00,"I'm not really sure how these changes could be merged nicely with other open PRs.
I'm happy to restructure it however is desired, so perhaps treat this as a sort of menu.

## What's Included

- Simplified the linting workflow to use a single file for both mac and linux testing.
- Use the rust-based package installer [uv](https://github.com/astral-sh/uv) to significantly reduce the GitHub actions runtime to allow a second python version in macOS testing.
- Functionalized `__init__.py` (which is kind of pointless but I did it while debugging an error I was getting) and fixed a typo in it (one of the tuples was a list instead).
- Fix import of HttpError in gmail.py (which for some reason your pytest workflow was not catching).
- Updated packages that were needed to get pytest to pass or were missing patch-level updates.
- Restructured limited dependencies mode to use the same dependency versions as the usual install.
- Resolve the TypeError raised when running pytest under Python 3.11. Required [upstream fix](https://github.com/petl-developers/petl/pull/656).
- Flake8 was set to maintain a 100 character max line length, but Black didn't have a line length option configured which meant it was defaulting to a much lower 88. These are now both 100.
- Add flake8, pytest, and black configurations to pyproject.toml rather than having a mix of separate config files and command-line args -- also standardized the commands used in all places.
- Updated Black and Flake8 to latest versions and applied their updated linting suggestions.
- Pytest now finishes with 3 warnings instead of 13.

## Notes / Considerations

- Removed support for python 3.7 as it is not supported with uv. This could be added back as a supported version along with its own test workflow or no test workflow, if preferred. I know dropping 3.7 was recently discussed in #932.
- Closes #1007 by using a version of braintree that has not been yanked",[],[],6
183,Remove self.project reference from GoogleBigQuery.get_columns() query,https://api.github.com/repos/move-coop/parsons/issues/1008,Erica Faulkenberry,1008,closed,2024-03-06 16:30:15+00:00,2024-03-07 18:28:44+00:00,2024-03-07 18:28:44+00:00,"The `GoogleBigQuery.get_columns()` base query `from` statement references `{self.project}` which causes an error (below). Removing `{self.project`} solves this issue and aligns with the format used in other queries in this class, such as `get_columns_list`.

```
raise exceptions.DatabaseError(exc)
03/06/2024 10:11:26 AM
google.cloud.bigquery.dbapi.exceptions.DatabaseError: 400 Invalid project ID 'None'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.
```

","[Label(name=""high priority""), Label(name=""bug fix"")]","[NamedUser(login=""coastlines"")]",0
184,[Bug] Parsons 3.1.0 targets yanked 4.0.0 version of braintree with critical bugs,https://api.github.com/repos/move-coop/parsons/issues/1007,Wil T,1007,closed,2024-03-06 14:16:13+00:00,2024-05-23 19:36:13+00:00,2024-05-23 19:36:12+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
Parsons 3.1.0 depends on braintree 4.0.0 which has been yanked due to critical bugs.

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
When I install parsons 3.1.0 via pip, I get this message:
> WARNING: The candidate selected for download or install is a yanked version: 'braintree' candidate (version 4.0.0 at https://files.pythonhosted.org/packages/a5/1e/641306eec960161028e0a30d2f635abdfbdb89029330df1701bc106e5d8c/braintree-4.0.0-py2.py3-none-any.whl (from https://pypi.org/simple/braintree/))
Reason for being yanked: critical bugs

When I install it via [uv](https://github.com/astral-sh/uv) it doesn't even install for the same reason.

## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->
Create a new virtual environment and run `pip install parsons==3.1.0`

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know): **v3.1.0**
* Environment name and version (e.g. Chrome 39, node.js 5.4): 
* Operating System and version (desktop or mobile): **Pop!_OS 22.04 LTS**


## Additional Context
<!--- Add any other context about the problem here -->
In a fork I'm playing around with, braintree==4.26.0 and google-cloud-bigquery==3.18.0 were needed to get pytests passing. I'm not sure why that isn't the case on this repo but it seems like these versions might be safe to update.

## Priority
<!--- Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by -->
For me it's low because I don't use braintree but ""critical bugs"" makes it sound rather serious for people who use that integration.","[Label(name=""bug"")]",[],1
185,Bump version number for minor release,https://api.github.com/repos/move-coop/parsons/issues/1006,Shauna Gordon-McKeon,1006,closed,2024-03-05 20:52:39+00:00,2024-03-05 21:01:03+00:00,2024-03-05 21:01:03+00:00,,[],[],0
186,[Pipelines] Design - Data orchestration input,https://api.github.com/repos/move-coop/parsons/issues/1005,Jason,1005,open,2024-02-27 21:40:54+00:00,2024-02-29 03:35:36+00:00,,"### Overview

The Pipelines project targets two user groups. One of them are advanced users who are already fluent in Python. One of the main value-add features of pipelines to advanced users is easy data orchestration integration. Data orchestration gives many benefits, such as error logging, data visibility, etc. It is a key goal of the pipelines system that you get drop-in data orchestration of your pipelines ""for free.""

Currently (2/27/2024) the pipelines branch has hard-coded Prefect integration. This is a good proof of concept, as the Prefect integration is entirely behind the scenes. However, because Prefect is closed source and cloud based, it's not acceptable to lock pipelines into that tool.

### Discussion

The initial goal of this discussion is to gather input from the community about:

1. What data orchestration tools you use
2. How you use those data orchestration tools
3. How your code interacts with those data orchestration tools.

Once we have collected data about a wide variety of tools, we will design an abstraction that allows the pipelines system to work with as many data orchestration tools as possible. Then, data orchestration ""plugins"" that target the abstraction can be added either inside or outside of Parsons, allowing pipelines to be used with any data orchestration platform.

Without a thorough discussion of different data orchestration use cases, we risk designing an abstraction that cannot accommodate many of the tools that pipelines users will want to target in their code.","[Label(name=""Pipelines Project"")]",[],2
187,[Pipelines] Design - Pipelines language should be accessible to new users,https://api.github.com/repos/move-coop/parsons/issues/1004,Jason,1004,open,2024-02-27 21:27:02+00:00,2024-02-27 21:45:50+00:00,,"The Pipelines project targets two user groups. One of them are new users to Python, who might be familiar with basic SQL but don't know programming. We'll call them _new users_ It's critical to the mission of Pipelines that it's accessible to new users.

New users should be able to easily:
- Use pipelines to perform ETL tasks
- Take advantage of advanced pipelines features like data orchestration (Prefect integration as an example) ""out of the box""

New users do **not** need to be able to perform more advanced tasks like:
- Writing new pipes
- Extending the pipelines framework with new functionality, like adding data orchestration plugins.

To focus the conversation, below is a copy of the current example script in the `pipelines` branch (as of 2/27/2024). I believe it captures the surface area of functionality new users will be expected to engage with.

```python
    clean_year = CompoundPipe(
        filter_rows(""{Year} is not None""),
        convert(""Year"", int)
    )

    load_after_1975 = Pipeline(
        ""Load after 1975"",
        load_from_csv(""deniro.csv""),
        clean_year(),
        filter_rows(""{Year} > 1975""),
        write_csv(""after_1975.csv"")
    )
    split_on_1980 = Pipeline(
        ""Split on 1980"",
        load_from_csv(""deniro.csv""),
        clean_year(),
        split_data(""'gte_1980' if {Year} >= 1980 else 'lt_1980'""),
        for_streams({
            ""lt_1980"": write_csv(""before_1980.csv""),
            ""gte_1980"": write_csv(""after_1979.csv"")
        })
    )

    save_lotr_books = Pipeline(
        ""Save LOTR Books"",
        load_lotr_books_from_api(),
        write_csv(""lotr_books.csv"")
    )

    after_1990_and_all_time = Pipeline(
        ""Copy into streams test"",
        load_from_csv(""deniro.csv""),
        clean_year(),
        copy_data_into_streams(""0"", ""1""),
        for_streams({
            ""0"": CompoundPipe(
                filter_rows(""{Year} > 1990""),
                write_csv(""after_1990.csv"")
            )(),
            ""1"": write_csv(""all_years.csv"")
        })
    )

    dashboard = Dashboard(
        load_after_1975,
        split_on_1980,
        save_lotr_books,
        after_1990_and_all_time,
    )
    dashboard.run()
```","[Label(name=""Pipelines Project"")]",[],0
188,EveryAction: Add email endpoint methods to retrieve email stats from TargetedEmail,https://api.github.com/repos/move-coop/parsons/issues/1003,Matthew Krausse,1003,closed,2024-02-25 03:19:46+00:00,2024-05-14 20:33:23+00:00,2024-05-14 20:33:23+00:00,"This pull request adds an email endpoint to the VANConnector class, allowing users to retrieve email messages and their stats. It also includes test cases for getting email messages and email stats.",[],"[NamedUser(login=""shaunagm"")]",2
189,Add email stats functionality to VAN connector,https://api.github.com/repos/move-coop/parsons/issues/1002,Matthew Krausse,1002,closed,2024-02-24 21:41:52+00:00,2024-02-24 22:00:27+00:00,2024-02-24 22:00:27+00:00,"This pull request adds email functionality to the VAN connector, allowing users to retrieve emails and email statistics from the VAN API. It includes the addition of the `Email` class and relevant methods, as well as tests and documentation for the new functionality.",[],[],0
190,Add head and tail methods to Parsons Table,https://api.github.com/repos/move-coop/parsons/issues/1001,Matthew Krausse,1001,closed,2024-02-24 03:20:27+00:00,2024-02-29 22:49:22+00:00,2024-02-29 22:49:22+00:00,"This pull request adds the `head` and `tail` methods to the `Parsons Table` class in the `ETL` module. The `head` method returns the first n rows of the table, while the `tail` method returns the last n rows of the table. Tests and documentation have also been added for these new methods.",[],[],1
191,"BigQuery connector handles queries without return values (create table, create view)",https://api.github.com/repos/move-coop/parsons/issues/1000,Austin Weisgrau,1000,closed,2024-02-21 19:24:54+00:00,2024-02-21 21:29:40+00:00,2024-02-21 21:29:35+00:00,"Addresses #999

The BigQuery connector can now successfully run a query to create a table or a view without raising an AttributeError when unsuccessfully trying to fetch results from the query.

Normally you can use the return_value=False flag, but that shouldn't be necessary",[],[],0
192,[Bug] BigQuery connector raises AttributeError on successful query to create view or table,https://api.github.com/repos/move-coop/parsons/issues/999,Austin Weisgrau,999,closed,2024-02-21 19:21:40+00:00,2024-02-23 02:16:15+00:00,2024-02-23 02:16:15+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
When a successful query to create a view runs, the connector fails on this exception:
```
AttributeError: 'NoneType' object has no attribute to_bqstorage
```
Implying somewhere in the stack, nothing is returned while something is expected.


## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->
BigQuery().query(""create view as select * from some_other_table"")


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.","[Label(name=""bug"")]","[NamedUser(login=""austinweisgrau"")]",1
193,Add cautionary note about performance in get_row_count method,https://api.github.com/repos/move-coop/parsons/issues/998,Matthew Krausse,998,closed,2024-02-21 00:55:24+00:00,2024-05-23 18:08:06+00:00,2024-05-23 18:08:06+00:00,"This pull request adds a cautionary note about the performance of the get_row_count method in the BigQuery materialization. The note warns that using SELECT COUNT(*) can be expensive for large tables, especially those with many columns, as BigQuery scans all table data to perform the count. The note aims to inform developers about the potential performance impact and encourage them to consider alternative approaches when dealing with large tables.",[],[],6
194,Raise Error for Big Query .copy() when no GCS_TEMP_BUCKET environment variable or tmp_gcs_bucket parameter,https://api.github.com/repos/move-coop/parsons/issues/997,Tracy Urquhart,997,closed,2024-02-20 23:16:10+00:00,2024-02-27 21:02:27+00:00,2024-02-27 21:02:27+00:00,Raise Error for Big Query .copy() when no GCS_TEMP_BUCKET environment variable is set or tmp_gcs_bucket parameter is passed,"[Label(name=""🎉 first PR"")]",[],0
195,Add config to setup.py so README appears on PyPI,https://api.github.com/repos/move-coop/parsons/issues/996,Shauna Gordon-McKeon,996,closed,2024-02-20 22:20:39+00:00,2024-02-22 16:05:28+00:00,2024-02-22 16:05:28+00:00,Addresses #944 ,[],[],0
196,[Bug] GoogleBigQuery get_row_count() method is inefficient and should be changed,https://api.github.com/repos/move-coop/parsons/issues/995,Matthew Krausse,995,closed,2024-02-20 05:57:41+00:00,2024-07-22 01:17:59+00:00,2024-07-22 01:17:59+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->

The current code runs this sql to get the row data. 

sql = f""SELECT COUNT(*) AS row_count FROM `{schema}.{table_name}`""

But we should change it to this:

SELECT table_name, row_count 
FROM project.dataset.INFORMATION_SCHEMA.TABLES 
WHERE table_name = 'your_table_name'


## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->

SELECT COUNT(*):

- Scans all rows in the table to count them individually.
- Reads the entire data size of the table.
- Can be computationally expensive, especially for large tables.

INFORMATION_SCHEMA.TABLES:

- Retrieves metadata about the table from the INFORMATION_SCHEMA.TABLES view.
- This view stores pre-aggregated information about tables, including row count.
- Only reads a small amount of data from the INFORMATION_SCHEMA.TABLES view.
- Is significantly faster and cheaper than scanning the entire table.

Therefore, by using the INFORMATION_SCHEMA.TABLES method, you only need to read a small amount of metadata instead of the entire table data, resulting in less data usage and improved performance.


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.

I believe this should be high priority to change as it could be costly for people using this method. I opened an issue mostly for discussion on this issue before writing the PR. ","[Label(name=""bug"")]",[],4
197,992 bug parsons bigquery upsert is broken,https://api.github.com/repos/move-coop/parsons/issues/994,,994,closed,2024-02-19 22:55:51+00:00,2024-02-27 18:04:07+00:00,2024-02-27 18:04:02+00:00,"Fixes the upsert bug by adding ""template_table"" back into the API for the three copy methods (copy, copy_s3, copy_from_gcs). 

Moved the schema determination logic into a single private function and moved some error handling into a lower level wrapper of the BQ client's `load_table_from_uri`. Also, did some basic tidying in a few functions.

These are some of the most central methods in the class. Obviously the unit tests are passing, your test script, and I got halfway through our Catalist load locally. But if people can do some meaningful kicking the tires with integration tests in your projects, I think that would be a good use of time.",[],"[NamedUser(login=""willyraedy"")]",0
198,[WIP] Upsert hotfix and load schema refactor,https://api.github.com/repos/move-coop/parsons/issues/993,,993,closed,2024-02-19 21:45:07+00:00,2024-02-19 22:56:25+00:00,2024-02-19 22:56:25+00:00,,[],"[NamedUser(login=""willyraedy"")]",1
199,[Bug] Parsons' BigQuery Upsert is Broken,https://api.github.com/repos/move-coop/parsons/issues/992,Jason,992,closed,2024-02-16 19:45:15+00:00,2024-02-27 18:04:03+00:00,2024-02-27 18:04:03+00:00,"The `upsert` functionality in the Parsons BigQuery connector seems to be broken.


## Detailed Description
```
Exception has occurred: TypeError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)
Client.load_table_from_uri() got an unexpected keyword argument 'template_table'
  File ""C:\...\parsons\google\google_bigquery.py"", line 461, in copy_from_gcs
    load_job = self.client.load_table_from_uri(
  File ""C:\...\parsons\google\google_bigquery.py"", line 818, in copy
    self.copy_from_gcs(
  File ""C:\...\parsons\google\google_bigquery.py"", line 949, in upsert
    self.copy(
  File ""C:\...\upsert_test.py"", line 29, in main
    bq.upsert(data2, table_name, ""a"")  # This errors
  File ""C:\...\upsert_test.py"", line 35, in <module>
    main()
  File ""C:\Python310\Lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""C:\Python310\Lib\runpy.py"", line 196, in _run_module_as_main (Current frame)
    return _run_code(code, main_globals, None,
TypeError: Client.load_table_from_uri() got an unexpected keyword argument 'template_table'
```

The error seems to be that the Google Python BigQuery client's method [load_table_from_uri](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_load_table_from_uri) does not take a `template_table` argument. But that is what's passed into the `parsons.BigQuery.copy()` method [here](https://github.com/move-coop/parsons/blob/main/parsons/google/google_bigquery.py#L956).

## To Reproduce
1. Make sure you have a `.env` file with a JSON service account credential set up under the `GOOGLE_APPLICATION_CREDENTIALS` environmental variable. I also set up a GCS_TEMP_BUCKET env variable as well.
2. Create a `test` dataset in your project (or change the dataset in the script below).
3. Run this script:

```python
import dotenv
from parsons import Table
from parsons.google.google_bigquery import GoogleBigQuery

dotenv.load_dotenv(override=True)

def main():
    bq = GoogleBigQuery()
    table_name = ""test.upsert_test_data""

    data = Table([{""a"": 1, ""b"": 2}, {""a"": 3, ""b"": 4}])

    bq.copy(data, table_name, if_exists=""drop"")

    data2 = Table([{""a"": 1, ""b"": 20}, {""a"": 5, ""b"": 6}])

    bq.upsert(data2, table_name, ""a"")  # This errors

    print(bq.query(f""SELECT * FROM {table_name}""))


if __name__ == ""__main__"":
    main()
```


## Your Environment
* Tested with the PyPi 3.0.0 release and Parsons `main` branch as of 2/16.
* Windows 10


## Priority
This is high priority for me, since we're migrating to BigQuery with TMC and have several scripts that rely on upsert functionality.","[Label(name=""bug"")]","[NamedUser(login=""willyraedy"")]",0
200,Add Formstack connector,https://api.github.com/repos/move-coop/parsons/issues/991,Jason,991,closed,2024-02-14 17:18:53+00:00,2024-02-27 21:01:24+00:00,2024-02-27 21:01:24+00:00,,[],[],0
201,Bump grpcio from 1.53.0 to 1.53.2,https://api.github.com/repos/move-coop/parsons/issues/990,,990,closed,2024-02-11 05:23:25+00:00,2024-02-20 22:27:53+00:00,2024-02-20 22:27:44+00:00,"Bumps [grpcio](https://github.com/grpc/grpc) from 1.53.0 to 1.53.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/grpc/grpc/releases"">grpcio's releases</a>.</em></p>
<blockquote>
<h2>Release v1.53.2</h2>
<p>This is release gRPC Core 1.53.2 (glockenspiel).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes.</p>
<h2>Core</h2>
<ul>
<li>[backport][iomgr][EventEngine] Improve server handling of file descriptor exhaustion by <a href=""https://github.com/drfloob""><code>@​drfloob</code></a> in <a href=""https://redirect.github.com/grpc/grpc/pull/33672"">grpc/grpc#33672</a></li>
</ul>
<h2>Release v1.53.1</h2>
<p>This is release gRPC Core 1.53.1 (glockenspiel).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes.</p>
<ul>
<li>Fixed <a href=""https://www.cve.org/cverecord?id=CVE-2023-32731"">CVE-2023-32731</a></li>
<li>Fixed <a href=""https://www.cve.org/cverecord?id=CVE-2023-32732"">CVE-2023-32732</a></li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/grpc/grpc/commit/afb307fb89ed83f358d82b5d359034a039a95e66""><code>afb307f</code></a> [v1.53.x][Interop] Backport Python image update (<a href=""https://redirect.github.com/grpc/grpc/issues/33864"">#33864</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/7a9373b049fc38c2c6f0ed07ab97908b380db967""><code>7a9373b</code></a> [Backport] [dependency] Restrict cython to less than 3.X (<a href=""https://redirect.github.com/grpc/grpc/issues/33770"">#33770</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/fdb64a69ec4c5a142aa7acbde4ba0d472c94ec19""><code>fdb64a6</code></a> [v1.53][Build] Update Phusion baseimage (<a href=""https://redirect.github.com/grpc/grpc/issues/33767"">#33767</a>) (<a href=""https://redirect.github.com/grpc/grpc/issues/33836"">#33836</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/cdf4186a176c293eca5de9aaac28706b6f5edd70""><code>cdf4186</code></a> [PSM Interop] Legacy tests: fix xDS test client build (v1.53.x backport) (<a href=""https://redirect.github.com/grpc/grpc/issues/33"">#33</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/ce5b93a3c69fcd60879a53272e2593366efa6065""><code>ce5b93a</code></a> [PSM Interop] Legacy test builds always pull the driver from master (v1.53.x ...</li>
<li><a href=""https://github.com/grpc/grpc/commit/b24b6eac9aa86773cc1d7b45b4c8efcaf3417fc8""><code>b24b6ea</code></a> [release] Bump release version to 1.53.2 (<a href=""https://redirect.github.com/grpc/grpc/issues/33709"">#33709</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/1e86ca5834b94cae7d5e6d219056c0fc895cf95d""><code>1e86ca5</code></a> [backport][iomgr][EventEngine] Improve server handling of file descriptor exh...</li>
<li><a href=""https://github.com/grpc/grpc/commit/aff3066cea9d0386521668983a610e9a1844d384""><code>aff3066</code></a> [PSM interop] Don't fail url_map target if sub-target already failed (v1.53.x...</li>
<li><a href=""https://github.com/grpc/grpc/commit/539d75cc57c922a2706cc114b8527d851f07f366""><code>539d75c</code></a> [PSM interop] Don't fail target if sub-target already failed (<a href=""https://redirect.github.com/grpc/grpc/issues/33222"">#33222</a>) (v1.53....</li>
<li><a href=""https://github.com/grpc/grpc/commit/3e79c88157c47ff04e1363be2f0452d89fc1b89c""><code>3e79c88</code></a> [Release] Bump version to 1.53.1 (on v1.53.x branch) (<a href=""https://redirect.github.com/grpc/grpc/issues/33047"">#33047</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/grpc/grpc/compare/v1.53.0...v1.53.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=grpcio&package-manager=pip&previous-version=1.53.0&new-version=1.53.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
202,[Feature/Addition] Parsons table head,https://api.github.com/repos/move-coop/parsons/issues/989,elyse-weiss,989,closed,2024-02-08 19:03:51+00:00,2024-03-11 18:21:40+00:00,2024-03-11 18:21:40+00:00,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
I would love for petl's head function to be built into Parsons. This allows you to just get the first X number of rows from a table. Sometimes you want to test code on a smaller subset of data, or something like that, and this is very helpful. 

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->


## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->


## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->


## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->","[Label(name=""enhancement""), Label(name=""good first issue"")]",[],2
203,[Bug] NGPVAN download_saved_list() only returning partial results,https://api.github.com/repos/move-coop/parsons/issues/988,Deleted user,988,closed,2024-02-08 18:04:40+00:00,2024-03-27 14:08:46+00:00,2024-03-27 14:08:45+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
When retrieving a saved list from van using the download_saved_list(), only a partial list of VanIDs is returned. 

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->

We have a saved list with 25 records in VAN. When using the Parsons download_saved_list() function and returning to a dataframe or csv, not all vanIDs are returned. Only 18 out of the 25 records are being returned. 

## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->

This is the query used when the bug occurs. 

```
### API KEY SETUP

### IMPORT PACKAEGS
import pandas as pd

#read api key file and assign variables
df = pd.read_csv('/key.csv')
parsons_api_key = df.loc[df['API'] == 'parsons']['KEY'].iloc[0]   

from parsons import VAN
van = VAN(api_key= parsons_api_key, db='MyVoters') 

##### CONFIG

config_vars = {
    # VAN
    ""VAN_API_KEY"": parsons_api_key,
    ""VAN_DB_NAME"": ""MyVoters"",  
    ""VAN_FOLDER_ID"": ""1234"", 
    ""VAN_SAVED_LIST_ID"": "" 9876954""
}

#### CODE

import os  
from parsons import VAN  
from parsons import logger  

download_saved_list = van.download_saved_list(""1073953"").to_dataframe().sort_values(by=['VanID'])

saved_list_df = pd.DataFrame.from_dict(download_saved_list)
 
saved_list_df
```

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know): v3.0.0
* Environment name and version (e.g. Chrome 39, node.js 5.4): Ubuntu, VS Code
* Operating System and version (desktop or mobile): Chrome


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.

Medium","[Label(name=""bug"")]",[],5
204,DRAFT - Migrate from psycopg2 to psycopg3,https://api.github.com/repos/move-coop/parsons/issues/987,Austin Weisgrau,987,open,2024-02-06 19:07:34+00:00,2024-07-23 21:11:27+00:00,,"psycopg3 is mostly built as a drop-in upgrade from psycopg2.
Differences are detailed here: https://www.psycopg.org/psycopg3/docs/basic/from_pg2.html

A few highlights:

* Cancelling running queries

One major DX improvement is that Ctrl-C can now cancel a running
query. With psycopg2, the python process can be killed but a running
query usually could not be killed (without knowing the PID and running
`pg_cancel_backend(pid)` from a separate connection), to the immediate
regret of all we who sometimes accidentally execute an unfortunately
large query.

See https://www.psycopg.org/psycopg3/docs/advanced/async.html#interrupting-async-operations

* Breaking changes to parameterization

A few parameterization patterns that worked with psycopg2 no longer
work with psycopg3. They are detailed on the migration guide linked
above, but these in particular should be highlighted:

** You can no longer use `IN %s` with a tuple

This would work with psycopg2 but will not work with psycopg3:
`conn.execute(""SELECT * FROM foo WHERE id IN %s"", parameters=[(10,20,30)])`

Instead, this format is advised:
`conn.execute(""SELECT * FROM foo WHERE id = ANY(%s)"", parameters=[[10,20,30]])`

** You can no longer use `IS %s`
This would work with psycopg2 but will not work with psycopg3:
`conn.execute(""SELECT * FROM foo WHERE field IS %s"", [None])`

Instead, `IS NOT DISTINCT FROM %s` should be used
`conn.execute(""SELECT * FROM foo WHERE field IS NOT DISTINCT FROM %s"", [None])`

* Breaking changes to psycopg2 `copy` methods

`copy` methods are refactored and consolidated in psycopg3. See the
migration guide for details.",[],[],3
205,Simpler use of pip cache in actions,https://api.github.com/repos/move-coop/parsons/issues/986,Wil T,986,closed,2024-02-06 12:53:22+00:00,2024-02-27 19:31:19+00:00,2024-02-27 19:31:19+00:00,"GitHub recommends using this approach:
https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows
https://github.com/actions/setup-python#caching-packages-dependencies

That being said, what you have is clearly working so this is not an important change.",[],[],1
206,Log progress when putting file through SFTP,https://api.github.com/repos/move-coop/parsons/issues/985,Austin Weisgrau,985,closed,2024-02-05 23:54:47+00:00,2024-02-15 00:57:43+00:00,2024-02-15 00:57:40+00:00,"It can be helpful to have progress updates when putting a file through SFTP, to know how the put is going, or if the upload is hanging, etc.",[],[],0
207,Allow Table(None),https://api.github.com/repos/move-coop/parsons/issues/984,Austin Weisgrau,984,open,2024-02-05 22:20:28+00:00,2024-08-22 22:38:06+00:00,,"This change allows `Table(None)` to work, equivalent to `Table()`. This is reasonable behavior and also simplifies the implementation to avoid the necessity of a sentinal as a default argument. 

This may be a breaking change if code depends on `Table(None)` raising a ValueError.",[],[],7
208,Refactor Table initialization to avoid iterating over every row,https://api.github.com/repos/move-coop/parsons/issues/983,Austin Weisgrau,983,closed,2024-02-05 21:49:09+00:00,2024-02-27 21:03:29+00:00,2024-02-27 21:03:28+00:00,"The prior implementation for initializing a Parsons table involved a check of the truthiness of a Parsons Table's underlying petl Table, which involves iterating over every row of the table. For very long tables, this can take a very long time. One important reason for the of the petl library is to enable very long tables to be worked with quickly and easily in python, so the existing implementation is a problem for that.

These changes refactor how a parsons Table is initialized slightly to avoid this iteration and more robustly and performatively set & check for the validity of the underlying petl Table.

The result involves one bit of esoteric code (the use of a sentinal as a default argument) in order to allow existing behavior to remain the same, specifically that `Table()` should work but `Table(None)` should raise an Error. I think that `Table(None)` should work equivalently to `Table()` and would simplify the implementation somewhat, but that would be a breaking change so I will make that in a separate PR to the major-release branch.

See commit messages for additional details about implementation changes and reasoning.",[],[],2
209,Remove doubled up imports to fix linting,https://api.github.com/repos/move-coop/parsons/issues/982,Austin Weisgrau,982,closed,2024-01-31 18:50:32+00:00,2024-02-01 01:05:25+00:00,2024-01-31 22:20:27+00:00,,[],[],0
210,Issue 910 - Added timeout to BigQuery load_job.result(),https://api.github.com/repos/move-coop/parsons/issues/981,Bear Jordan,981,closed,2024-01-27 06:26:17+00:00,2024-05-22 05:16:04+00:00,2024-05-22 05:16:04+00:00,"# Overview

This PR addresses [Issue #910](https://github.com/move-coop/parsons/issues/910). A timeout is added to prevent indefinite hanging.


From the original issue:

> Per [Google BQ query job documentation](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.job.QueryJob#google_cloud_bigquery_job_QueryJob_result), it's possible to pass a default timeout parameter to the synchronous call that awaits a response.

> This change would require choosing a reasonable maximum default (30s?), setting that up as a configurable parameter, and passing that parameter into load_job.result.

# Changes
27/1/2024
- ~~Added `MAX_TIMEOUT = 30` constant~~ 
- ~~Updated `load_job.result()` to use `MAX_TIMEOUT`~~

31/1/2024 — @Jason94 
- Added `max_timeout: int = 30` as a keyword argument
- Updated `load_job.result()` to use `max_timeout`
- Added `DeadlineExceeded` error handling ([docs](https://googleapis.dev/python/google-api-core/latest/exceptions.html?highlight=deadlineexceeded#google.api_core.exceptions.DeadlineExceeded))","[Label(name=""🎉 first PR"")]",[],6
211,DRAFT PROPOSAL: Add Pipelines Framework to Parsons,https://api.github.com/repos/move-coop/parsons/issues/980,Jason,980,open,2024-01-26 08:16:41+00:00,2024-02-22 18:38:53+00:00,,"### Overview
This PR implements a draft of the _pipelines_ API that we discussed at the January contributors meeting. The goal of the pipelines API is to add an extra level of abstraction on top of the existing Parsons' connectors and Table to make it easier to write and manage ETL pipelines.

### Justification

The contributors recently had a discussion about what we can do to take Parsons to the next level to make it even easier to build ETL scripts. Another big goal that would benefit TMC and the whole space significantly is to make it easier for new data staff who don't have a handle on control flow and data structures to assemble ETL scripts.

In my opinion, Parsons connectors already make it very easy to extract and load data. The two things that it does not do much to help the user with are:

- The transform step. We offer very little abstraction over the PETL data structure. Transformation code is still relatively low level compared to the kind of no-code solutions that are getting popular. You still need to be proficient with functions, control flow, basic Python data structures like dictionaries and arrays, etc to be able to write ETL scripts with a non-trivial transform step.
- Parsons doesn't do anything to manage your pipelines for you. Data orchestration shouldn't be a goal of Parsons, IMO, but I think we could do a lot more than we do. Tasks like logging, error handling, and pipeline visibility are all repetitive tasks that everyone writing ETL scripts needs to manage somehow. Parsons could help with this.

During the call we discussed adding a Pipelines system to Parsons. This system would exist on top of the connectors and Parsons Table, and possibly would hide those details from a new user completely. The idea is this:

- The community defines a discrete set of extract, transform, and load Pipes that perform higher-level operations on data than the current Table functions. This could be anything from loading a data from a VAN endpoint, to performing a complex transformation, to transforming a data model from one particular source into an intermediate representation.
- These pipes can be combined into a Pipeline, which is a composable data structure that knows how to run pipes to perform a complete ETL job. Combined, pipes and pipelines also provide features for validation, logging strategies, error handling strategies, etc.
- A Runner which is capable of running pipelines in parallel to make use of CPU cores or cloud scaling capabilities.
- A Dashboard which can actually give the user some visibility into their pipelines that are running.
---
## Update - Revision 2

Based on contributor feedback we identified two distinct user groups with different sets of issues on the original proposal.

### New Engineers/Analysts

There was concern that the syntax was still too complex for analysts/engineers (_new users_) and would prove too big a barrier for them to use the framework successfully. To accommodate those concerns, the following changes were made in revision 2:

#### Removal of `lambda` syntax.

The original syntax used lambda functions, a difficult concept, heavily.

- Lambda functions were replaced with the much simpler PETL expression strings:
```python
        filter_rows(""{Year} > 1975""),
```

- A `CompoundPipe` constructor was introduced to replace the use of `lambda` to build up a more complex pipe from more basic pipes:
```python
    clean_year = CompoundPipe(
        filter_rows(""{Year} is not None""),
        convert(""Year"", int)
    )
```

#### Simplification of Pipeline syntax

The previous pipe-chain syntax in the Pipeline constructor was a repeated function call, which would have been confusing to newer users:

```python
    load_after_1975 = Pipeline(
        load_from_csv(""deniro.csv"")
        (
            print_data(""Raw Data"")
        )(
            clean_year()
        )(
            filter_rows({
                ""Year"": lambda year: year > 1975
            })
        )(
            print_data(""After 1975"")
        )(
            write_csv(""after_1975.csv"")
        )
    )
```

This has been replaced with a much simpler syntax where the pipes are just listed in the `Pipeline` constructor:
```python
    load_after_1975 = Pipeline(
        ""Load after 1975"",
        load_from_csv(""deniro.csv""),
        clean_year(),
        filter_rows(""{Year} > 1975""),
        write_csv(""after_1975.csv"")
    )
```

### Power Users

The main concern for experienced engineers/analysts with a high degree of Python skill (_power users_) was that the framework didn't offer them enough value for the cost of construction. In particular, data orchestration was wanting here.

>[My organization] started to build an orchestration platform to meet the needs of semi-technical members and resolve some of the civis pain points. Long story short, maintaining software is a lot of work and we're hoping to deprecate that eventually...
> My opinion is that there are really good, well funded, and active open source projects to do those things and nothing super specific about our use case vs. private enterprise

#### Prefect integration

To meet that need, this revision of Pipelines has incorporated Prefect under the hood with **no additional complexity** to _anyone_ using the pipeline framework, be they writing pipes or just assembling pipelines.

Each pipe is defined exactly the same as before, but the framework constructs a Prefect task for that pipe behind the scenes.
```python
# This pipe is transformed into a Prefect pipe named ""write_csv"", but is written like normal Parsons code.

@define_pipe(""write_csv"")
def write_csv(data: Table, csv_name: str) -> Table:
    data.to_csv(csv_name)
    return data
```

Each Pipeline is transformed into a Prefect Flow, and is logged in the Prefect Cloud when run:
```python
    load_after_1975 = Pipeline(
        ""Load after 1975"",
        load_from_csv(""deniro.csv""),
        clean_year(),
        filter_rows(""{Year} > 1975""),
        write_csv(""after_1975.csv"")
    )
```

In addition to providing modern data orchestration out of the box, the Prefect integration will make it possible for us to integrate the extensive Prefect Integrations library with Pipelines.

### Revision 2 Demo

This code sets up and runs the same series of pipelines as before:
```python
    clean_year = CompoundPipe(
        filter_rows(""{Year} is not None""),
        convert(""Year"", int)
    )

    load_after_1975 = Pipeline(
        ""Load after 1975"",
        load_from_csv(""deniro.csv""),
        clean_year(),
        filter_rows(""{Year} > 1975""),
        write_csv(""after_1975.csv"")
    )
    split_on_1980 = Pipeline(
        ""Split on 1980"",
        load_from_csv(""deniro.csv""),
        clean_year(),
        split_data(""'gte_1980' if {Year} >= 1980 else 'lt_1980'""),
        for_streams({
            ""lt_1980"": write_csv(""before_1980.csv""),
            ""gte_1980"": write_csv(""after_1979.csv"")
        })
    )

    save_lotr_books = Pipeline(
        ""Save LOTR Books"",
        load_lotr_books_from_api(),
        write_csv(""lotr_books.csv"")
    )

    after_1990_and_all_time = Pipeline(
        ""Copy into streams test"",
        load_from_csv(""deniro.csv""),
        clean_year(),
        copy_data_into_streams(""0"", ""1""),
        for_streams({
            ""0"": CompoundPipe(
                filter_rows(""{Year} > 1990""),
                write_csv(""after_1990.csv"")
            )(),
            ""1"": write_csv(""all_years.csv"")
        })
    )

    dashboard = Dashboard(
        load_after_1975,
        split_on_1980,
        save_lotr_books,
        after_1990_and_all_time,
    )
    dashboard.run()
```

**Here are the runs on my Prefect cloud**
![image](https://github.com/move-coop/parsons/assets/3255423/8b964cd4-34f6-4023-81da-079b6d7c5a8d)

**The declarative Pipeline syntax is easy to comprehend next to the Prefect viewer makes it easy to see the data flow**
![image](https://github.com/move-coop/parsons/assets/3255423/ea54ba83-0a24-4bc6-afcb-56186929861c)

**Prefect error handling shows you which pipe failed, what the error was, and even what time it failed.**
![image](https://github.com/move-coop/parsons/assets/3255423/c9e61b58-555b-47b3-90e2-5b3cf9a01537)


---
## Original Draft - Revision 1
### Prototype

This prototype is in the `/pipelines` folder in the PR branch. All of the code is contained in the `parsons_pipelines.py` file, which is a runnable file that executes three pipelines based on a stored CSV file and the open-source _Lord of the Rings_ API.

**Here are some highlights:**

_Declarative pipelines syntax, made by composing pipes._
```python
    load_after_1975 = Pipeline(
        load_from_csv(""deniro.csv"")
        (
            print_data(""Raw Data"")
        )(
            clean_year()
        )(
            filter_rows({
                ""Year"": lambda year: year > 1975
            })
        )(
            print_data(""After 1975"")
        )(
            write_csv(""after_1975.csv"")
        )
    )
    split_on_1980 = Pipeline(
        load_from_csv(""deniro.csv"")
        (
            print_data(""Raw Data"")
        )(
            clean_year()
        )(
            split_data(lambda row: 1 if row[""Year""] >= 1980 else 0)
        )(
            all_streams(print_data(""Split Data""))
        )(
            for_streams({
                0: write_csv(""before_1980.csv""),
                1: write_csv(""after_1979.csv"")
            })
        )
    )
```

_Pipe definitions are normal Parsons' code_
```python
@define_pipe(""convert"")
def convert(data: Table, *args, **kwargs) -> Table:
    return data.convert_column(*args, **kwargs)


@define_pipe(""write_csv"")
def write_csv(data: Table, csv_name: str) -> Table:
    data.to_csv(csv_name)
    return data
```

_Load data in via pipes, either using Parsons or `requests`_
```python
@define_pipe(""load_from_csv"", input_type=PipeResult.Unit)
def load_from_csv(filename: str, **kwargs) -> Table:
    return Table.from_csv(filename, **kwargs)

@define_pipe(""load_lotr_books"", input_type=PipeResult.Unit)
def load_lotr_books_from_api() -> Table:
    # Set up the endpoint and headers
    url = ""https://the-one-api.dev/v2/book""
    headers = {}

    # Make the request to the API
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Raises an HTTPError if the response was an error

    # Convert the JSON response into a Parsons Table
    books_json = response.json().get(""docs"", [])
    books_table = Table(books_json)

    return books_table
```

_Trivially compose & name commonly-combined pipes_
```python
    clean_year = lambda: (
        filter_rows({
            ""Year"": lambda year: year is not None
        })
    )(
        convert(
            ""Year"",
            lambda year_str: int(year_str)
        )
    )
```

_Group pipelines together in a Dashboard to facilitate logging, reporting, etc_
```python
    dashboard = Dashboard(
        load_after_1975,
        split_on_1980,
        save_lotr_books
    )
    dashboard.run()
```

_Call Dashboard with a logger to get free logging of the output of every step in every pipeline_
```python
    dashboard = Dashboard(
        load_after_1975,
        split_on_1980, save_lotr_books,
        logger=CsvLogger()
    )
    dashboard.run()
```
![logging](https://github.com/move-coop/parsons/assets/3255423/00f6d994-cd44-4706-b8f2-e0ed37646be3)

_Generate an HTML report with your pipelines' results_
```python
    dashboard.run()
    dashboard.generate_report(""report.html"")
```
![pipeline_report](https://github.com/move-coop/parsons/assets/3255423/d560cacb-e4f2-4a34-adb4-806c9347a995)

_Individual pipelines can be run to retrieve their data and captured as a Parsons table_
```python
    save_lotr_books_data: Table = Pipeline(
        load_lotr_books_from_api()
        (
            write_csv(""lotr_books.csv"")
        )
    ).run()
    save_lotr_books_data.to_bigquery(...)
```

### Next Steps

If the Parsons contributors decide to move forward with the proposal, I believe these features are necessary to implement an MVP of the pipelines framework:

- [ ] Error handling features. Since the pipelines framework will manage the control flow of the pipelines, it will need to handle errors gracefully.
- [ ] Connector pipes. It's not practical to write a pipeline for every method on every possible connector. We'll need to create a few pipes that compose well with the existing connector library to make pipelines useful.
- [ ] Transformation pipes. Unlike connectors, I do believe that we should complete hide the details of Table from the surface of the pipelines API. This serves the purpose of targeting less technically experienced developers/analysts. If someone needs to drop into the level of Table manipulations, they can write their own pipe. With that said, we should expose all of the necessary table transformations as pipes and write a set of higher-level pipes that can perform more complex transformations than the relatively simple Table API.

These features would be nice to implement, but I don't believe they need to be completed to justify releasing the framework:

- [ ] Data validation, to fail the pipeline in some controlled fashion if some conditions are not met.
- [ ] Better logging. For larger datasets it would be nice to log a sample of the data, not the full data. It would also be nice to target cloud platforms like AWS or GCS for logging records.
- [ ] Alert mechanisms. Enable a dashboard to send an email, slack, etc alert when a pipeline fails.
- [ ] Better reporting . In-interface viewing of the logs, color coded error statuses for pipes, real-time view as production pipelines are running.",[],[],2
212,Fix ActionKit collect_upload_errors() Bug,https://api.github.com/repos/move-coop/parsons/issues/979,Alex French,979,closed,2024-01-25 23:49:21+00:00,2024-01-26 00:18:46+00:00,2024-01-26 00:11:18+00:00,"Test steps:
* Create an .csv file with many invalid rows (e.g. a donations import file with 200+ rows, with a non-existent payment account like ""BAD PAYMENT ACCOUNT"" as the donation_payment_account value for each row).
* Launch a python terminal
* Import dependencies: `from parsons.etl.table import Table`, `from parsons.action_kit import ActionKit`
* Create an ActionKit object `ak` with access to a sandbox ActionKit instance (e.g. `ak = ActionKit(**credentials['action_kit_sandbox'])`)
* Import the data from .csv: `ak_table = Table.from_csv('path/to/upload.csv')`
* Set the import page: `import_page='some-test-import-page'`
* Then, all on one line, run: `res = ak.bulk_upload_table(ak_table, import_page=import_page); errors = ak.collect_upload_errors(res['results']); print(errors); print(len(errors))`.

The final commands are on a single line because the delay of a human entering terminal commands is long enough hide the problem. Before this change, you will get fewer errors than the total number of invalid rows in the file. After the change, you will get as many errors as there are invalid rows.",[],[],0
213,(re) draft Census connector,https://api.github.com/repos/move-coop/parsons/issues/978,Charlie Kramer,978,closed,2024-01-25 14:04:21+00:00,2024-02-20 20:37:17+00:00,2024-02-20 20:37:17+00:00,"Draft census connector--fixed typo in index.html (thanks Austin!), linted, re-ran tests.","[Label(name=""🎉 first PR"")]",[],3
214,Draft Census Connector,https://api.github.com/repos/move-coop/parsons/issues/977,Charlie Kramer,977,closed,2024-01-24 20:57:13+00:00,2024-01-25 13:28:45+00:00,2024-01-25 13:28:45+00:00,Drafted a census connector--includes unit tests (live and mocked) and draft documentation.  Many thanks for all the help and looking forward to your feedback! Charlie,[],[],1
215,fixes copy_s3 by using generic GCS function,https://api.github.com/repos/move-coop/parsons/issues/976,,976,open,2024-01-22 23:08:25+00:00,2024-07-24 01:05:24+00:00,,"During the migration, we generalized a function to use the Transfer Service to copy file(s) from an arbitrary bucket rather than having bespoke functions for S3, GCS etc. However, we missed one reference to the old file. 

Austin this should get you started. Could stand some more manual testing and we should add a unit test here obviously. I can get to that probably post the Feb 5th release of BQ to the rest of the members but wanted to get you what I had.",[],[],4
216,Bump Multiple Docs Deps Around Sphinx,https://api.github.com/repos/move-coop/parsons/issues/975,Soren Spicknall,975,closed,2024-01-22 15:52:35+00:00,2024-01-23 17:11:37+00:00,2024-01-23 17:11:37+00:00,"Several docs-related dependencies in Parsons are pretty far behind their latest project versions, and began causing [CI failures](https://github.com/move-coop/parsons/pull/969) related to Sphinx. This bumps Sphinx up to the latest 5.x version before introduction of 6.0, bumps sphinx-rtd-theme correspondingly, and attempts to upgrade myst-parser as well. If build steps fail, we will likely need to be a little less aggressive with the version number leaps of the latter two packages.",[],[],0
217,BigQuery - Call .result() regardless of compression,https://api.github.com/repos/move-coop/parsons/issues/974,Ian,974,closed,2024-01-22 15:32:48+00:00,2024-02-16 15:03:46+00:00,2024-02-16 15:03:36+00:00,"We found that some of the `try`/`except`/`finally` logic employed in the `GoogleBigQuery` connector is suppressing exceptions. This PR explicitly returns a `LoadJobConfig` object when a flat file is forcibly decompressed, such that any errors raised are encountered in the `try` block",[],"[NamedUser(login=""IanRFerguson"")]",1
218,Update get_columns_list() to accept tables with spaces in the name,https://api.github.com/repos/move-coop/parsons/issues/973,Erica Faulkenberry,973,closed,2024-01-21 22:17:18+00:00,2024-01-24 16:12:28+00:00,2024-01-24 16:12:22+00:00,Updates `get_columns_list()` to work with table names that include spaces or begin with invalid characters.,"[Label(name=""bug""), Label(name=""high priority""), Label(name=""connector update""), Label(name=""🎉 first PR"")]","[NamedUser(login=""coastlines"")]",0
219,BigQuery returns empty Table on query with no rows returned,https://api.github.com/repos/move-coop/parsons/issues/972,Austin Weisgrau,972,closed,2024-01-19 22:40:45+00:00,2024-01-31 18:45:16+00:00,2024-01-31 18:09:52+00:00,"This empty Table has columns defined. Prior behavior was to return None. This change brings the behavior into alignment with the Redshift connector.

Addresses #971 ",[],[],0
220,[Bug] BigQuery connector returns None rather than empty Table on query with no rows returned,https://api.github.com/repos/move-coop/parsons/issues/971,Austin Weisgrau,971,closed,2024-01-19 22:22:51+00:00,2024-03-21 16:17:03+00:00,2024-03-21 16:17:03+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
The Redshift connector, on running a query that returns no rows, will return a parsons Table with no rows but with all of the columns defined. The BigQuery connector instead returns None.

I like the Redshift connector behavior better - it's more useful to still have the columns named even if there are no rows, and makes using the output from the connector the same whether or not rows are returned, making downstream use easier without conditionals.

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->
```
redshift_tbl = Redshift().query('select current_user limit 0')
type(redshift_tbl)
>> parsons.etl.table.Table
redshift_tbl.columns
>> ['current_user']

bigquery_tbl = BigQuery().query('select session_user() limit 0')
type(bigquery_tbl)
>> NoneType
```


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.
medium","[Label(name=""bug"")]",[],0
221,[Bug] BigQuery client issues warning about missing dependency,https://api.github.com/repos/move-coop/parsons/issues/970,Austin Weisgrau,970,open,2024-01-19 19:57:09+00:00,2024-01-19 19:57:09+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
Initializing the BigQuery client emits this warning:

```
~/.pyenv/versions/3.10.9/envs/prefect/lib/python3.10/site-packages/google/cloud/bigquery/client.py:567: UserWarning: Cannot create BigQuery Storage client, the dependency google-cloud-bigquery-storage is not installed.
  warnings.warn(
```

Although google-cloud-storage is a specified dependency of the google extras for parsons, the google-cloud-bigquery-storage package is not a requirement. 

I don't know what the consequences are for missing this package, besides that this warning is emitted.

# Priority

low","[Label(name=""bug"")]",[],0
222,add client_options with default scopes to client,https://api.github.com/repos/move-coop/parsons/issues/969,,969,closed,2024-01-18 23:57:17+00:00,2024-01-23 20:22:33+00:00,2024-01-23 20:22:26+00:00,"Add a client_options parameter to the client function, including default scopes. These scopes are necessary to allow querying of external tables stored in Google Drive. The formatting should make it possible to pass in other client options as required.","[Label(name=""high priority""), Label(name=""connector update"")]","[NamedUser(login=""graemelorimer"")]",1
223,Kasiahinkson/extend list blobs,https://api.github.com/repos/move-coop/parsons/issues/968,Kasia Hinkson,968,closed,2024-01-12 15:45:59+00:00,2024-01-12 15:49:40+00:00,2024-01-12 15:49:34+00:00,"Extends the functionality of list_blobs to allow the inclusion of more file details, to facilitate things like filtering on size",[],[],0
224,Bump jinja2 from 3.0.2 to 3.1.3,https://api.github.com/repos/move-coop/parsons/issues/967,,967,closed,2024-01-11 18:55:24+00:00,2024-01-18 21:09:17+00:00,2024-01-18 21:09:15+00:00,"Bumps [jinja2](https://github.com/pallets/jinja) from 3.0.2 to 3.1.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/pallets/jinja/releases"">jinja2's releases</a>.</em></p>
<blockquote>
<h2>3.1.3</h2>
<p>This is a fix release for the 3.1.x feature branch.</p>
<ul>
<li>Fix for <a href=""https://github.com/pallets/jinja/security/advisories/GHSA-h5c8-rqwp-cp95"">GHSA-h5c8-rqwp-cp95</a>. You are affected if you are using <code>xmlattr</code> and passing user input as attribute keys.</li>
<li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-3"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-3</a></li>
<li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/15?closed=1"">https://github.com/pallets/jinja/milestone/15?closed=1</a></li>
</ul>
<h2>3.1.2</h2>
<p>This is a fix release for the <a href=""https://github.com/pallets/jinja/releases/tag/3.1.0"">3.1.0</a> feature release.</p>
<ul>
<li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-2"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-2</a></li>
<li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/13?closed=1"">https://github.com/pallets/jinja/milestone/13?closed=1</a></li>
</ul>
<h2>3.1.1</h2>
<ul>
<li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-1"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-1</a></li>
<li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/12?closed=1"">https://github.com/pallets/jinja/milestone/12?closed=1</a></li>
</ul>
<h2>3.1.0</h2>
<p>This is a feature release, which includes new features and removes previously deprecated features. The 3.1.x branch is now the supported bugfix branch, the 3.0.x branch has become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades. We also encourage upgrading to MarkupSafe 2.1.1, the latest version at this time.</p>
<ul>
<li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0</a></li>
<li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/8?closed=1"">https://github.com/pallets/jinja/milestone/8?closed=1</a></li>
<li>MarkupSafe changes: <a href=""https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1"">https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1</a></li>
</ul>
<h2>3.0.3</h2>
<ul>
<li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.0.x/changes/#version-3-0-3"">https://jinja.palletsprojects.com/en/3.0.x/changes/#version-3-0-3</a></li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/main/CHANGES.rst"">jinja2's changelog</a>.</em></p>
<blockquote>
<h2>Version 3.1.3</h2>
<p>Released 2024-01-10</p>
<ul>
<li>Fix compiler error when checking if required blocks in parent templates are
empty. :pr:<code>1858</code></li>
<li><code>xmlattr</code> filter does not allow keys with spaces. GHSA-h5c8-rqwp-cp95</li>
<li>Make error messages stemming from invalid nesting of <code>{% trans %}</code> blocks
more helpful. :pr:<code>1918</code></li>
</ul>
<h2>Version 3.1.2</h2>
<p>Released 2022-04-28</p>
<ul>
<li>Add parameters to <code>Environment.overlay</code> to match <code>__init__</code>.
:issue:<code>1645</code></li>
<li>Handle race condition in <code>FileSystemBytecodeCache</code>. :issue:<code>1654</code></li>
</ul>
<h2>Version 3.1.1</h2>
<p>Released 2022-03-25</p>
<ul>
<li>The template filename on Windows uses the primary path separator.
:issue:<code>1637</code></li>
</ul>
<h2>Version 3.1.0</h2>
<p>Released 2022-03-24</p>
<ul>
<li>
<p>Drop support for Python 3.6. :pr:<code>1534</code></p>
</li>
<li>
<p>Remove previously deprecated code. :pr:<code>1544</code></p>
<ul>
<li><code>WithExtension</code> and <code>AutoEscapeExtension</code> are built-in now.</li>
<li><code>contextfilter</code> and <code>contextfunction</code> are replaced by
<code>pass_context</code>. <code>evalcontextfilter</code> and
<code>evalcontextfunction</code> are replaced by <code>pass_eval_context</code>.
<code>environmentfilter</code> and <code>environmentfunction</code> are replaced
by <code>pass_environment</code>.</li>
<li><code>Markup</code> and <code>escape</code> should be imported from MarkupSafe.</li>
<li>Compiled templates from very old Jinja versions may need to be
recompiled.</li>
<li>Legacy resolve mode for <code>Context</code> subclasses is no longer
supported. Override <code>resolve_or_missing</code> instead of</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pallets/jinja/commit/d9de4bb215fd1cc8092a410fb834c7c4060b1fc1""><code>d9de4bb</code></a> release version 3.1.3</li>
<li><a href=""https://github.com/pallets/jinja/commit/50124e16561f17f6c1ec85a692f6551418971cdc""><code>50124e1</code></a> skip test pypi</li>
<li><a href=""https://github.com/pallets/jinja/commit/9ea7222ef3f184480be0d0884e30ccfb4172b17b""><code>9ea7222</code></a> use trusted publishing</li>
<li><a href=""https://github.com/pallets/jinja/commit/da703f7aae36b1e88baaa20de334d7ff6378fdde""><code>da703f7</code></a> use trusted publishing</li>
<li><a href=""https://github.com/pallets/jinja/commit/bce174692547464512383ec40e0f8338b8811983""><code>bce1746</code></a> use trusted publishing</li>
<li><a href=""https://github.com/pallets/jinja/commit/7277d8068be593deab3555c7c14f974ada373af1""><code>7277d80</code></a> update pre-commit hooks</li>
<li><a href=""https://github.com/pallets/jinja/commit/5c8a10522421270f66376a24ec8e0d6812bc4b14""><code>5c8a105</code></a> Make nested-trans-block exceptions nicer (<a href=""https://redirect.github.com/pallets/jinja/issues/1918"">#1918</a>)</li>
<li><a href=""https://github.com/pallets/jinja/commit/19a55db3b411343309f2faaffaedbb089e841895""><code>19a55db</code></a> Make nested-trans-block exceptions nicer</li>
<li><a href=""https://github.com/pallets/jinja/commit/716795349a41d4983a9a4771f7d883c96ea17be7""><code>7167953</code></a> Merge pull request from GHSA-h5c8-rqwp-cp95</li>
<li><a href=""https://github.com/pallets/jinja/commit/7dd3680e6eea0d77fde024763657aa4d884ddb23""><code>7dd3680</code></a> xmlattr filter disallows keys with spaces</li>
<li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/3.0.2...3.1.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=3.0.2&new-version=3.1.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],1
225,Bump python-dateutil from v2.8.1 to v2.8.2,https://api.github.com/repos/move-coop/parsons/issues/966,Wil T,966,closed,2024-01-09 23:30:40+00:00,2024-02-22 16:11:17+00:00,2024-02-06 20:26:25+00:00,"Pandas requires python-dateutil 2.8.2 since at least pandas v2.0.0, with pandas v3.0.0 coming soon.
It would be helpful to be able to be able to use parsons with pandas.","[Label(name=""🎉 first PR"")]",[],2
226,Action Builder Deactivate & Reactivate Connections,https://api.github.com/repos/move-coop/parsons/issues/965,,965,closed,2024-01-02 21:08:07+00:00,2024-01-18 16:36:57+00:00,2024-01-18 16:36:49+00:00,"Connection records in Action Builder cannot be deleted. Instead, there's an ""Inactive"" flag that determines whether they appear in the UI or not on either of the two Entity records they connect. This PR introduces a new method, `deactivate_connection()`, which deactivates an existing Connection. 

The method requires the ""interact ID"" for one of the two Entities as a positional argument. One of two additional (keyword) arguments is required to make the change: either the interact ID for the other Entity (POST request to the connection helper endpoint), or the interact ID of the connection itself (PUT request to the specific connection endpoint).

This PR also updates the existing `upsert_connection()` method to include by default an explicit `False` value for the `inactive` flag when upserting Connection data, under the assumption that upserting to a given Connection implies that the user wants that Connection to be active. The `reactivate` argument can be set to `False` to prevent this and make changes to inactive Connections _without_ reactivating them.

Tests were written and adapted for these additions and changes, and are running successfully. Furthermore, these changes were successful in live tests done by defining the two methods locally and overriding the Class definitions before instantiation.

A good future PR might be to adapt the existing `upsert_connection()` method to accept the Connection ID as an optional keyword arg, to trigger a PUT request directly to the specific Connection endpoint. Argument validation could be updated such that a single-item list (or even a string) could be accepted for the positional `identifiers` arg if the Connection ID is provided.","[Label(name=""enhancement"")]","[NamedUser(login=""ydamit"")]",0
227,Explicitly name vars,https://api.github.com/repos/move-coop/parsons/issues/964,,964,closed,2024-01-02 15:45:44+00:00,2024-01-03 03:28:35+00:00,2024-01-03 03:28:02+00:00,To fix an error regarding mismatched arguments within the match function.,[],[],0
228,Enable email dict to be passed to VAN.upsert_person(),https://api.github.com/repos/move-coop/parsons/issues/963,Austin Weisgrau,963,closed,2023-12-20 21:54:27+00:00,2024-02-20 20:58:30+00:00,2024-02-20 20:58:25+00:00,"This allows for greater configurability of email data when upserting person records in EveryAction. This change is backwards compatible, but allows, for example, setting emails to not be subscribed by default when loading into EveryAction.",[],[],2
229,Bump paramiko from 2.11.0 to 3.4.0,https://api.github.com/repos/move-coop/parsons/issues/962,,962,closed,2023-12-19 20:48:46+00:00,2024-02-06 20:29:38+00:00,2024-02-06 20:29:30+00:00,"Bumps [paramiko](https://github.com/paramiko/paramiko) from 2.11.0 to 3.4.0.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/paramiko/paramiko/commit/f0881ba8af57d1a122ef19c40d144afdcb6e0824""><code>f0881ba</code></a> Cut 3.4.0</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/3e4bdf998f5b1508322234a527e8fa432220368b""><code>3e4bdf9</code></a> Changelog/comment updates</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/30b447b911c39460bbef5e7834e339c43a251316""><code>30b447b</code></a> Linting</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/33508c920309860c4a775be70f209c2a400e18ec""><code>33508c9</code></a> Expand MessageOrderError use to handle more packet types</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/96db1e2be856eac66631761bae41167a1ebd2b4e""><code>96db1e2</code></a> Raise exception when sequence numbers rollover during initial kex</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/58785d29c47570fa700e096d16b9a0d3a6069048""><code>58785d2</code></a> Changelog tweak re: other new Transport kwarg</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/8dcb237f0ee095b1d8b26765c3d41d0ab6963be9""><code>8dcb237</code></a> Test-suite-only bugfix: defer did not actually imply skip_verify</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/fa46de7feeeb8a01dc471581a0258252ce4f2db6""><code>fa46de7</code></a> Reset sequence numbers on rekey</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/75e311d3c0845a316b6e7b3fae2488d86ad5a270""><code>75e311d</code></a> Enforce zero seqno on kexinit</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/73f079f5a4bbba7f3048dadbe05b24242206745e""><code>73f079f</code></a> Fill in CVE number for Terrapin attack</li>
<li>Additional commits viewable in <a href=""https://github.com/paramiko/paramiko/compare/2.11.0...3.4.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=paramiko&package-manager=pip&previous-version=2.11.0&new-version=3.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

You can trigger a rebase of this PR by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>

> **Note**
> Automatic rebases have been disabled on this pull request as it has been open for over 30 days.
","[Label(name=""dependency update"")]",[],1
230,Canalespy gsheet helper functions,https://api.github.com/repos/move-coop/parsons/issues/961,,961,open,2023-12-19 19:04:09+00:00,2024-07-17 19:03:10+00:00,,"Added some internal tmc canalespy gsheet functions to parsons! One function recursively re-attempts a specified gsheet method, while the other combines data from multiple gsheets into one parsons table. Also, not quite sure why there's the import issue in the unit tests? ",[],[],2
231,Adding TMC canalespy query helpers,https://api.github.com/repos/move-coop/parsons/issues/960,,960,open,2023-12-19 15:50:58+00:00,2024-07-15 18:36:04+00:00,,"Adds two sql querying helper functions (one gets sql from a file and the other dedups a given table using the partition order by logic). Also, not entirely sure why the gmail unit tests are failing. I'm pretty sure I didn't edit the gmail functions or their unit tests, so would appreciate any help that anyone could provide on this!",[],[],0
232,Miscellaneous fixes to BigQuery connector,https://api.github.com/repos/move-coop/parsons/issues/959,Austin Weisgrau,959,closed,2023-12-14 22:06:17+00:00,2024-01-31 18:45:30+00:00,2024-01-31 18:11:08+00:00,"The BigQuery.copy() method does not seem to work for a variety of situations, fixes are made here as I encounter these issues and resolve them.

# Fixed BigQuery type map

Source types ultimately come from `petl.typeset`, which calls
`type(v).__name__`. This call does not include source module, but only
the type name itself. e.g. `date` and not `datetime.date`

# Prefer not NoneType when inferring schema for Table load to BigQuery 

If a Parsons Table column has values like `[None, None, True, False]`,
the BigQuery connector will infer that the appropriate type for this
column is NoneType, which it will translate into a STRING type.

This change ensures that types returned by petl.typecheck() will
choose the first available type that isn't 'NoneType' if that is
available.

# Fix commented out row to use job_config passed as argument to BigQuery.copy()

It looks like this line was accidentally commented out

# Parse python datetime objects for BigQuery as datetime or timestamp

Python datetime objects may represent timestamps or datetimes in
BigQuery, depending on whether they do or do not have a timezone
attached.

Before this change, a parsons Table that included datetimes with
timestamps would fail to load to BigQuery because BigQuery
would reject datetime strings with timezone information as the
""datetime"" data type.

# Only generate schema for BigQuery when table does not already exist

Always passing a schema to BigQuery is not necessary, and introduces
situations for provided schema to mismatch actual schema.

When table already exists in BigQuery, fetch the schema from BigQuery",[],"[NamedUser(login=""sharinetmc"")]",1
233,Fix nullable column type parsing by BigQuery connector,https://api.github.com/repos/move-coop/parsons/issues/958,Austin Weisgrau,958,closed,2023-12-12 20:47:19+00:00,2023-12-14 22:06:55+00:00,2023-12-14 22:06:38+00:00,"If a Parsons Table column has values like `[None, None, True, False]`,
the BigQuery connector will infer that the appropriate type for this
column is NoneType, which it will translate into a STRING type.

This change ensures that types returned by petl.typecheck() will
choose the first available type that isnt `NoneType` if that is
available.

Adds a test that checks if the connector is able to successfully parse a column type when nulls are included. First commit here will allow test to run and fail, second commit will make the fix and demonstrate the test succeeds.",[],[],1
234,[Feature/Addition] Google Slides Connector,https://api.github.com/repos/move-coop/parsons/issues/957,elyse-weiss,957,open,2023-12-12 15:49:33+00:00,2024-04-02 16:46:29+00:00,,"
It would be great to have a Google Slides Connector to help folks automate the population of slides! I already wrote some code, but I am less familiar with actually adding a connector.

## Detailed Description
https://gist.github.com/elyse-weiss/648223893da6c15013eed78a65831557

## Context
This is GREAT for automating the creation of slides -- could be helpful for making one slide per state, one slide per year, etc.

## Possible Implementation

## Priority
Low - I have working code, but would be great to add to Parsons!","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""new connector"")]","[NamedUser(login=""elyse-weiss"")]",0
235,Fix BigQuery type map for datetime objects,https://api.github.com/repos/move-coop/parsons/issues/956,Austin Weisgrau,956,closed,2023-12-11 22:48:56+00:00,2023-12-14 22:06:58+00:00,2023-12-14 22:06:49+00:00,"Source types ultimately come from `petl.typeset`, which calls `type(v).__name__`. This call does not include source module, but only the type name itself. e.g. `date` and not `datetime.date`",[],[],1
236,[Bug] Some methods listed in documentation are not present in Table class,https://api.github.com/repos/move-coop/parsons/issues/955,Lydia-Rose Kesich,955,closed,2023-12-11 17:29:23+00:00,2023-12-11 19:29:51+00:00,2023-12-11 19:29:51+00:00,"## Detailed Description
The method rename_columns() is not present when the Table class is imported. Running `some_table.rename_columns(name_dict)` fails with the error ""AttributeError: 'Table' object has no attribute 'rename_columns'.""

I ran dir() to get a list of imported Table methods and noticed that rename_columns() is not listed. deduplicate() is also not listed despite appearing in the documentation.

The rename_columns() function works fine when copy/pasted from source code, just not as an attribute of the table object.

## To Reproduce

```
from parsons import Table
tbl = Table()
dir(tbl)
```
rename_columns and deduplicate are not present in listed methods.

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used: 1.0.0


## Priority
Low","[Label(name=""bug"")]",[],2
237,"[Bug]  ERROR at setup of TestCatalist.test_load_matches, test_status, test_upload_with_options, test_upload, test_load_table_to_sftp, test_validate_table,test_fixtures_active",https://api.github.com/repos/move-coop/parsons/issues/954,,954,closed,2023-12-10 09:41:23+00:00,2023-12-10 09:56:10+00:00,2023-12-10 09:56:10+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
Errors coming back from test_catalist nothing passes did anyone else experienced this issue?

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
=============================================================================== ERRORS ================================================================================
_________________________________________________________ ERROR at setup of TestCatalist.test_fixtures_active _________________________________________________________
file  parsons/parsons/test/test_catalist/test_catalist.py, line 37
      def test_fixtures_active(self) -> None:
file parsons/parsons/test/test_catalist/conftest.py, line 36
  @pytest.fixture(autouse=True)
  def mock_miscellaneous(mocker) -> Generator[MagicMock, None, None]:
E       fixture 'mocker' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, datadir, doctest_namespace, mock_miscellaneous, mock_requests, mock_sftp, monkeypatch, original_datadir, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, requests_mock, shared_datadir, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

parsons/parsons/test/test_catalist/conftest.py:36
_________________________________________________________ ERROR at setup of TestCatalist.test_validate_table __________________________________________________________
file parsons/parsons/test/test_catalist/test_catalist.py, line 43
      def test_validate_table(self) -> None:
fileparsons/parsons/test/test_catalist/conftest.py, line 36
  @pytest.fixture(autouse=True)
  def mock_miscellaneous(mocker) -> Generator[MagicMock, None, None]:
E       fixture 'mocker' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, datadir, doctest_namespace, mock_miscellaneous, mock_requests, mock_sftp, monkeypatch, original_datadir, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, requests_mock, shared_datadir, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

parsons/parsons/test/test_catalist/conftest.py:36
_______________________________________________________ ERROR at setup of TestCatalist.test_load_table_to_sftp ________________________________________________________
file parsons/parsons/test/test_catalist/test_catalist.py, line 55
      def test_load_table_to_sftp(self) -> None:
file parsons/parsons/test/test_catalist/conftest.py, line 36
  @pytest.fixture(autouse=True)
  def mock_miscellaneous(mocker) -> Generator[MagicMock, None, None]:
E       fixture 'mocker' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, datadir, doctest_namespace, mock_miscellaneous, mock_requests, mock_sftp, monkeypatch, original_datadir, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, requests_mock, shared_datadir, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

parsons/parsons/test/test_catalist/conftest.py:36
_____________________________________________________________ ERROR at setup of TestCatalist.test_upload ______________________________________________________________
file parsons/parsons/test/test_catalist/test_catalist.py, line 82
      def test_upload(self, mock_requests) -> None:
file parsons/parsons/test/test_catalist/conftest.py, line 36
  @pytest.fixture(autouse=True)
  def mock_miscellaneous(mocker) -> Generator[MagicMock, None, None]:
E       fixture 'mocker' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, datadir, doctest_namespace, mock_miscellaneous, mock_requests, mock_sftp, monkeypatch, original_datadir, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, requests_mock, shared_datadir, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

parsons/parsons/test/test_catalist/conftest.py:36
_______________________________________________________ ERROR at setup of TestCatalist.test_upload_with_options _______________________________________________________
file parsons/test/test_catalist/test_catalist.py, line 101
      def test_upload_with_options(self, mock_requests) -> None:
file parsons/parsons/test/test_catalist/conftest.py, line 36
  @pytest.fixture(autouse=True)
  def mock_miscellaneous(mocker) -> Generator[MagicMock, None, None]:
E       fixture 'mocker' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, datadir, doctest_namespace, mock_miscellaneous, mock_requests, mock_sftp, monkeypatch, original_datadir, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, requests_mock, shared_datadir, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

parsons/parsons/test/test_catalist/conftest.py:36
_____________________________________________________________ ERROR at setup of TestCatalist.test_status ______________________________________________________________
file parsons/parsons/test/test_catalist/test_catalist.py, line 119
      def test_status(self, mock_requests) -> None:
file parsons/test/test_catalist/conftest.py, line 36
  @pytest.fixture(autouse=True)
  def mock_miscellaneous(mocker) -> Generator[MagicMock, None, None]:
E       fixture 'mocker' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, datadir, doctest_namespace, mock_miscellaneous, mock_requests, mock_sftp, monkeypatch, original_datadir, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, requests_mock, shared_datadir, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

parsons/parsons/test/test_catalist/conftest.py:36
__________________________________________________________ ERROR at setup of TestCatalist.test_load_matches ___________________________________________________________
file parsons/parsons/test/test_catalist/test_catalist.py, line 135
      def test_load_matches(self) -> None:
file parsons/parsons/test/test_catalist/conftest.py, line 36
  @pytest.fixture(autouse=True)
  def mock_miscellaneous(mocker) -> Generator[MagicMock, None, None]:
E       fixture 'mocker' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, datadir, doctest_namespace, mock_miscellaneous, mock_requests, mock_sftp, monkeypatch, original_datadir, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, requests_mock, shared_datadir, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.



## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->

Mac
Python 3.10.13

## Additional Context

Maybe something needs to be added to the requrements.txt? did anyone else experienced this?

<img width=""402"" alt=""image"" src=""https://github.com/move-coop/parsons/assets/75395024/057076ce-f340-41f1-a279-15992f8b6959"">
<img width=""1363"" alt=""image"" src=""https://github.com/move-coop/parsons/assets/75395024/737de095-db09-4524-96b9-2e9c64bf8167"">


## Priority

High I want to apply changes on PR that is waiting to be merged.

@austinweisgrau the changes were by you can you help on this matter?","[Label(name=""bug"")]",[],1
238,Kasiahinkson/extend list blobs,https://api.github.com/repos/move-coop/parsons/issues/953,Kasia Hinkson,953,closed,2023-12-08 21:46:01+00:00,2023-12-11 14:58:33+00:00,2023-12-11 14:58:33+00:00,,[],[],0
239,GCP Docs,https://api.github.com/repos/move-coop/parsons/issues/952,Ian,952,closed,2023-12-08 16:55:48+00:00,2023-12-08 17:03:58+00:00,2023-12-08 17:03:55+00:00,"Pretty minimal changes here to be honest, I think we're good to go",[],"[NamedUser(login=""IanRFerguson"")]",0
240,Install dependencies for tests based on package install,https://api.github.com/repos/move-coop/parsons/issues/951,Austin Weisgrau,951,closed,2023-12-07 22:38:14+00:00,2023-12-08 20:11:57+00:00,2023-12-08 15:36:06+00:00,"Parsons dependencies can be installed in two different ways.
`pip install -r requirements.txt` is equivalent to pip install parsons
UNLESS the environment variable `PARSONS_LIMITED_DEPENDENCIES` is set,
in which case the dependencies are installed based on the explicit
configuration in `setup.py`.

There can be drift between what is defined in requirements.txt and
what is defined in `setup.py`.

These changes ensure that both sets of dependencies are used to run
tests, ensuring alignment between test environments and actual usage.


An example is tests passing on [this PR](https://github.com/move-coop/parsons/pull/948) even though the same tests would fail when using `PARSONS_LIMITED_DEPENDENCIES=1 pip install parsons[all]` (as explained and fixed in [this PR](https://github.com/move-coop/parsons/pull/946))",[],[],1
241,Batch upsert method for Airtable class,https://api.github.com/repos/move-coop/parsons/issues/950,Lydia-Rose Kesich,950,closed,2023-12-07 20:40:30+00:00,2024-07-22 01:19:07+00:00,2024-07-22 01:19:07+00:00,"## Detailed Description
It would help me if there was a Parsons method in the Airtable class that creates a [performUpsert request](https://airtable.com/developers/web/api/update-multiple-records). For my purposes, it's important to be able to supply arguments to the fieldsToMergeOn array. This would reduce the number of calls needed to achieve the same effect with existing update_record and insert_record methods.

## Priority
This is a low priority for me--I will use it when it's complete, but in the meantime I'll use another Python wrapper for the airtable API.","[Label(name=""enhancement""), Label(name=""low priority"")]","[NamedUser(login=""matthewkrausse"")]",2
242,Add Tests to major-release Branch,https://api.github.com/repos/move-coop/parsons/issues/949,Ian,949,closed,2023-12-07 16:09:19+00:00,2023-12-07 19:39:31+00:00,2023-12-07 19:39:28+00:00,Adds `major-release` branch to GitHub workflow to run tests / `black` / `flake8` checks,[],"[NamedUser(login=""IanRFerguson"")]",2
243,Resolve GCP Test Failures For Major Release,https://api.github.com/repos/move-coop/parsons/issues/948,Ian,948,closed,2023-12-07 14:25:54+00:00,2023-12-08 15:01:37+00:00,2023-12-08 15:01:34+00:00,There are several unit tests that are failing for new GCP connectors - this PR resolves those test failures in preparation for major release,[],"[NamedUser(login=""IanRFerguson"")]",7
244,Enable parameterization of bigquery queries,https://api.github.com/repos/move-coop/parsons/issues/947,Austin Weisgrau,947,closed,2023-12-06 23:49:06+00:00,2023-12-07 21:47:10+00:00,2023-12-07 21:47:09+00:00,"Addresses https://github.com/move-coop/parsons/issues/929

Documentation on BigQuery query parameterization was wrong, but also with the existing implementation of the query method, parameterization was not possible.

Updates are made to enable parameterization of queries using the bigquery client, and documentation is updated to reflect correct usage.

BigQuery python query parameterization requires the use of client.query() rather than a more generic database connection & cursor.

See documentation here: https://cloud.google.com/bigquery/docs/parameterized-queries

This PR replaces #931 to make these updates on top of the major-release branch version of the bigquery client, rather than main.

TODO:
- [ ] Tests on google_bigquery module will need to be fixed, but they were already broken on the major-release branch so require some work there first",[],[],1
245,Fix import of google.storage.storage_transfer by including dependency,https://api.github.com/repos/move-coop/parsons/issues/946,Austin Weisgrau,946,closed,2023-12-06 23:26:45+00:00,2023-12-08 15:43:28+00:00,2023-12-08 15:43:28+00:00,This import can only work if the google-cloud-storage-transfer dependency is included,[],[],0
246,Re-apply revert that got lost in major-release branch,https://api.github.com/repos/move-coop/parsons/issues/945,Austin Weisgrau,945,closed,2023-12-06 00:12:45+00:00,2023-12-08 15:49:01+00:00,2023-12-08 15:49:01+00:00,For some reason #876 got lost in the major-release branch. Somehow the reverted commit got included in #873 and the reversion wasn't included when the main branch was merged into major-release with ee4ec8b9872288cce20a2ac2c58bc1379fe39672. This PR duplicates #876 to once again revert 77ead6079ee03c399bbc83314351bc719ed457c3,[],[],0
247,PyPi Page Does Not Detect README.md,https://api.github.com/repos/move-coop/parsons/issues/944,Jason,944,closed,2023-12-05 14:53:15+00:00,2024-04-16 18:17:51+00:00,2024-04-16 18:17:51+00:00,"Currently the [Parsons PyPi page has a blank description](https://pypi.org/project/parsons/). This looks unprofessional, and might cause confusion for potential users trying to learn about Parsons. It's easy to configure the Python build system to use the GitHub readme file as the PyPi description.

If we add this code to our `setup.py` file, it should have the PyPi page use the README.md file as the description:
[https://packaging.python.org/en/latest/guides/making-a-pypi-friendly-readme/](https://packaging.python.org/en/latest/guides/making-a-pypi-friendly-readme/).

```python

# read the contents of your README file
from pathlib import Path
this_directory = Path(__file__).parent
long_description = (this_directory / ""README.md"").read_text()

setup(
    name='an_example_package',
    # other arguments omitted
    long_description=long_description,
    long_description_content_type='text/markdown'
)
```","[Label(name=""bug""), Label(name=""high priority"")]",[],1
248,Parse Boolean types by default,https://api.github.com/repos/move-coop/parsons/issues/943,Austin Weisgrau,943,closed,2023-12-04 21:36:38+00:00,2023-12-08 20:15:20+00:00,2023-12-08 20:15:10+00:00,"Commit 766cfaedc5652af8c39fde890067a99b6fa58518 created a feature for parsing boolean types but turned it off by default. This commit turns that feature on by default and adds a comment about how to turn it off and what that does.

When this feature is turned on, parsons will detect boolean column types when copying a parsons Table to a database and create a boolean column in the database. The default behavior without this change is to coerce booleans to be strings when loading to database.

Partially addresses #942 ",[],[],5
249,"[Feature/Addition] parsons.databases.database.DatabaseCreateStatement.detect_data_type() should detect more data types than str, float, int",https://api.github.com/repos/move-coop/parsons/issues/942,Austin Weisgrau,942,open,2023-12-04 19:49:15+00:00,2023-12-19 22:16:37+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
`parsons.databases.database.DatabaseCreateStatement.detect_data_type()` is only able to currently detect data types str, float, and int. As a result, downstream methods (e.g. `parsons.databases.redshift.rs_create_table.RedshiftCreateTable.data_type()`) that generate DDL statements to create database tables are only able to infer data types for database tables for these 3 types.

It should be possible to infer types for more data types, including datetimes, dates, and booleans.

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->


## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
When using `parsons.Redshift.copy()` to upload a parsons Table into a Redshift database, parsons should be able to infer data types for most column types and pass those into Redshift accurately. However, the current implementation only accomplishes this for string, float, and integer types. All other types are coerced into strings. There should be enough information available in a parsons table to detect and accurately represent other types, including datetimes, dates, and booleans.

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->
postgres, mysql, and redshift database connectors all use the underlying `detect_data_type()` method, so whatever change is made here will need to be compatible with all 3 downstream connectors and SQL generators.

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->","[Label(name=""enhancement""), Label(name=""medium priority"")]",[],7
250,[Feature/Addition] parsons.databases.redshift.copy() should infer column types,https://api.github.com/repos/move-coop/parsons/issues/941,Austin Weisgrau,941,closed,2023-12-04 19:30:16+00:00,2023-12-04 19:49:31+00:00,2023-12-04 19:49:30+00:00,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
By default, `parsons.databases.redshift.copy()` makes all column types VARCHAR in redshift. It is possible to pass a `columntypes` dictionary mapping argument to explicitly set column types, but this method should be able to infer column types from the `parsons.Table` input if no `columntypes` are explicitly provided.

This behavior is built in to the new `parsons.google.bigquery.BigQuery.copy()` method (see [here](https://github.com/move-coop/parsons/blob/9fe9a857ea6a94199fd45797f977bdc31d0d29d2/parsons/google/google_bigquery.py#L1057)) and should be available for copying into redshift as well.

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->


## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
Creating all column types as VARCHAR is more naive than is necessary, the source parsons Table has information available about column types and those should be used to create column types in Redshift as well.

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->
Should be able to closely match the behavior of[ this method for bigquery](https://github.com/move-coop/parsons/blob/9fe9a857ea6a94199fd45797f977bdc31d0d29d2/parsons/google/google_bigquery.py#L1057), with changes for Redshift compatibility.

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->","[Label(name=""enhancement"")]",[],2
251,Nirs actionnetwork changes,https://api.github.com/repos/move-coop/parsons/issues/940,,940,closed,2023-12-03 19:28:23+00:00,2024-01-08 23:10:39+00:00,2024-01-08 23:10:38+00:00,"Hi @shaunagm, Round 2 of functions (POST, PUT and even DELTE just for taggings) to create or update any resource that is available to be updated.
Please let me know if there is any feedback!
Thanks",[],[],1
252,Adjust so tests are run on major-release branch as well as main,https://api.github.com/repos/move-coop/parsons/issues/939,Shauna Gordon-McKeon,939,closed,2023-12-01 20:26:02+00:00,2023-12-08 17:45:51+00:00,2023-12-08 17:45:50+00:00,"We have a suite of tests that run when when a PR Is opened against our main branch, but they don't run against the major-release branch. But we want to test those changes too! If anything it's more important, because they're breaking changes. 

That said, we will still catch test failures. It will just be as part of the release process, which is not ideal as (a) it delays a release and (b) the context of the specific PR will be lost, we may have to ping the submitter, etc.

See [here](https://github.com/move-coop/parsons/pull/938#issuecomment-1836741788) for the headache this caused.","[Label(name=""bug""), Label(name=""high priority""), Label(name=""testing"")]",[],2
253,Merge major-release into main & bump version number in setup.py,https://api.github.com/repos/move-coop/parsons/issues/938,Shauna Gordon-McKeon,938,closed,2023-12-01 20:08:19+00:00,2023-12-08 20:24:50+00:00,2023-12-08 20:24:50+00:00,"I just merged main into major-release, per @Jason94's advice, so this should be clean.",[],[],4
254,GoogleCloudStorage - Handle zip / gzip files flexibly,https://api.github.com/repos/move-coop/parsons/issues/937,Ian,937,closed,2023-11-29 22:26:42+00:00,2023-12-01 18:43:33+00:00,2023-12-01 18:43:29+00:00,"The current `gcs.unzip_blob()` implementation only handles `gzip` files, we're updating to handle other forms of compression",[],"[NamedUser(login=""IanRFerguson"")]",2
255,GoogleCloudStorage - Add GCS Destination Path Param,https://api.github.com/repos/move-coop/parsons/issues/936,Ian,936,closed,2023-11-29 21:03:22+00:00,2023-12-01 18:44:11+00:00,2023-12-01 18:44:07+00:00,"The `s3 -> gcs` component of the `copy_bucket_to_gcs()` function allows for the user to supply a destination path argument, such that blobs are stored in a nested sub-bucket.

This PR replicates that logic in `gcs -> gcs` transfers.",[],"[NamedUser(login=""IanRFerguson"")]",0
256,[Feature/Addition] Adding a connector for the LegiScan API,https://api.github.com/repos/move-coop/parsons/issues/935,Matthew Krausse,935,open,2023-11-29 18:18:08+00:00,2023-12-19 22:12:48+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
I'm working on a python library to wrap the Legiscan API. This allows you to get data on legislators, bills, votes, etc. 
Wanted to see if other people would be interested in this. There is not that much work to be done on it but would be happy to have someone pair on it. 

https://legiscan.com/

## Priority
low","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""new connector"")]",[],7
257,Add IDRT connector for duplicate contact detection,https://api.github.com/repos/move-coop/parsons/issues/934,Jason,934,open,2023-11-29 00:00:20+00:00,2023-12-19 22:51:51+00:00,,"This PR adds a connector for the IDRT (Identity Resolution Transformer) library in Parsons. IDRT is an open-source library that I wrote ([https://github.com/Jason94/identity-resolution](https://github.com/Jason94/identity-resolution)) to use neural networks to match duplicate contacts in a database.

If any reviewers want to test the code, they can use these model files to run the example scripts in the documentation: [models.zip](https://github.com/move-coop/parsons/files/13494585/models.zip)

The connector _does_ provide easy function calls into the two steps of the main algorithm that the library exposes. This algorithm is designed to run directly against a large dataset of contacts stored in a database (Redshift, BigQuery, etc). It makes use of the database during several intermediate steps to reduce execution time. 

The connector does _not_ provide an easy way to quickly match against a Parsons Table containing contact data. The focus of the library is to do this at-scale, so that's where the current focus is. I'd like to add another function at some point that is simpler, and just takes a Parsons table of contact data and some basic configuration and does a match search among the rows of the table. If reviewers think that is likely to be a common use-case, I can add it to the PR before merging.

The connector also does _not_ provide any ways to train the neural networks. That is a much more advanced task than using an existing model, and I didn't see how adding anything to Parsons would make that any easier.

**Notes for reviewers:**
1. The IDRT library pulls in some pretty hefty deep learning libraries as dependencies. I _really_ did not want to add those as default dependencies to Parsons. I noticed that anything listed as a Parsons ""extra"" still gets installed by default, if you don't have the limited dependencies option turned on. I modified the Parsons dependency mechanism in the `setup.py` file to allow truly optional dependencies that must be explicitly installed. In this case, by running `pip install parsons[idr]`.
2. The code is pretty lightweight, all things considered. It's mostly wrapping the calls to the library function in our standard environment variable conventions and providing documentation. The library uses PETL, so it's easy to convert to and from a Parsons Table.
3. The connector includes an adapter to use any Parsons `DatabaseConnector` with the algorithm. It does check to make sure that the `upsert` function is defined on the database object, which currently isn't standard in the DatabaseConnector interface. It should currently work for Redshift and BigQuery.",[],"[NamedUser(login=""IanRFerguson"")]",0
258,Enable Table.to_bigquery(),https://api.github.com/repos/move-coop/parsons/issues/933,Austin Weisgrau,933,closed,2023-11-28 23:47:58+00:00,2023-12-07 21:56:29+00:00,2023-12-07 21:56:25+00:00,This creates parity with Table.to_redshift(),[],[],5
259,Documentation discrepancy regarding supported Python versions,https://api.github.com/repos/move-coop/parsons/issues/932,,932,closed,2023-11-28 22:54:10+00:00,2024-07-25 00:49:41+00:00,2024-07-25 00:49:41+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
The README.md file says ""Parsons is only supported for Python 3.8-10."" However on PyPI it says ""Requires: Python >=3.7.0, <3.11.0""

Not sure which is correct. ","[Label(name=""bug""), Label(name=""medium priority"")]",[],6
260,Enable parameterization of bigquery queries,https://api.github.com/repos/move-coop/parsons/issues/931,Austin Weisgrau,931,closed,2023-11-28 21:43:03+00:00,2023-12-06 23:49:26+00:00,2023-12-06 23:49:26+00:00,"Addresses #929

Documentation on BigQuery query parameterization was wrong, but also with the existing implementation of the query method, parameterization was not possible.

Updates are made to enable parameterization of queries using the bigquery client, and documentation is updated to reflect correct usage.

BigQuery python query parameterization requires the use of `client.query()` rather than `cursor.query()`

See documentation here: https://cloud.google.com/bigquery/docs/parameterized-queries",[],[],3
261,Fix gcs hidden error,https://api.github.com/repos/move-coop/parsons/issues/930,,930,closed,2023-11-28 10:01:15+00:00,2023-11-30 17:30:53+00:00,2023-11-30 17:30:44+00:00,Changes the object that gets pulled when an error gets raised.,[],[],1
262,"Parameterization of Bigquery queries is not possible, documentation is wrong",https://api.github.com/repos/move-coop/parsons/issues/929,Austin Weisgrau,929,closed,2023-11-28 00:34:45+00:00,2023-12-07 21:45:43+00:00,2023-12-07 21:45:43+00:00,"Documentation docstring in BigQuery client query() method is copied from the parsons.Redshift.query() method docstring and is not fully modified to reflect differences. 

The example is based on parsons.Redshift.query() syntax and does not demonstrate how to parametrize a query using the BigQuery client.

https://github.com/move-coop/parsons/blob/90c0f43b2a0207991f58d22dcad4c08aef81223e/parsons/google/google_bigquery.py#L199",[],[],3
263,Add utility method to format phone numbers,https://api.github.com/repos/move-coop/parsons/issues/928,,928,closed,2023-11-24 20:29:12+00:00,2023-12-19 22:56:23+00:00,2023-12-19 22:56:23+00:00,"This pull request adds a new utility method, `format_phone_number`, which formats a phone number in E.164 format. The method takes a phone number as input and an optional country code, and returns the formatted phone number. It removes non-numeric characters and leading zeros, adds the country code prefix if necessary, and formats the number in E.164 format. The pull request also includes unit tests for the `format_phone_number` method to ensure its correctness and handle different scenarios.",[],[],6
264,Fix bug in NGPVAN apply_canvass_result method,https://api.github.com/repos/move-coop/parsons/issues/927,,927,closed,2023-11-24 16:53:40+00:00,2023-12-14 20:48:28+00:00,2023-12-14 20:48:28+00:00,"This pull request fixes a bug in the `apply_canvass_result` function. The problem was that the phone was only being sent when the contact_type_id was either 1 (phone) or 37 (SMS text) even though it was required for other contact types. Now it raises an error if the contact type requires phone. But, if the phone is provided, it sends it regardless of the contact type id. ",[],[],1
265,Enhancement: Added get_representatives_by_address method to Google Civic Connector,https://api.github.com/repos/move-coop/parsons/issues/926,,926,closed,2023-11-23 18:04:26+00:00,2023-12-14 18:10:49+00:00,2023-12-14 18:10:49+00:00,"This PR introduces the get_representative_info_by_address method to our Google Civic API wrapper. This method allows users to retrieve representative information for a given US address. The method returns the raw response from the Google Civic API, which includes information about offices, officials, and divisions.

The method accepts the following parameters:

address: A valid US address in a single string.
include_offices: Whether to return information about offices and officials. Defaults to True.
levels: A list of office levels to filter by.
roles: A list of office roles to filter by.
The method performs validation checks on the input parameters and raises a ValueError if the input is not valid. It also checks the API response for errors and raises a ValueError if the address was invalid.

In addition to the new method, this PR also includes:

Comprehensive unit tests for get_representative_info_by_address, covering different parameters, error handling, and different mock responses.

Updates to the documentation to include example usage of get_representative_info_by_address.

This new functionality enhances our Google Civic API wrapper by providing users with a convenient way to retrieve detailed representative information for a given address.",[],"[NamedUser(login=""KasiaHinkson"")]",0
266,[Bug] Error on apply_canvas_result() in VAN for lack of phone even though phone is provided,https://api.github.com/repos/move-coop/parsons/issues/925,,925,closed,2023-11-22 18:26:15+00:00,2023-12-19 22:05:25+00:00,2023-12-19 22:05:24+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
The `apply_canvas_result()` method fails saying phone is needed even when phone is provided. It looks like there are many phone contacttypes that might require phone ever since VAN made the update to require phone numbers for phone contact types. There was a previous pr that only addressed the situation for a couple contacttypes - https://github.com/move-coop/parsons/issues/525
I am trying to load canvas results for contacttypeid = 19 (Auto Dial) and it is errorring saying that phone is now required. 

I queried and looks like there are potentially a lot more contacttypes that we should include in the section of requiring phone but that are not documented in VAN's documentation as requiring phone...? 
<img width=""949"" alt=""Screenshot 2023-11-22 at 10 19 38 AM"" src=""https://github.com/move-coop/parsons/assets/62267774/6b0c4acf-da74-46d8-979a-d4bccef996b0"">



## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
When I try and load canvas results with the contacttypeid of 19, i get the error that 

`INFO The error was: HTTP error occurred (400): Bad Request, json: {'errors': [{'code': 'INVALID_PARAMETER', 'text': ""'phone.phoneNumber' is required."", 'properties': ['canvassContext']}]}`

## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->

        van.apply_canvass_result(
            row['vanid'],
            result_code_id=row['resultid'], 
            date_canvassed=str(row['datecanvassed']),
            contact_type_id = 19,
            input_type_id=11,
            phone=str(row['phone'])
        )

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know): current
* Environment name and version (e.g. Chrome 39, node.js 5.4): parsons docker image?
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
medium/ high this is for a member that would like to be able to use a script using this parsons function!","[Label(name=""bug""), Label(name=""high priority""), Label(name=""connector update"")]",[],2
267,Add http response to update_mailer,https://api.github.com/repos/move-coop/parsons/issues/924,Sophie Waldman,924,closed,2023-11-15 22:53:43+00:00,2023-11-21 20:54:33+00:00,2023-11-21 20:54:32+00:00,"Without returning the response, or at least the status code, it's impossible to check for errors.",[],[],0
268,Adding rename_columns method to Parsons Table,https://api.github.com/repos/move-coop/parsons/issues/923,,923,closed,2023-11-15 17:44:05+00:00,2023-12-08 20:26:57+00:00,2023-11-16 18:56:30+00:00,"This pull request introduces a new method, rename_columns, to the ETL process. This method allows for the renaming of multiple columns at once in a Parsons Table.

The rename_columns method takes a dictionary as an argument (column_map), where the keys represent the current column names and the values represent the new column names. The method checks if the old column name exists and if the new column name does not already exist in the table. If these conditions are met, the method uses the underlying petl.rename function to perform the renaming operation.

The method returns the updated Parsons Table and also updates the self.table attribute of the class instance.

Unit tests have been added to ensure the rename_columns method works as expected and handles edge cases appropriately. These tests cover scenarios such as renaming columns with a valid column map, renaming only some columns, and attempting to rename a column that does not exist or to a name that already exists.

This enhancement provides a more efficient way to rename multiple columns, improving the flexibility and usability of the ETL process.","[Label(name=""🎉 first PR"")]",[],2
269,Add QuickBooks connector,https://api.github.com/repos/move-coop/parsons/issues/922,,922,open,2023-11-14 21:33:40+00:00,2024-02-08 17:36:22+00:00,,"This pull request adds a QuickBooks connector to the Parsons library, which allows users to pull time tracking data from QuickBooks Time. 

This includes methods for getting data out of the software like users, groups, jobcodes, timesheets, schedules. ",[],"[NamedUser(login=""shaunagm"")]",2
270,[Feature/Addition] Add support for BigQuery export/extract to Google Cloud Storage,https://api.github.com/repos/move-coop/parsons/issues/921,Austin Weisgrau,921,closed,2023-11-13 19:16:42+00:00,2024-07-22 01:22:11+00:00,2024-07-22 01:22:11+00:00,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
BigQuery can export a query result to a file in Google Cloud Storage. This is helpful for various workflows, e.g. running a query with a huge result. Having this feature will also lead to closer parity with the Redshift connector and its ""unload"" method.


## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->
Here's the documentation on this including an example implementation in python: https://cloud.google.com/bigquery/docs/exporting-data#export-data-in-bigquery

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
medium","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],0
271,Update stale references to parsons.databases.bigquery,https://api.github.com/repos/move-coop/parsons/issues/920,,920,closed,2023-11-13 17:40:24+00:00,2023-11-13 18:19:41+00:00,2023-11-13 18:19:38+00:00,"This PR is a minor update to import statements in the `major-release` branch, to point to the updated BigQuery module path in the main`__init__.py` file.

Unit tests for `discover_database` have additionally been updated to reflect the new path.",[],"[NamedUser(login=""dexchan"")]",0
272,Enhancement: Parsons Table: added rename_columns for multiple cols,https://api.github.com/repos/move-coop/parsons/issues/919,,919,closed,2023-11-12 23:36:09+00:00,2023-11-15 17:50:02+00:00,2023-11-15 16:53:47+00:00,"This pull request introduces a new method, rename_columns, to the ETL process. This method allows for the renaming of multiple columns at once in a Parsons Table.

The rename_columns method takes a dictionary as an argument (column_map), where the keys represent the current column names and the values represent the new column names. The method checks if the old column name exists and if the new column name does not already exist in the table. If these conditions are met, the method uses the underlying petl.rename function to perform the renaming operation.

The method returns the updated Parsons Table and also updates the self.table attribute of the class instance.

Unit tests have been added to ensure the rename_columns method works as expected and handles edge cases appropriately. These tests cover scenarios such as renaming columns with a valid column map, renaming only some columns, and attempting to rename a column that does not exist or to a name that already exists.

This enhancement provides a more efficient way to rename multiple columns, improving the flexibility and usability of the ETL process.",[],[],2
273,Generalize the oauth_api_connector to accept authorization kwargs,https://api.github.com/repos/move-coop/parsons/issues/918,Austin Weisgrau,918,closed,2023-11-09 23:07:08+00:00,2024-02-20 20:34:26+00:00,2024-02-20 20:34:26+00:00,"The oauth_api_connector can simplify authorization with APIs that use OAuth2, but requires some modifications to be general enough to work with different implementations of OAuth2.

This PR enables the use of authorization kwargs that are passed to authorization endpoints when requesting a token. The updated connector is then implemented in the Zoom connector as a test / demonstration.

Someone who has Zoom credentials should check if this update works.

Prior to this PR, the only connector that used the oauth_api_connector is the controlshift connector. That one should be unaffected, because the new argument is optional and doesn't change behavior if it is not used.

This should be tested by folks who have credentials with all three directly affected connectors:

- [x] Catalist Match
- [x] Zoom
- [x] Controlshift",[],[],3
274,Enhancement: Action Network Connector: Added unpack_statistics param in get_messages method,https://api.github.com/repos/move-coop/parsons/issues/917,Matthew Krausse,917,closed,2023-11-09 22:43:28+00:00,2023-11-28 23:32:11+00:00,2023-11-15 17:19:19+00:00,"This PR introduces an enhancement to the get_messages method in the action_network.py file. The update includes an optional parameter unpack_statistics which, when set to True, unpacks the statistics dictionary into the returned Parsons Table. This feature provides users with more granular data access, allowing them to directly interact with the statistics related to each message.

The unpack_statistics parameter defaults to False, ensuring backward compatibility for users who do not require this level of detail. When unpack_statistics is True, the method uses the unpack_dict function on the statistics field of the Parsons Table.

This enhancement improves the flexibility and usefulness of the get_messages method, providing users with more control over the data they retrieve.

Please review and provide any feedback.",[],[],1
275,Enable passing arbitrary additional fields to NGPVAN people match API,https://api.github.com/repos/move-coop/parsons/issues/916,Austin Weisgrau,916,closed,2023-11-09 00:18:50+00:00,2023-11-28 17:34:34+00:00,2023-11-28 17:30:20+00:00,"The existing arguments for using the NGPVAN people match API are quite limited. There are many more fields that would be useful to have access to. Currently it is possible to access those by using the `find_person_json()` method, but this requires reproducing a lot of the behavior around structuring the JSON response. Allowing kwargs to be passed through the stack would simplify the usage here.

If I want to add an employer to a contact record, the current use looks like:
```
parsons.VAN(...).upsert_person_json({
    ""first_name"": ""Austin"",
    ""last_name"": ""Weisgrau"",
    ""emails"": [{
        ""email"": ""austinweisgrau@gmail.com""
    }],
    ""employer"": ""Working Families Party""
})
```

After this change, it looks like this:
```
parsons.VAN(...).upsert_person(first_name='Austin', last_name='Weisgrau', email='austinweisgrau@gmail.com', employer='Working Families Party')
```

The more fields are used, the more the added simplicity of this new implementation.",[],"[NamedUser(login=""KasiaHinkson"")]",1
276,Fix incorrect documentation on ActionNetwork upsert_person argument,https://api.github.com/repos/move-coop/parsons/issues/915,Austin Weisgrau,915,closed,2023-11-07 23:17:16+00:00,2023-12-19 23:08:08+00:00,2023-12-19 23:05:50+00:00,"Existing documentation is wrong. When email addresses are passed as described in a dict, they are ignored by the method.

I added a type check that will raise an error if a dict is passed, in order to make it more clear to the user that this is an invalid argument.

I also updated the documentation to clarify that a list of dicts is acceptable but not a plain dict. Also, the keys described for the dict are wrong - those are also updated.

See https://actionnetwork.org/docs/v2/people/","[Label(name=""breaking change"")]",[],1
277,Google BigQuery - Clean Up Autodetect Logic,https://api.github.com/repos/move-coop/parsons/issues/914,Ian,914,closed,2023-11-02 17:26:42+00:00,2023-11-07 18:45:39+00:00,2023-11-02 19:44:26+00:00,The job config logic was written such that incoming `job_config` objects were set to `autodetect=True` ... we cleaned up the logic that's applied such that an explicitly passed-in schema will take precedence,[],"[NamedUser(login=""IanRFerguson"")]",0
278,BigQuery - Add row count function to connector,https://api.github.com/repos/move-coop/parsons/issues/913,Ian,913,closed,2023-10-31 18:48:29+00:00,2023-12-08 16:47:56+00:00,2023-12-08 16:47:52+00:00,"Adds a `get_row_count` function to the BigQuery connector. The native version of this function in the BigQuery API client only works with tables, wondering if we should stick with that logic here and add another function for views",[],"[NamedUser(login=""IanRFerguson"")]",3
279,New connector for working with the Catalist Match API,https://api.github.com/repos/move-coop/parsons/issues/912,Austin Weisgrau,912,closed,2023-10-26 17:42:05+00:00,2024-02-05 19:08:36+00:00,2023-11-14 20:41:22+00:00,"# Connector for working with the Catalist Match API.

The Catalist Match tool requires OAuth2.0 client credentials for the API as well as
credentials for accessing the Catalist sftp bucket. Each Catalist client is given
their own bucket alias named after a tree species, used for constructing the
filepath within the sftp bucket.

Accessing the Catalist sftp bucket and Match API both require the source IP address
to be explicitly white-listed by Catalist.

Example usage:
```
tbl = Table.from_csv(...)
client = CatalistMatch(...)
match_result = client.match(tbl)
```


# Tests
Tests are included in this PR.

# Changes to parsons
Two changes are made to the parsons package outside of this new connector.

28949a9a20fe4422aae3b0ac6373e912c61d4b4c makes a small change to the api_connector error handler to look for error messages in a response `text` attribute, where the error message is found in responses from the Catalist API.

1ffb04631bc79e99bbd523bdf314ba179849104a adds a development dependency `pytest-mock` to support the ergonomic use of mocking in pytest. This dependency is then used in the tests on the catalist match connector in 31f3a1816cdb09d5004fb11f70d1bd6da0182cd1.

# Expected future changes
Following a community development session, JSON responses from the API will be parsed using python dataclasses. Currently JSON responses are returned raw, which matches the normal pattern across parsons connectors.",[],"[NamedUser(login=""IanRFerguson"")]",4
280,BigQuery - Add Column Helpers,https://api.github.com/repos/move-coop/parsons/issues/911,Ian,911,closed,2023-10-26 16:00:56+00:00,2023-10-30 18:35:08+00:00,2023-10-30 18:35:03+00:00,Quick PR to add two column helpers to the BigQuery connector,[],"[NamedUser(login=""IanRFerguson"")]",0
281,[Feature/Addition] Add Default Timeout to BigQuery Database Connections,https://api.github.com/repos/move-coop/parsons/issues/910,,910,open,2023-10-24 00:06:56+00:00,2023-12-19 22:22:28+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
Set a default `timeout` parameter for BigQuery queries.

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->

Per [Google BQ query job documentation](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.job.QueryJob#google_cloud_bigquery_job_QueryJob_result), it's possible to pass a default timeout parameter to the synchronous call that awaits a response.

This change would require choosing a reasonable maximum default (30s?), setting that up as a configurable parameter, and passing that parameter into `load_job.result`.


## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->

Generally, it's best practice to establish a timeout for database connections, to prevent code from indefinitely waiting for a response under unreliable network conditions. 

Per https://github.com/move-coop/parsons/issues/452, there was existing low-priority work to set this up with the built-in SQL connector.  Having a well-chosen default timeout and good notifications around errors will make everything dependent on parsons more robust.

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->

Should be as easy as changing 
https://github.com/move-coop/parsons/blob/55d371712193287fa775b23299e6e65756a26a17/parsons/google/google_bigquery.py#L417

and 
https://github.com/move-coop/parsons/blob/55d371712193287fa775b23299e6e65756a26a17/parsons/google/google_bigquery.py#L572

to `load_job.result(timeout=MAX_TIMEOUT)`

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->

High -- this is a very low lift now, but becomes much harder to implement and test once more code becomes dependent on the new BQ connector.","[Label(name=""enhancement""), Label(name=""high priority""), Label(name=""connector update"")]",[],0
282,[Feature/Addition] BigQuery get_columns_list() using information schema,https://api.github.com/repos/move-coop/parsons/issues/909,elyse-weiss,909,closed,2023-10-19 15:03:51+00:00,2023-10-30 18:35:20+00:00,2023-10-30 18:35:20+00:00,"A BigQuery version of `Redshift.get_columns_list()` leveraging the information schema to save costs.


## Detailed Description
Idea here is to get same result as `get_columns_list()`, but without querying the table directly to save money in BigQuery!


## Context
TMC leverages this function a lot in Redshift, so we'd like it in BigQuery too!


## Possible Implementation
Shared above.


## Priority
High

CC: @shaunagm as this may be duplicative of other issues on bringing BigQuery up to parity with Redshift","[Label(name=""enhancement"")]","[NamedUser(login=""IanRFerguson"")]",2
283,[Feature/Addition] BigQuery copy table between projects,https://api.github.com/repos/move-coop/parsons/issues/908,elyse-weiss,908,open,2023-10-19 15:00:59+00:00,2023-12-19 22:22:41+00:00,,"A simple function to move a table between BigQuery projects.

## Detailed Description


## Context
This is definitely helpful to TMC staff who may be working in Dev or TMC projects, and want to move tables to Prod or Member projects.

## Possible Implementation
Google CLI code from @IanRFerguson:
`bq cp {{ project1 }}:dataset1.table1 {{ project2 }}:dataset2.table2`


## Priority
Medium","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update"")]",[],0
284,Fix airtable.insert_records table arg,https://api.github.com/repos/move-coop/parsons/issues/907,Cody Gordon,907,closed,2023-10-19 00:25:05+00:00,2023-11-01 12:49:35+00:00,2023-11-01 12:49:35+00:00,"It seems that `airtable.batch_insert` was used without any conversion to a list from the table arg, and [expects a raw list](https://airtable-python-wrapper.readthedocs.io/en/master/api.html#airtable.Airtable.batch_insert), but this parsons method expects a parsons table. So, either sending a table or list in would cause an error, though sending the list would actually insert the rows into Airtable but throw the error on the final `table.num_rows` log.

This PR allows for either to work!

Errors for reference:

Table source:
```
  File ""/root/.pyenv/versions/letters-comments-vetting-ve/lib/python3.9/site-packages/airtable/airtable.py"", line 154, in _chunk
    for i in range(0, len(iterable), chunk_size):
TypeError: object of type 'Table' has no len()
```
list source:
```
  File ""/root/.pyenv/versions/letters-comments-vetting-ve/lib/python3.9/site-packages/parsons/airtable/airtable.py"", line 147, in insert_records
    logger.info(f'{table.num_rows} records inserted.')
AttributeError: 'list' object has no attribute 'num_rows'
```",[],"[NamedUser(login=""sharinetmc"")]",1
285,Fix airtable.insert_records table arg type,https://api.github.com/repos/move-coop/parsons/issues/906,Cody Gordon,906,closed,2023-10-19 00:16:14+00:00,2023-10-19 00:23:41+00:00,2023-10-19 00:22:32+00:00,"It seems that `airtable.batch_insert` was used without any conversion to a list from the table arg, and [expects a raw list](https://airtable-python-wrapper.readthedocs.io/en/master/api.html#airtable.Airtable.batch_insert), but this parsons method expects a parsons table. So, either sending a table or list in would cause an error, though sending the list would actually insert the rows into Airtable but throw the error on the final `table.num_rows` log.

This PR allows for either to work!

Errors for reference:

Table source:
```
  File ""/root/.pyenv/versions/letters-comments-vetting-ve/lib/python3.9/site-packages/airtable/airtable.py"", line 154, in _chunk
    for i in range(0, len(iterable), chunk_size):
TypeError: object of type 'Table' has no len()
```
list source:
```
  File ""/root/.pyenv/versions/letters-comments-vetting-ve/lib/python3.9/site-packages/parsons/airtable/airtable.py"", line 147, in insert_records
    logger.info(f'{table.num_rows} records inserted.')
AttributeError: 'list' object has no attribute 'num_rows'
```",[],[],0
286,[Feature/Addition] Add methods for messages to Action Network connector,https://api.github.com/repos/move-coop/parsons/issues/905,,905,closed,2023-10-18 19:10:54+00:00,2023-11-13 14:42:32+00:00,2023-11-13 14:42:32+00:00,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
There is a messages endpoint which allows the ability to access mass emails sent. Building a get_messages() and get_message() method would allow people to create analysis of emails outside of the Action Network platform. It can return body text, subject lines, etc. Allowing for complex analysis not possible inside the software itself. 

Here is the API documentation: https://actionnetwork.org/docs/v2/messages 


## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->


## Context
<!--- Why is this change important to you? How would you use it? -->
I created a similar process for EveryAction and use it in analysis of donations per email, etc. 
<!--- How can it benefit other users? -->


## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->


## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
Medium. I actually already built it and I am creating an issue to allow others to comment on it. ","[Label(name=""enhancement"")]",[],1
287,Bump urllib3 from 1.26.17 to 1.26.18,https://api.github.com/repos/move-coop/parsons/issues/904,,904,closed,2023-10-18 01:31:38+00:00,2023-11-01 17:25:36+00:00,2023-11-01 17:25:24+00:00,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.17 to 1.26.18.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>
<blockquote>
<h2>1.26.18</h2>
<ul>
<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses. (GHSA-g4mx-q9vg-27p4)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>
<blockquote>
<h1>1.26.18 (2023-10-17)</h1>
<ul>
<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses.</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/urllib3/urllib3/commit/9c2c2307dd1d6af504e09aac0326d86ee3597a0b""><code>9c2c230</code></a> Release 1.26.18 (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3159"">#3159</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/b594c5ceaca38e1ac215f916538fb128e3526a36""><code>b594c5c</code></a> Merge pull request from GHSA-g4mx-q9vg-27p4</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/944f0eb134485f41bc531be52de12ba5a37bca73""><code>944f0eb</code></a> [1.26] Use vendored six in urllib3.contrib.securetransport</li>
<li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.17...1.26.18"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.17&new-version=1.26.18)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]","[NamedUser(login=""sharinetmc"")]",0
288,implement new hustle api call,https://api.github.com/repos/move-coop/parsons/issues/903,Nathan Mannes,903,closed,2023-10-09 17:31:55+00:00,2023-12-08 22:05:30+00:00,2023-12-08 22:05:30+00:00,"This is my first PR, so i figured I'd do something pretty basic. I just implemented an additional API call in the Hustle module in accordance with their docs that I found in something tagged as a good first issue

- [issue](https://github.com/move-coop/parsons/issues/538)
- [api docs](https://api.hustle.com/docs/#operation/createGroupMembership)","[Label(name=""🎉 first PR"")]",[],5
289,Add Empower Connector,https://api.github.com/repos/move-coop/parsons/issues/902,,902,open,2023-10-07 21:50:57+00:00,2024-11-12 15:23:24+00:00,,"This connector adds support for the [Empower](https://empowerproject.us/empower-app/) relational app. As of my most recent testing, it includes support for all of the different json objects that are included in their API.","[Label(name=""new connector"")]","[NamedUser(login=""sharinetmc"")]",4
290,Bump urllib3 from 1.26.5 to 1.26.17,https://api.github.com/repos/move-coop/parsons/issues/901,,901,closed,2023-10-03 04:38:16+00:00,2023-10-11 13:04:05+00:00,2023-10-11 13:03:55+00:00,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.5 to 1.26.17.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>
<blockquote>
<h2>1.26.17</h2>
<ul>
<li>Added the <code>Cookie</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>. (GHSA-v845-jxx5-vc9f)</li>
</ul>
<h2>1.26.16</h2>
<ul>
<li>Fixed thread-safety issue where accessing a <code>PoolManager</code> with many distinct origins would cause connection pools to be closed while requests are in progress (<a href=""https://redirect.github.com/urllib3/urllib3/issues/2954"">#2954</a>)</li>
</ul>
<h2>1.26.15</h2>
<ul>
<li>Fix socket timeout value when HTTPConnection is reused (<a href=""https://redirect.github.com/urllib3/urllib3/issues/2645"">urllib3/urllib3#2645</a>)</li>
<li>Remove &quot;!&quot; character from the unreserved characters in IPv6 Zone ID parsing (<a href=""https://redirect.github.com/urllib3/urllib3/issues/2899"">urllib3/urllib3#2899</a>)</li>
<li>Fix IDNA handling of 'x80' byte (<a href=""https://redirect.github.com/urllib3/urllib3/issues/2901"">urllib3/urllib3#2901</a>)</li>
</ul>
<h2>1.26.14</h2>
<ul>
<li>Fixed parsing of port 0 (zero) returning None, instead of 0 (<a href=""https://redirect.github.com/urllib3/urllib3/issues/2850"">#2850</a>)</li>
<li>Removed deprecated <code>HTTPResponse.getheaders()</code> calls in <code>urllib3.contrib</code> module.</li>
</ul>
<h2>1.26.13</h2>
<ul>
<li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>
<li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected even when the port number after removing the zeroes was valid.</li>
<li>Fixed a deprecation warning when using cryptography v39.0.0.</li>
<li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>
</ul>
<h2>1.26.12</h2>
<ul>
<li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module. Both will be removed in v2.x. See this <a href=""https://redirect.github.com/urllib3/urllib3/issues/2680"">GitHub issue</a> for justification and info on how to migrate.</li>
</ul>
<h2>1.26.11</h2>
<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>
<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>
<ul>
<li>Fixed an issue where reading more than 2 GiB in a call to HTTPResponse.read would raise an OverflowError on Python 3.9 and earlier.</li>
</ul>
<h2>1.26.10</h2>
<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>
<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>
<p>:closed_lock_with_key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>
<ul>
<li>Removed support for Python 3.5</li>
<li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>
</ul>
<h2>1.26.9</h2>
<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>
<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>
<blockquote>
<h1>1.26.17 (2023-10-02)</h1>
<ul>
<li>Added the <code>Cookie</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>. (<code>[#3139](https://github.com/urllib3/urllib3/issues/3139) &lt;https://github.com/urllib3/urllib3/pull/3139&gt;</code>_)</li>
</ul>
<h1>1.26.16 (2023-05-23)</h1>
<ul>
<li>Fixed thread-safety issue where accessing a <code>PoolManager</code> with many distinct origins
would cause connection pools to be closed while requests are in progress (<code>[#2954](https://github.com/urllib3/urllib3/issues/2954) &lt;https://github.com/urllib3/urllib3/pull/2954&gt;</code>_)</li>
</ul>
<h1>1.26.15 (2023-03-10)</h1>
<ul>
<li>Fix socket timeout value when <code>HTTPConnection</code> is reused (<code>[#2645](https://github.com/urllib3/urllib3/issues/2645) &lt;https://github.com/urllib3/urllib3/issues/2645&gt;</code>__)</li>
<li>Remove &quot;!&quot; character from the unreserved characters in IPv6 Zone ID parsing
(<code>[#2899](https://github.com/urllib3/urllib3/issues/2899) &lt;https://github.com/urllib3/urllib3/issues/2899&gt;</code>__)</li>
<li>Fix IDNA handling of '\x80' byte (<code>[#2901](https://github.com/urllib3/urllib3/issues/2901) &lt;https://github.com/urllib3/urllib3/issues/2901&gt;</code>__)</li>
</ul>
<h1>1.26.14 (2023-01-11)</h1>
<ul>
<li>Fixed parsing of port 0 (zero) returning None, instead of 0. (<code>[#2850](https://github.com/urllib3/urllib3/issues/2850) &lt;https://github.com/urllib3/urllib3/issues/2850&gt;</code>__)</li>
<li>Removed deprecated getheaders() calls in contrib module. Fixed the type hint of <code>PoolKey.key_retries</code> by adding <code>bool</code> to the union. (<code>[#2865](https://github.com/urllib3/urllib3/issues/2865) &lt;https://github.com/urllib3/urllib3/issues/2865&gt;</code>__)</li>
</ul>
<h1>1.26.13 (2022-11-23)</h1>
<ul>
<li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>
<li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected
even when the port number after removing the zeroes was valid.</li>
<li>Fixed a deprecation warning when using cryptography v39.0.0.</li>
<li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>
</ul>
<h1>1.26.12 (2022-08-22)</h1>
<ul>
<li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module.
Both will be removed in v2.x. See this <code>GitHub issue &lt;https://github.com/urllib3/urllib3/issues/2680&gt;</code>_
for justification and info on how to migrate.</li>
</ul>
<h1>1.26.11 (2022-07-25)</h1>
<ul>
<li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would
raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>
</ul>
<h1>1.26.10 (2022-07-07)</h1>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/urllib3/urllib3/commit/c9016bf464751a02b7e46f8b86504f47d4238784""><code>c9016bf</code></a> Release 1.26.17</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/01220354d389cd05474713f8c982d05c9b17aafb""><code>0122035</code></a> Backport GHSA-v845-jxx5-vc9f (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3139"">#3139</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/e63989f97d206e839ab9170c8a76e3e097cc60e8""><code>e63989f</code></a> Fix installing <code>brotli</code> extra on Python 2.7</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/2e7a24d08713a0131f0b3c7197889466d645cc49""><code>2e7a24d</code></a> [1.26] Configure OS for RTD to fix building docs</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/57181d6ea910ac7cb2ff83345d9e5e0eb816a0d0""><code>57181d6</code></a> [1.26] Improve error message when calling urllib3.request() (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3058"">#3058</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/3c0148048a523325819377b23fc67f8d46afc3aa""><code>3c01480</code></a> [1.26] Run coverage even with failed jobs</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/d94029b7e2193ff47b627906a70e06377a09aae8""><code>d94029b</code></a> Release 1.26.16</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/18e92145e9cddbabdf51c98f54202aa37fd5d4c8""><code>18e9214</code></a> Use trusted publishing for PyPI</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/d25cf83bbae850a290fe34ed1610ae55c0558b36""><code>d25cf83</code></a> [1.26] Fix invalid test_ssl_failure_midway_through_conn</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/25cca389496b86ee809c21e5b641aeaa74809263""><code>25cca38</code></a> [1.26] Fix test_ssl_object_attributes</li>
<li>Additional commits viewable in <a href=""https://github.com/urllib3/urllib3/compare/1.26.5...1.26.17"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.5&new-version=1.26.17)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
291,Nir's actionnetwork changes,https://api.github.com/repos/move-coop/parsons/issues/900,,900,closed,2023-10-01 10:17:14+00:00,2023-10-26 17:58:26+00:00,2023-10-26 17:58:13+00:00,"Added some get functions for ActionNetwork module, I have more functions to add (create and update functions) once this batch is approved.
Please let me know if it looks ok :)","[Label(name=""🎉 first PR"")]",[],4
292,[Bug] setting notify = False in gs.share_spreadsheet() doesn't work,https://api.github.com/repos/move-coop/parsons/issues/899,elyse-weiss,899,open,2023-09-29 14:44:38+00:00,2023-11-21 21:29:58+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
I had set `notify = False` in `share_spreadsheet()`. I still received an email that I was shared on the spreadsheet. Seems like this piece isn't working well.

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
https://move-coop.github.io/parsons/html/stable/_modules/parsons/google/google_sheets.html#GoogleSheets.share_spreadsheet

## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Seems very low priority.","[Label(name=""bug""), Label(name=""low priority"")]",[],0
293,Nirs actionnetwork changes,https://api.github.com/repos/move-coop/parsons/issues/898,,898,closed,2023-09-28 14:37:31+00:00,2023-09-28 14:42:56+00:00,2023-09-28 14:42:56+00:00,,[],[],0
294,Check for empty tables in zoom poll results,https://api.github.com/repos/move-coop/parsons/issues/897,Jason,897,closed,2023-09-27 20:27:47+00:00,2023-10-02 14:30:18+00:00,2023-10-02 14:30:06+00:00,"The new Zoom poll code fails when trying to get results for a meeting/webinar that does not return poll data. This commit checks that there is poll data before trying to apply the table transformations.

```
Traceback (most recent call last):
  File ""parsons\parsons\zoom\zoom.py"", line 509, in get_meeting_poll_results
    return self.__process_poll_results(tbl=tbl)
  File ""parsons\parsons\zoom\zoom.py"", line 159, in __process_poll_results
    tbl = tbl.unpack_nested_columns_as_rows(
  File ""parsons\parsons\etl\etl.py"", line 576, in unpack_nested_columns_as_rows
    ignore_cols.remove(column)
ValueError: list.remove(x): x not in list
```",[],[],0
295,Add MobileCommons Connector,https://api.github.com/repos/move-coop/parsons/issues/896,Cormac Martinez del Rio,896,closed,2023-09-26 18:20:23+00:00,2023-10-25 17:39:39+00:00,2023-10-25 17:39:29+00:00,"There's definitely room for more methods, so this is just a starting place for others to build off of! This is my first time adding a new class, so appreciate all the feedback you can provide. ","[Label(name=""new connector"")]",[],0
296,Add Mobile Commons Connector,https://api.github.com/repos/move-coop/parsons/issues/895,Cormac Martinez del Rio,895,closed,2023-09-26 17:59:21+00:00,2023-09-26 18:11:20+00:00,2023-09-26 18:11:20+00:00,"There aren't that many methods, but it's a starting place upon which others can build! ",[],[],0
297,Update release,https://api.github.com/repos/move-coop/parsons/issues/894,Kasia Hinkson,894,closed,2023-09-26 17:18:43+00:00,2023-09-26 17:19:24+00:00,2023-09-26 17:19:24+00:00,,[],[],0
298,"[Bug] FAILED test/test_etl.py::TestParsonsTable::test_from_petl - TypeError: The only supported seed types are: None,",https://api.github.com/repos/move-coop/parsons/issues/893,,893,closed,2023-09-26 14:21:46+00:00,2023-10-01 09:20:18+00:00,2023-10-01 09:20:18+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->


## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->

Tried to follow the steps at https://move-coop.github.io/parsons/html/stable/contributing#contributing-code-to-parsons
When I try to run the Unit tests (pytest -rf test/) I get an error on the test_etl.py.
Did anyone else experienced this issue? Is it OK to start working on changes although this test is failing?
The full output:
============================================================================== FAILURES ===============================================================================
___________________________________________________________________ TestParsonsTable.test_from_petl ___________________________________________________________________

self = <test.test_etl.TestParsonsTable testMethod=test_from_petl>

    def test_from_petl(self):
    
        nrows = 10
        ptbl = petl.dummytable(numrows=nrows)
>       tbl = Table(ptbl)

test/test_etl.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
parsons/etl/table.py:54: in __init__
    if not self.is_valid_table():
parsons/etl/table.py:242: in is_valid_table
    if not self.table:
../venv/lib/python3.11/site-packages/petl/util/base.py:28: in __len__
    return sum(1 for _ in self)
../venv/lib/python3.11/site-packages/petl/util/base.py:28: in <genexpr>
    return sum(1 for _ in self)
../venv/lib/python3.11/site-packages/petl/util/random.py:170: in __iter__
    random.seed(seed)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <random.Random object at 0x11f85b220>, a = datetime.datetime(2023, 9, 26, 17, 13, 18, 984197), version = 2

    def seed(self, a=None, version=2):
        """"""Initialize internal state from a seed.
    
        The only supported seed types are None, int, float,
        str, bytes, and bytearray.
    
        None or no argument seeds from current time or from an operating
        system specific randomness source if available.
    
        If *a* is an int, all bits are used.
    
        For version 2 (the default), all of the bits are used if *a* is a str,
        bytes, or bytearray.  For version 1 (provided for reproducing random
        sequences from older versions of Python), the algorithm for str and
        bytes generates a narrower range of seeds.
    
        """"""
    
        if version == 1 and isinstance(a, (str, bytes)):
            a = a.decode('latin-1') if isinstance(a, bytes) else a
            x = ord(a[0]) << 7 if a else 0
            for c in map(ord, a):
                x = ((1000003 * x) ^ c) & 0xFFFFFFFFFFFFFFFF
            x ^= len(a)
            a = -2 if x == -1 else x
    
        elif version == 2 and isinstance(a, (str, bytes, bytearray)):
            if isinstance(a, str):
                a = a.encode()
            a = int.from_bytes(a + _sha512(a).digest())
    
        elif not isinstance(a, (type(None), int, float, str, bytes, bytearray)):
>           raise TypeError('The only supported seed types are: None,\n'
                            'int, float, str, bytes, and bytearray.')
E           TypeError: The only supported seed types are: None,
E           int, float, str, bytes, and bytearray.

/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/random.py:160: TypeError
======================================================================= short test summary info =======================================================================
FAILED test/test_etl.py::TestParsonsTable::test_from_petl - TypeError: The only supported seed types are: None,
================================================= 1 failed, 710 passed, 188 skipped, 1 xfailed, 12 warnings in 26.71s =================================================

## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->
Try to follow the steps at https://move-coop.github.io/parsons/html/stable/contributing#making-changes-to-parsons

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context




## Priority
ASAP?","[Label(name=""bug"")]",[],1
299,#816 Airtable.get_records() fields argument can be either str or list,https://api.github.com/repos/move-coop/parsons/issues/892,Angela Gloyna,892,closed,2023-09-25 05:12:43+00:00,2023-10-25 18:05:55+00:00,2023-10-25 18:05:55+00:00,Fixes #816 and adds test replicating the issue when passing a single field into the `fields` argument as a string,[],[],1
300,#741 : Deprecate Slack chat.postMessage `as_user` argument and allow for new authorship arguments,https://api.github.com/repos/move-coop/parsons/issues/891,Angela Gloyna,891,closed,2023-09-24 23:13:08+00:00,2023-10-25 18:02:39+00:00,2023-10-25 18:02:22+00:00,"Addresses #741 

This PR adds a deprecation warning for usage of the `as_user` argument in calls to `chat.postMessage`, allows the user to see failed Slack API `deprecated_argument` responses, and facilitates usage of the new Slack authorship customization arguments `username`, `icon_url`, and `icon_emoji` using optional keyword arguments (See: https://api.slack.com/methods/chat.postMessage#authorship)

`as_user` will still be passed on to the slack request and will still fail if the user passes in `as_user=False`, but the user should see more information in the failed response now.","[Label(name=""deprecation"")]",[],1
301,Action Builder Remove Tag Method,https://api.github.com/repos/move-coop/parsons/issues/890,,890,closed,2023-09-14 12:40:32+00:00,2023-09-14 20:56:54+00:00,2023-09-14 20:55:14+00:00,"Removing tags is not an option through the person signup helper endpoint, so this allows us to remove tags by providing certain combinations of arguments (the best being the tag's interact ID and the individual tagging's interact ID, as those are direct components of the DELETE endpoint).","[Label(name=""enhancement""), Label(name=""connector update"")]","[NamedUser(login=""ydamit"")]",0
302,chore(actionnetwork): Edge case and cleaning up type checking,https://api.github.com/repos/move-coop/parsons/issues/889,,889,closed,2023-09-09 01:07:04+00:00,2023-09-14 15:02:03+00:00,2023-09-14 15:02:03+00:00,"Handle edge case where acitonnetwork doesn't return a person_id, and use pythonic style for testing types",[],[],0
303,update zoom docs,https://api.github.com/repos/move-coop/parsons/issues/888,,888,closed,2023-09-08 19:13:27+00:00,2023-09-08 19:24:44+00:00,2023-09-08 19:24:41+00:00,,[],[],0
304,Action Builder Entity Record Insert Name Key Hotfix,https://api.github.com/repos/move-coop/parsons/issues/887,,887,closed,2023-09-06 17:39:45+00:00,2023-09-06 17:48:04+00:00,2023-09-06 17:47:59+00:00,"Action Builder's API is not consistent about what key you need in your data for the name of the entity. For People (and other Entities configured like them) you need at least a `given_name` key. Other Entity types can be configured just to take a single name. These used to be insertable with the `name` key, but now seem to require `action_builder:name`. Rather than trying to force that prefix to be added, I'm just making sure all three keys pass the error handling in `insert_entity_record()`.","[Label(name=""bug fix"")]",[],0
305,Zoom Polls,https://api.github.com/repos/move-coop/parsons/issues/886,Ian,886,closed,2023-09-06 16:51:49+00:00,2023-09-26 20:35:40+00:00,2023-09-26 20:35:35+00:00,"This PR adds the following functions to the `Zoom` connector:
* `get_meeting_poll_metadata` - Returns metadata for a specific poll from a given meeting
* `get_meeting_all_polls_metadata` - Returns metadata for all polls from a given meeting
* `get_past_meeting_poll_metadata` - Returns metadata for a poll from a past meeting
* `get_webinar_poll_metadata` - Returns metadata for a specific poll from a given webinar
* `get_webinar_all_polls_metadata` - Returns metadata for all polls from a given webinar
* `get_past_webinar_poll_metadata` - Returns metadata for a poll from a past webinar
* `get_meeting_poll_results` - Returns poll results from a past meeting
* `get_webinar_poll_results` - Returns poll results from a webinar meeting

[Meeting API Reference](https://developers.zoom.us/docs/api/rest/reference/zoom-api/methods/#tag/Meetings)
[Webinar API Reference](https://developers.zoom.us/docs/api/rest/reference/zoom-api/methods/#tag/Webinars)

I'd like to request that @Jason94 / Indivisible test drive these functions before merging, as I've had a hard time finding polls in past TMC Zoom meetings

---
- To see the specific tasks where the Asana app for GitHub is being used, see below:
  - https://app.asana.com/0/0/1205295200749537","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update""), Label(name=""requires access"")]","[NamedUser(login=""IanRFerguson"")]",1
306,Add canales s3 functions,https://api.github.com/repos/move-coop/parsons/issues/885,,885,closed,2023-09-06 16:20:32+00:00,2023-11-01 12:50:25+00:00,2023-11-01 12:50:19+00:00,,[],[],3
307,Update example VAN apply_response JSON to reflect accurate keys,https://api.github.com/repos/move-coop/parsons/issues/884,,884,closed,2023-09-05 16:59:50+00:00,2023-09-14 14:49:06+00:00,2023-09-14 14:48:55+00:00,"Current example shows JSON with `SurveyResponse` as the value for the key `action`, when that key should actually be `type` instead.","[Label(name=""documentation"")]",[],0
308,[Bug] NewMode Connector rate limit,https://api.github.com/repos/move-coop/parsons/issues/883,Sade Chessidy,883,open,2023-08-30 19:09:53+00:00,2023-12-19 22:18:02+00:00,,"When I try to use the get_outreach() more than 30 times (in a loop) I get a CSRF Token error, potentially suggesting a limit to the number of consecutive calls that can be made.


## Detailed Description
In my example code below if I give it a list of 100 outreach_ids, i'd expect it to return all the associated target_ids. But it's breaking after the 29th loop. The error message that I am getting is:

<img width=""481"" alt=""Screen Shot 2023-08-28 at 1 07 58 PM"" src=""https://github.com/move-coop/parsons/assets/85197581/933383d8-67cb-43aa-81db-d02af595fbb9"">

I think the 'NoneType' error is a red herring because If I run the 30th outreach_id on it's own I get the expected results.

I should also add that switching from using the Parsons connector to using the functions/class in Canales fixed my issue.

## To Reproduce
Example code that I was trying to run.

```
from Parsons import Newmode

def get_target_id(outreach_id):
  newmode = Newmode()
 
  target_id_list = []

  for x in outreach_id:
      data = dict()
      data['outreach_id'] = x
      data['targets'] = newmode.get_outreach(x)['targets']
      target_id_list.append(data)
      
  return target_id_list
```


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4): 
* Operating System and version (desktop or mobile): MacOS Monterey 12.0.1


## Additional Context
Add any other context about the problem here.


## Priority
Low priority, as I said, using the Canales NewMode code instead allowed me to do what I needed to do","[Label(name=""bug""), Label(name=""low priority"")]",[],0
309,v2.0.0,https://api.github.com/repos/move-coop/parsons/issues/882,Ian,882,closed,2023-08-30 17:11:34+00:00,2023-08-30 17:21:13+00:00,2023-08-30 17:21:09+00:00,Changing version number in `setup.py`,[],[],0
310,Prepping for Major Release,https://api.github.com/repos/move-coop/parsons/issues/881,Ian,881,closed,2023-08-29 19:23:40+00:00,2023-08-30 17:03:06+00:00,2023-08-30 17:03:05+00:00,,[],"[NamedUser(login=""IanRFerguson"")]",0
311,Merging Main Before Release,https://api.github.com/repos/move-coop/parsons/issues/880,Ian,880,closed,2023-08-29 17:55:41+00:00,2023-08-29 19:23:02+00:00,2023-08-29 19:23:02+00:00,Merging main into the `major-release` branch before it's deployed,[],"[NamedUser(login=""IanRFerguson"")]",0
312,Generate release notes,https://api.github.com/repos/move-coop/parsons/issues/879,,879,closed,2023-08-29 16:54:35+00:00,2023-09-29 05:52:55+00:00,2023-09-29 05:52:55+00:00,"Added a header to the yml config file to see if it works now. The autogeneraion of release notes doesn't seem to be working, so seeing if this works!",[],[],3
313,add release notes yml,https://api.github.com/repos/move-coop/parsons/issues/878,,878,closed,2023-08-28 08:39:07+00:00,2023-08-29 14:28:13+00:00,2023-08-29 14:28:13+00:00,github configuration yml to automate raw release notes,[],[],0
314,fix: handle 429 error code,https://api.github.com/repos/move-coop/parsons/issues/877,,877,closed,2023-08-26 04:49:35+00:00,2023-09-14 14:20:43+00:00,2023-09-14 14:20:43+00:00,"If you retrieve too many records, auth0 throws a 429 too many requests error, which gets handled naively. this pr throws an exception instead","[Label(name=""enhancement"")]",[],0
315,"Revert ""Enable passing `identifiers` to ActionNetwork `upsert_person()`",https://api.github.com/repos/move-coop/parsons/issues/876,Austin Weisgrau,876,closed,2023-08-22 01:00:57+00:00,2023-12-06 00:03:00+00:00,2023-09-21 12:07:58+00:00,"This reverts most of commit 77ead6079ee03c399bbc83314351bc719ed457c3 / PR #861 

I got more details on the consequences of using an identifier that is not globally unique in ActionNetwork, and was warned by ActionNetwork support that the consequences can be severe. In a more intense way than is mentioned in the official documentation, I was strongly warned against using custom identifiers.

In the PR we merged, documentation on the identifiers feature reports that if an identifier is used that is not globally unique in ActionNetwork, it is simply ignored. However, AN support told me that updates may be made to both the new resource and whatever resource already exists in ActionNetwork that holds that identifier. This behavior may cause updates that violate expectations and it won't be transparent or reported back to the developer that those updates were made. Many users who use identifiers encounter issues of this kind, and for these reasons AN support strongly encourage folks not to use identifiers unless they really have to.

At WFP we are going to be retiring our use of identifiers, because of this feedback and we don't really need them. In general, it does make sense to me that this feature should, by design, be hard to use, so I think we should actually remove this feature from the ActionNetwork connector. 

A note is added to the `upsert_person()` method explaining why identifiers are not included as an option in case a future developer starts to go down the same rabbit hole I have just traversed.","[Label(name=""bug fix"")]",[],4
316,Wraedy/bigquery db connector,https://api.github.com/repos/move-coop/parsons/issues/875,,875,closed,2023-08-20 12:28:36+00:00,2023-10-06 15:20:27+00:00,2023-10-06 15:20:27+00:00,"Three main changes:
- Extends the BigQuery database connector to mimic more of the functionality of the Redshift connector.
- Extends the DB Connector class slightly.
- Moves BigQuery into the databases folder.

Main goal is to prepare the TMC network to migrate scripts from Redshift to BigQuery.","[Label(name=""connector update"")]",[],4
317,Feature/more auth0 functions,https://api.github.com/repos/move-coop/parsons/issues/874,,874,closed,2023-08-20 00:28:35+00:00,2023-12-20 00:03:55+00:00,2023-12-20 00:03:55+00:00,"Add functions for retrieving all users, blocking users, and retrieve connection ids",[],[],1
318,Zoom Authentication + Polling API,https://api.github.com/repos/move-coop/parsons/issues/873,Ian,873,closed,2023-08-17 19:58:16+00:00,2023-12-06 14:17:03+00:00,2023-08-29 17:28:31+00:00,"This PR adds two significant changes to the Zoom connector

## Server to Server OAuth
Zoom is deprecating JWT authentication on September 1st, which necessitates an update to the authentication strategy used by this connector. The `Zoom()` constructor now requires three values to connect to the API:

* `account_id`
* `client_id`
* `client_secret`

These values are passed to the Zoom OAuth callback URL and an access token is assigned to a class attribute.

**Please note** that these values replace the `api_key` and `api_secret` previously passed into the Zoom connector

### Setting Up a Server-to-Server OAuth App

Follow the steps below to generate the required env variables and scopes to authenticate your Zoom connector:

* Go to [Zoom's API Marketplace](https://developers.zoom.us/docs/api/)
* Click  ""Create an App"" -> ""Develop"" -> ""Build an App"" 
* Select **Server-to-Server OAuth**
* Save the client ID, client secret, and account ID somewhere safe that your connector can access
* Under ""scopes"" select the relevant endpoints that you want your connector to access (meetings, users, etc.)

## Zoom Poll API

Per request from @Jason94 and co, we've added functions to query Zoom polls from a given meeting. There are two relevant functions here, one to get a `Table` of all polls for a given meeting (just requires `meeting_id`) and one to get data about one specific poll (requires `meeting_id` AND `poll_id`)

---
- To see the specific tasks where the Asana app for GitHub is being used, see below:
  - https://app.asana.com/0/0/1205295200749537",[],"[NamedUser(login=""IanRFerguson"")]",6
319,change release number,https://api.github.com/repos/move-coop/parsons/issues/872,,872,closed,2023-08-17 16:34:52+00:00,2023-08-17 17:46:31+00:00,2023-08-17 17:46:09+00:00,,[],[],0
320,"Enable use of ""extension"" argument to Redshift unload",https://api.github.com/repos/move-coop/parsons/issues/871,Austin Weisgrau,871,closed,2023-08-17 16:32:41+00:00,2023-09-07 19:41:48+00:00,2023-09-07 19:41:48+00:00,"Extension is a helpful argument. This PR makes it possible to use the extension argument on Redshift.unload()

I have a workflow where I unload  SQL query to S3, and then make the resulting file available to download. It's great if that file already has a proper extension and I don't need to rename the file in S3 first. Currently all Redshift.unload() files have an extension of .gz if the file is compressed or simply ends with `000` if not.",[],[],0
321,Add more ActionKit methods,https://api.github.com/repos/move-coop/parsons/issues/870,Kathy Nguyen,870,closed,2023-08-16 00:28:09+00:00,2023-09-08 19:51:45+00:00,2023-09-08 19:51:45+00:00,"- get_blackholed_email
- blackhole_email
- delete_user_data",[],[],0
322,[Feature/Addition] Enable passing parameters to Redshift unload statement,https://api.github.com/repos/move-coop/parsons/issues/869,Austin Weisgrau,869,open,2023-08-14 19:05:48+00:00,2023-12-19 22:45:26+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
Currently it is not possible to pass parameters to the `Redshift.unload()` method. This is because of a limitation in Redshift itself. See issue [here](https://github.com/aws/amazon-redshift-python-driver/issues/31).

As discussed in the upstream issue, this really should be possible in order to encourage best practices around escaping variables and to avoid SQL injection in unload statements, the same way it is in normal Redshift query statements.

This will not be possible unless either:
- [the upstream issue](https://github.com/aws/amazon-redshift-python-driver/issues/31) in redshift_connector or the Redshift server is fixed
- We migrate from redshift-connector to psycopg2 for the Redshift database driver


## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->
See also https://github.com/sqlalchemy-redshift/sqlalchemy-redshift/issues/215


## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->


## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->


## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->","[Label(name=""enhancement""), Label(name=""low priority"")]",[],2
323,add pypika to requirements,https://api.github.com/repos/move-coop/parsons/issues/868,elyse-weiss,868,closed,2023-08-14 14:24:43+00:00,2023-08-17 14:59:01+00:00,2023-08-17 14:58:57+00:00,,[],[],2
324,add pypika,https://api.github.com/repos/move-coop/parsons/issues/867,elyse-weiss,867,closed,2023-08-11 13:59:11+00:00,2023-08-14 14:22:46+00:00,2023-08-14 14:22:26+00:00,we are now using pypika at TMC and just want to add this to our docker image!,[],[],1
325,Airtable switch from API key to personal access token,https://api.github.com/repos/move-coop/parsons/issues/866,Kathy Nguyen,866,closed,2023-08-10 21:25:51+00:00,2023-09-07 19:30:57+00:00,2023-09-07 19:30:57+00:00,"Fixes #788 

The authentication process using Airtable API key vs personal access token is the same. Switching the name from `api_key` to `personal_access_token` to reflect the fact that Airtable will be deprecating API keys and a personal access token should be used instead: https://community.airtable.com/t5/announcements/new-api-capabilities-now-in-ga-and-upcoming-api-keys-deprecation/ba-p/141824",[],[],5
326,Add Events Helpers to PDI Connector,https://api.github.com/repos/move-coop/parsons/issues/865,Ian,865,closed,2023-08-09 16:47:06+00:00,2023-08-16 19:57:49+00:00,2023-08-16 19:56:45+00:00,Adding helpers written by @cmdelrio to the `Events()` object. These functions are inherited by the `PDI()` connector and will be available post-authentication,"[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]","[NamedUser(login=""IanRFerguson"")]",2
327,Add pre-commit hook config to run flake8 and black on commit,https://api.github.com/repos/move-coop/parsons/issues/864,Austin Weisgrau,864,closed,2023-08-07 18:19:03+00:00,2023-08-28 08:07:14+00:00,2023-08-10 20:47:36+00:00,"Notes added to README on how to install and set up

Resolves #863 ","[Label(name=""testing"")]",[],15
328,Feature Request: Automatically run black and flake8 using pre-commits,https://api.github.com/repos/move-coop/parsons/issues/863,Kathy Nguyen,863,closed,2023-08-04 22:41:40+00:00,2023-08-10 20:47:37+00:00,2023-08-10 20:47:37+00:00,"## Detailed Description
Implement something similar to [this tutorial](https://ljvmiranda921.github.io/notebook/2018/06/21/precommits-using-black-and-flake8/) to automatically run black and flake8 using pre-commits.


## Context
I've noticed that often times when contributors open pull requests, the automated tests will fail because it fails the black and/or flake8 test. This issue can be mitigated by automatically running black and flake8 via pre-commits


## Possible Implementation
Something similar to [this tutorial](https://ljvmiranda921.github.io/notebook/2018/06/21/precommits-using-black-and-flake8/)


## Priority
Low priority, nice-to-have","[Label(name=""enhancement"")]",[],0
329,[Feature] Standardize Docker image build,https://api.github.com/repos/move-coop/parsons/issues/862,Giovanni Collazo,862,open,2023-08-03 19:12:06+00:00,2023-12-19 22:17:40+00:00,,"Build and tag Docker images in a predictable way to facilite their usage.


## Detailed Description

- Tag the latest stable release with the version number (`1.1.0`)
- Tag the latest stable release with the `latest` tag
- Tag the head of the main branch with a short git commit hash (`4b9e1af`)
- Tag the head of the main branch with the `prerelease` tag (edited)


## Context

Currently the Docker image is built from `main` and tagged `latest`. This makes the image very unpredictable because it can change at any time.

This has produced issues in the past where a consumer of the Docker image had issues when a bad PR got merged. The image build was triggered and the `latest` image was replaced without a Parsons release.


## Possible Implementation

**Option 1**: Keep the current Docker image system using Docker Hub. From my investigation I believe we won't be able to easily implement the ""short git commit hash"". This option is not available on Docker Hub

Based on what I've learned about Docker hub build this is the required setup to implement the tags we want with the exception of the short git commit hash tag.

| Source Type | Source        | Docker Tag    | Dockerfile   | Context | Build | Cache | Example  |
| ----------- | ------------- | ------------- | ------------ | ------- | ----- | ---- | ------------ |
| Branch      | `main`        | `prerelease`  | `Dockerfile` | `/`     | TRUE  | TRUE | `prerelease` |
| Tag         | `/^[0-9.]+$/` | `latest`      | `Dockerfile` | `/`     | TRUE  | TRUE | `latest`     |
| Tag         | `/^[0-9.]+$/` | `{sourceref}` | `Dockerfile` | `/`     | TRUE  | TRUE | `1.2.3`      |

**Option 2**: Build and tag the image in the CI/CD pipeline. This is my preferred option because it will enable us to set all the tags we want.

## Priority

I think this is a `low` priority feature because as far as I know there is only one consumer of the Docker image. One could argue that making the images more predictable might incentivize more people to use it but that's speculation.

## Open Questions

Currently it looks like the project is using CicleCI for CI/CD but I recently say [an issue](https://github.com/move-coop/parsons/issues/860) that looks like the project is also using GitHub actions.

Should this be implemented in the existing Circle CI pipeline or on top of GitHub actions?
","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""testing"")]",[],1
330,Enable passing `identifiers` to ActionNetwork `upsert_person()`,https://api.github.com/repos/move-coop/parsons/issues/861,Austin Weisgrau,861,closed,2023-08-02 19:19:03+00:00,2023-08-28 08:07:48+00:00,2023-08-10 16:54:22+00:00,"`identifiers` is a useful method when upserting a person, for either matching to an existing contact or creating a new contact.

The `**kwargs` keyword arguments on upsert_person() are passed as custom fields and so excludes the ability to add identifiers as additional keyword arguments without adding this functionality.

Small code readability improvements are also made to the `get_tags()` method in this PR by removing code that didn't do anything.","[Label(name=""connector update"")]",[],4
331,Add Windows tests to CI setup,https://api.github.com/repos/move-coop/parsons/issues/860,Shauna Gordon-McKeon,860,closed,2023-07-31 20:01:13+00:00,2024-07-25 19:23:30+00:00,2024-07-25 19:23:30+00:00,"We currently run the Parsons test (and install & linting process) on Mac and Ubuntu. We can fairly easily add Windows testing (see below for instructions) but doing so reveals some underlying issues on Windows. 

To address this issue, one needs to:

- [ ] uncomment the lines in `.github/workflows/test-linux-windows.yml` and add `windows-latest` back into the list of OS's, then submit a PR for it
- [ ] this will run the windows tests which will fail, you can use the failures to debug the problems
- [ ] once all the problems are fixed, add them to the PR, and signal it's ready for review

Once we have Windows tests, and they are passing, we can merge the PR.

","[Label(name=""bug""), Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""testing"")]",[],0
332,Get and Update ActionKit orderrecurring,https://api.github.com/repos/move-coop/parsons/issues/859,Alex French,859,closed,2023-07-27 17:44:19+00:00,2023-08-28 08:08:39+00:00,2023-07-27 19:35:49+00:00,"Add methods to get and to update `orderrecurring` objects in ActionKit, and add corresponding tests.","[Label(name=""connector update"")]",[],0
333,Add multiple python versions to CI tests,https://api.github.com/repos/move-coop/parsons/issues/858,Shauna Gordon-McKeon,858,closed,2023-07-26 19:37:54+00:00,2023-08-28 08:10:17+00:00,2023-08-04 21:53:38+00:00,"We had a pair programming session where we attempted to address #790 within CircleCI, that was unsuccessful so we switched to Github Actions.

- [x] get tests working in github actions
- [x] add caching
- [x] move Windows error to new issue #860 
  - see [this windows error](https://github.com/move-coop/parsons/actions/runs/5718559743/job/15494634900?pr=858)
- [x] check usage requirements to make sure we won't run out of minutes/storage
   - a single run of the tests (both Linux and Mac) is about 5 minutes, and we seem to have 2000 free minutes total, so we have about 400 runs per month or an average of 12-13 test runs per day average. that may not be enough but it may be fine? Buying additional minute seems to cost about $10 per thousand minutes so it's not a huge deal if we overrun occasionally
- [x] check that the docs build stuff is still working on circle CI","[Label(name=""testing"")]",[],1
334,NewMode - Add bulk `targets` endpoint,https://api.github.com/repos/move-coop/parsons/issues/857,Ian,857,closed,2023-07-17 16:00:57+00:00,2023-07-27 13:44:50+00:00,2023-07-27 13:44:47+00:00,"This PR adds an additional `GET` request to the NewMode client that allows ALL targets to be queried and returned as a table. The addition of this function will give Parsons users flexibility in their API requests to NewMode, as they may be interested in exploring target data broadly and across outreaches.

---
- To see the specific tasks where the Asana app for GitHub is being used, see below:
  - https://app.asana.com/0/0/1204935310402358","[Label(name=""low priority""), Label(name=""connector update"")]","[NamedUser(login=""IanRFerguson"")]",8
335,Update setup.py with new release number,https://api.github.com/repos/move-coop/parsons/issues/856,Kasia Hinkson,856,closed,2023-07-17 12:46:25+00:00,2023-11-01 15:59:31+00:00,2023-11-01 15:59:31+00:00,,[],[],0
336,[Feature/Addition] SQLite database connector,https://api.github.com/repos/move-coop/parsons/issues/855,Josh Fayer,855,open,2023-07-12 14:01:20+00:00,2023-12-19 22:11:50+00:00,,"I'm thinking of trying to implement this but just wanted to put up an issue for comments and feedback from the community and to see if anyone else had an interest in contributing!

SQLite is a performant flat-file database that's often touted as the ""zero-config"" database - you can even run it entirely in memory in your program! Python's standard library already includes a [SQLite connector](https://docs.python.org/3/library/sqlite3.html), meaning the addition of a SQLite Parsons database connector should add no additional dependencies to the library.

## Detailed Description
There should be a SQLite database connector with full parity (to the extent that the SQLite library allows, there are some limitations) with other database connectors.

## Context
Parsons already exposes functionality to export tables as CSVs for longterm storage, but SQLite could fill a gap where an analysis might involve multiple tables or be too complicated or large for CSVs to be practical, but setting up a whole database would be overly burdensome or not necessary.

## Priority
Low Priority","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""new connector"")]",[],0
337,Feature/remove an tags,https://api.github.com/repos/move-coop/parsons/issues/854,,854,open,2023-07-08 01:27:36+00:00,2023-07-25 19:19:32+00:00,,Adds `remove_tags` as an optional field to actionnetwork `upsert_person`,[],[],3
338,Feature/background processing,https://api.github.com/repos/move-coop/parsons/issues/853,,853,closed,2023-07-08 01:03:39+00:00,2023-08-28 08:10:55+00:00,2023-07-24 18:18:11+00:00,Adds an optional argument to use ActionNetwork's background processing feature: https://actionnetwork.org/docs/v2/#background-processing,"[Label(name=""connector update"")]",[],5
339,Bump grpcio from 1.51.1 to 1.53.0,https://api.github.com/repos/move-coop/parsons/issues/852,,852,closed,2023-07-05 21:45:46+00:00,2023-07-24 18:26:59+00:00,2023-07-24 18:26:51+00:00,"Bumps [grpcio](https://github.com/grpc/grpc) from 1.51.1 to 1.53.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/grpc/grpc/releases"">grpcio's releases</a>.</em></p>
<blockquote>
<h2>Release v1.53.0</h2>
<p>This is release 1.53.0 (<a href=""https://github.com/grpc/grpc/blob/master/doc/g_stands_for.md"">glockenspiel</a>) of gRPC Core.</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<p>This release contains refinements, improvements, and bug fixes, with highlights listed below.</p>
<h2>Core</h2>
<ul>
<li>xDS: fix crash when removing the last endpoint from the last locality in weighted_target. (<a href=""https://redirect.github.com/grpc/grpc/pull/32592"">#32592</a>)</li>
<li>filter stack: pass peer name up via recv_initial_metadata batch. (<a href=""https://redirect.github.com/grpc/grpc/pull/31933"">#31933</a>)</li>
<li>[EventEngine] Add advice against blocking work in callbacks. (<a href=""https://redirect.github.com/grpc/grpc/pull/32397"">#32397</a>)</li>
<li>[http2] Dont drop connections on metadata limit exceeded. (<a href=""https://redirect.github.com/grpc/grpc/pull/32309"">#32309</a>)</li>
<li>xDS: reject aggregate cluster with empty cluster list. (<a href=""https://redirect.github.com/grpc/grpc/pull/32238"">#32238</a>)</li>
<li>Fix Python epoll1 Fork Support. (<a href=""https://redirect.github.com/grpc/grpc/pull/32196"">#32196</a>)</li>
<li>server: introduce ServerMetricRecorder API and move per-call reporting from a C++ interceptor to a C-core filter. (<a href=""https://redirect.github.com/grpc/grpc/pull/32106"">#32106</a>)</li>
<li>[EventEngine] Add invalid handle types to the public API. (<a href=""https://redirect.github.com/grpc/grpc/pull/32202"">#32202</a>)</li>
<li>[EventEngine] Refactoring the EventEngine Test Suite: Part 1. (<a href=""https://redirect.github.com/grpc/grpc/pull/32127"">#32127</a>)</li>
<li>xDS: fix WeightedClusters total weight handling. (<a href=""https://redirect.github.com/grpc/grpc/pull/32134"">#32134</a>)</li>
</ul>
<h2>C++</h2>
<ul>
<li>Update minimum MSVC version to 2019. (<a href=""https://redirect.github.com/grpc/grpc/pull/32615"">#32615</a>)</li>
<li>Use CMake variables for paths in pkg-config files. (<a href=""https://redirect.github.com/grpc/grpc/pull/31671"">#31671</a>)</li>
</ul>
<h2>C#</h2>
<ul>
<li>Grpc.Tools: Use x86 protoc binaries on arm64 Windows. (<a href=""https://redirect.github.com/grpc/grpc/pull/32017"">#32017</a>)</li>
</ul>
<h2>Python</h2>
<ul>
<li>Support python 3.11 on aarch64. (<a href=""https://redirect.github.com/grpc/grpc/pull/32270"">#32270</a>)</li>
<li>Include .pyi file. (<a href=""https://redirect.github.com/grpc/grpc/pull/32268"">#32268</a>)</li>
<li>De-experimentalize wait-for-ready. (<a href=""https://redirect.github.com/grpc/grpc/pull/32143"">#32143</a>)</li>
<li>De-experimentalize compression. (<a href=""https://redirect.github.com/grpc/grpc/pull/32138"">#32138</a>)</li>
</ul>
<h2>Ruby</h2>
<ul>
<li>[ruby]: add pre-compiled binaries for ruby 3.2; drop them for ruby 2.6. (<a href=""https://redirect.github.com/grpc/grpc/pull/32089"">#32089</a>)</li>
</ul>
<h2>Release v1.53.0-pre2</h2>
<p>This is a prerelease of gRPC Core 1.53.0 (glockenspiel).</p>
<p>For gRPC documentation, see <a href=""https://grpc.io/"">grpc.io</a>. For previous releases, see <a href=""https://github.com/grpc/grpc/releases"">Releases</a>.</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/grpc/grpc/commit/358bfb581feeda5bf17dd3b96da1074d84a6ef8d""><code>358bfb5</code></a> Bump version to 1.53.0 (<a href=""https://redirect.github.com/grpc/grpc/issues/32685"">#32685</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/6e1ebe76d87a2e9b643c08b3e234d374edcd9e92""><code>6e1ebe7</code></a> Backport: Ensure compatibility with the new custom kokoro win2019 image (<a href=""https://redirect.github.com/grpc/grpc/issues/326"">#326</a>...</li>
<li><a href=""https://github.com/grpc/grpc/commit/44a77f6e911b95e1bc2c909b348123b2da2c4375""><code>44a77f6</code></a> Backport 1.53: Update minimum MSVC version to 2019 (<a href=""https://redirect.github.com/grpc/grpc/issues/32615"">#32615</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/c11153cb4ef01ca5f83304b2e28edd0182b3c0d0""><code>c11153c</code></a> backport to 1.53: xDS: fix crash when removing the last endpoint from the las...</li>
<li><a href=""https://github.com/grpc/grpc/commit/7c7712a6b08ebf1bdc18fc43dc871b47b3dffe97""><code>7c7712a</code></a> Bump version to 1.53.0-pre2. (<a href=""https://redirect.github.com/grpc/grpc/issues/32545"">#32545</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/a4017dc45e342064722a36181ed14e6d7b469d29""><code>a4017dc</code></a> backport to 1.53: [promises] Make Poll&lt;T&gt; its own type, not a variant&lt;&gt; (<a href=""https://redirect.github.com/grpc/grpc/issues/32540"">#32540</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/3f93c1667280e6f11a1eb35cccfb8c81c698bee5""><code>3f93c16</code></a> Fuzzer fix backport to v1.53 (<a href=""https://redirect.github.com/grpc/grpc/issues/32511"">#32511</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/5b244b25c2b87a85781ceeecd34ce0f8e8e7e840""><code>5b244b2</code></a> Bump release version to 1.53.0-pre1 (<a href=""https://redirect.github.com/grpc/grpc/issues/32428"">#32428</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/6589340efc39b87c94897d221eaf949213cdac87""><code>6589340</code></a> Bump core version 202302161703 (<a href=""https://redirect.github.com/grpc/grpc/issues/32416"">#32416</a>)</li>
<li><a href=""https://github.com/grpc/grpc/commit/d49e1513063e6624e08eb6f59049596178a28783""><code>d49e151</code></a> [backoff] Add random early detection classifier (<a href=""https://redirect.github.com/grpc/grpc/issues/32354"">#32354</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/grpc/grpc/compare/v1.51.1...v1.53.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=grpcio&package-manager=pip&previous-version=1.51.1&new-version=1.53.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
340,Update docs to point to website,https://api.github.com/repos/move-coop/parsons/issues/851,Shauna Gordon-McKeon,851,closed,2023-06-30 16:35:19+00:00,2023-09-07 20:03:26+00:00,2023-09-07 20:02:53+00:00,"We moved a lot of content to our website. This PR updates our version controlled docs to point to our website instead of holding their own deprecated version of various instructions. 

I'm leaving the pages themselves for now as some people may have links to them, but at some point we'll want to go back and remove the pages entirely.

Also, the testing guide needs to be moved over too, but it hasn't been fully duplicated in the website so I don't want to remove it yet.",[],[],2
341,Add targets handler to NewMode Connector,https://api.github.com/repos/move-coop/parsons/issues/850,Ian,850,closed,2023-06-29 18:49:52+00:00,2023-08-17 18:31:32+00:00,2023-08-17 18:31:32+00:00,"The Newmode class currently doesn't allow the user to query the targets endpoint directly. This proposed feature would allow the user to query one or more target values from the NewMode API.

## Detailed Description
This proposal simply adds a `get_targets()` function to the `Newmode` class, following the same logic as other functions in this class.


## Context
TMC recently developed a sync-in for `targets` data from the New/Mode API and developed a modular solution in our in-house code. This enhancement in Parsons is a relatively simple lift, and would reduce redundancies in the Parsons / TMC codebases.


## Priority
Quite low priority - TMC has code to accomplish this already written, so it will simply be a matter of adding and testing","[Label(name=""enhancement"")]","[NamedUser(login=""IanRFerguson"")]",0
342,[Bug] Zoom Credentials type change,https://api.github.com/repos/move-coop/parsons/issues/849,Sade Chessidy,849,closed,2023-06-27 20:27:49+00:00,2023-09-08 19:37:00+00:00,2023-09-08 19:36:59+00:00,"## Detailed Description
I was alerted to the fact that Zoom's JWT authentication type is going to be fully [deprecated](https://developers.zoom.us/docs/internal-apps/jwt-app-migration/) September 1st, 2023. They are recommending that folks switch over to using server to server oauth or an Oauth app. I have a couple of Zoom data syncs that I manage using the Zoom parsons class that will likely break if the authentication type is not updated.


## To Reproduce
See link above with announcement from Zoom


## Your Environment
N/A


## Additional Context
N/A


## Priority
Medium - because there is still 2 months to implement a fix before it breaks.","[Label(name=""bug""), Label(name=""high priority"")]","[NamedUser(login=""IanRFerguson"")]",6
343,feat: add nation builder connector,https://api.github.com/repos/move-coop/parsons/issues/848,Giovanni Collazo,848,closed,2023-06-26 18:47:11+00:00,2023-08-28 08:09:32+00:00,2023-06-30 15:05:49+00:00,"**NOTE:** This is the second attempt at merging this feature. The first attempt had issues when running under **Python 3.7.17**. This new PR makes sure that the class types are compatible with that version of Python.

---

This PR adds a new [Nation Builder](https://nationbuilder.com/developers) connector.

The connector implements the following public methods:

- `get_people`: make all necessary HTTP request to get all people and return a `Table`. If a request fails, it will wait 30 seconds and keep trying until it fetches everything.
- `update_person`: update a person by their Nation Builder `id`.
- `upsert_person`: update or create a person.

## Examples

The following examples help illustrate the API usage.

```python
from dotenv import load_dotenv

from parsons.nation_builder.nation_builder import NationBuilder

load_dotenv()

nb = NationBuilder()

# get all people and return a Table
people_table = nb.get_people()
people_table.to_csv(""nbexport.csv"")

# update person by id
data = {""email"": ""foo@example.com"", ""tags"": [""foo"", ""bar""]}
updated_person_1 = nb.update_person(""123456789"", data)

# update or create a person
data = {""email"": ""foo@example.com"", ""tags"": [""dogs"", ""cats""]}
created, person = nb.upsert_person(data)

if created:
    print(""Person created:"", person)
else:
    print(""Person updated:"", person)
```

","[Label(name=""new connector"")]",[],0
344,"Revert ""feat: add Nation Builder Connector""",https://api.github.com/repos/move-coop/parsons/issues/847,Shauna Gordon-McKeon,847,closed,2023-06-26 18:05:28+00:00,2023-08-28 08:09:31+00:00,2023-06-26 18:08:58+00:00,"Reverts move-coop/parsons#837

Reverting due to unexpected error from type hints","[Label(name=""new connector"")]",[],0
345,Implement delete actionfield for ActionKit,https://api.github.com/repos/move-coop/parsons/issues/846,Kathy Nguyen,846,closed,2023-06-23 20:42:34+00:00,2023-08-28 08:08:38+00:00,2023-06-26 17:44:43+00:00,,"[Label(name=""connector update"")]",[],0
346,add script,https://api.github.com/repos/move-coop/parsons/issues/845,,845,closed,2023-06-22 20:43:06+00:00,2024-05-23 21:59:04+00:00,2024-05-23 21:59:02+00:00,,[],[],9
347,Add json as allowable file type to copy_s3,https://api.github.com/repos/move-coop/parsons/issues/844,Kasia Hinkson,844,closed,2023-06-21 18:12:38+00:00,2023-09-07 17:41:30+00:00,2023-09-07 17:41:30+00:00,"Add json as allowable file type to copy_s3. Since it's converting to a Parsons table, should be no downstream effects. ",[],[],2
348,[Feature/Addition] Change ActionNetwork Connector's handling of Custom Fields,https://api.github.com/repos/move-coop/parsons/issues/843,,843,open,2023-06-20 20:54:47+00:00,2023-11-21 21:30:44+00:00,,"Currently, ActionNetwork Connector handles ""custom fields"" by just uploading all extra keyword args as custom fields. While this works, it's poor design as it makes updating ActionNetwork data implicit rather than explicit, and it means that adding future keyword args can risk breaking use cases if those keyword arguments happen to overlap with a particular user's custom fields.

Instead, custom fields should need to be explicitly defined by the user when uploading.

## Possible Implementation
This should be simply removing the **kwargs in upsert_person and adding `custom_fields`.  Unfortunately, this would not be a backwards-compatible change. It's probably possible to set this on a deprecation path and add other keyword arguments and warnings.


## Priority
Medium priority.","[Label(name=""enhancement""), Label(name=""medium priority"")]",[],0
349,"Implement, test, and document Parsons Table deduplicate method",https://api.github.com/repos/move-coop/parsons/issues/842,Josh Fayer,842,closed,2023-06-16 20:41:22+00:00,2023-06-30 15:16:20+00:00,2023-06-30 15:16:19+00:00,"This PR resolves #820.

# Summary of changes:

`parsons/etl/etl.py`: adds a `deduplicate` method which optionally accepts a key or list of keys to deduplicate on, and an optional flag to indicate whether the method should first sort the table.
`docs/table.rst`: adds deduplicate method to Table documentation.
`test/test_etl.py`: adds several deduplicate unit tests for various scenarios involving zero, one, or multiple keys on both sorted and unsorted tables.

This method is essentially a wrapper around the built-in [petl.transform.dedup.distinct](https://petl.readthedocs.io/en/stable/transform.html#petl.transform.dedup.distinct) method and extends the functionality natively to the Parsons Table class. This solution also heavily references the implementation provided in the original issue by @shaunagm!

## One sticky point on terminology
As suggested in the original issue, this method accepts an optional `sort` parameter to instruct petl (imperatively) whether to first sort the table. However, this is more or less the opposite definition used internally by the petl library. In petl, the same argument is called `presorted` which (declaratively) describes a table as either presorted or not presorted &mdash; if it's not presorted, petl sorts it.

It's a small detail, but anyone who is used to the petl definition may be confused by this. A `sort=True` value (meaning, ""this table is not yet sorted, sort it for me"") is the exact opposite of a `presorted=True` value in petl (meaning, ""this table is sorted, don't sort it""). In the code, this is represented by an explicit inversion (`presorted=not sort`) on line 1186 when it's passed into petl's method. I've also included a comment explaining this for anyone who might be confused by the definitions.

For one, I totally agree that ""sort"" makes more sense here to instruct the deduplicate method to sort the array, but I'm newer to the community and wanted to note here that it might run against what someone might expect if they're used to interacting with petl and Parsons Tables directly.

Happy to adjust any of the above!","[Label(name=""🎉 first PR"")]",[],4
350,Create new feature for running and logging dbt commands,https://api.github.com/repos/move-coop/parsons/issues/841,Austin Weisgrau,841,closed,2023-06-15 22:00:18+00:00,2024-07-15 23:05:48+00:00,2024-01-18 21:08:22+00:00,"dbt is an increasingly popular tool in the data analytics community, for very good reason!

dbt's primary interface is through shell commands, which makes interacting with dbt commands programmatically somewhat difficult.

This new feature enables executing dbt commands from python, logs results to the python logger and optionally can also send formatted results to slack. The slack formatted message follows the formatting convention that dbt cloud uses for its slack messages.",[],"[NamedUser(login=""shaunagm"")]",6
351,Bump version number to 1.1.0,https://api.github.com/repos/move-coop/parsons/issues/840,Shauna Gordon-McKeon,840,closed,2023-06-15 17:12:51+00:00,2023-08-28 06:29:08+00:00,2023-06-15 17:31:34+00:00,,[],[],0
352,add exists_ok param to add_column,https://api.github.com/repos/move-coop/parsons/issues/839,,839,closed,2023-06-08 21:56:38+00:00,2023-06-20 17:56:36+00:00,2023-06-20 17:56:36+00:00,,[],[],2
353,Database connector feature parity,https://api.github.com/repos/move-coop/parsons/issues/838,Shauna Gordon-McKeon,838,open,2023-06-06 22:02:10+00:00,2023-08-17 20:28:19+00:00,,"We're hoping to help other connectors (especially the BigQuery connector) get closer to feature parity with the RedShift connector. Here's a list of features RedShift has that we'd like other connectors to have, ordered by priority. 

### High Priority

- [ ] table_exists()
- [ ] get_tables()
  - NOTE: the columns surfaced in the Redshift connector are based on the `pg_tables` table in Redshift. Functionally, I think we only need schemaname and tablename, but BigQuery may want to also surface project name. 
- [ ] get_views()
  - NOTE: the columns surfaced in the Redshift connector are based on the `pg_views` table in Redshift. Functionally, I think we only need schemanam, viewname, and view definition, but BigQuery may want to also surface project name. 

### Medium Priority

- [ ] get_row_count()
- [ ] populate_table_from_query()
- [ ] get_columns()
  - Similar to get_tables() - this output is based on a standard Redshift system table, but I think the real need is column name, type, max length, which is likely doable in all databases
- [ ] get_columns_list()

### Low Priority

- [ ] rename_table()
- [ ] move_table()
- [ ] duplicate_table()
- [ ] get_table_definition()
- [ ] get_table_definitions()
- [ ] get_view_definition()
- [ ] get_view_definitions()


PRs addressing this issue will likely only add a feature or two at a time. That's totally ok. :) Also feel free to comment if you have a different prioritization. This list comes via @elyse-weiss and represents internal TMC priorities but we are happy to accommodate community needs as well.","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],2
354,feat: add Nation Builder Connector,https://api.github.com/repos/move-coop/parsons/issues/837,Giovanni Collazo,837,closed,2023-06-06 20:17:28+00:00,2023-06-26 17:39:00+00:00,2023-06-26 17:38:59+00:00,"This PR adds a new [Nation Builder](https://nationbuilder.com/developers) connector.

The connector implements the following public methods:

- `get_people`: make all necessary HTTP request to get all people and return a `Table`. If a request fails, it will wait 30 seconds and keep trying until it fetches everything.
- `update_person`: update a person by their Nation Builder `id`.
- `upsert_person`: update or create a person.

## Examples

The following examples help illustrate the API usage.

```python
from dotenv import load_dotenv

from parsons.nation_builder.nation_builder import NationBuilder

load_dotenv()

nb = NationBuilder()

# get all people and return a Table
people_table = nb.get_people()
people_table.to_csv(""nbexport.csv"")

# update person by id
data = {""email"": ""foo@example.com"", ""tags"": [""foo"", ""bar""]}
updated_person_1 = nb.update_person(""123456789"", data)

# update or create a person
data = {""email"": ""foo@example.com"", ""tags"": [""dogs"", ""cats""]}
created, person = nb.upsert_person(data)

if created:
    print(""Person created:"", person)
else:
    print(""Person updated:"", person)
```","[Label(name=""🎉 first PR"")]",[],2
355,Restructure utilities and improve documentation of them,https://api.github.com/repos/move-coop/parsons/issues/836,Shauna Gordon-McKeon,836,open,2023-06-06 19:47:27+00:00,2023-10-13 17:22:31+00:00,,"Most of our helper code can be found in [parsons.utilities](https://github.com/move-coop/parsons/tree/main/parsons/utilities) however almost none of it is [in the documentation](https://move-coop.github.io/parsons/html/stable/utilities.html). We should update the docs!

There's also, separately, a [tools folder](https://github.com/move-coop/parsons/tree/main/parsons/tools) containing a single file with credentials tools. I think it makes sense to move the credentials file into `utilities`. If there's a chance users are calling this directly, we should probably replace it with a file that imports from the new location in `utilities` and logs a deprecation warning, then come back and delete it in like 6 months. If we think it's only being used internally we can just update our own calls and delete the file+folder.

While we are doing this, it's worth considering:

- structuring the utilities folder to give it subfolders, to help people reason about and find different classes of utilities
- adding additional documentation/writing a guide to adding utilities that helps people discover and add to the utilities
- rename `etl` to `table` or something slightly more descriptive (while we are moving/renaming things, might as well handle other issues, although I don't think this is high priority)

Also I just want to mention #724 (which I am now closing) which I think is a bad choice as written but contains some useful code and comments for however we decide to approach utilities/helpers.","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""needs discussion"")]",[],1
356,Change approach to fake test data,https://api.github.com/repos/move-coop/parsons/issues/835,Shauna Gordon-McKeon,835,open,2023-06-06 19:27:53+00:00,2023-06-16 18:56:32+00:00,,"Currently, our connector tests involve large amounts of fake data, usually in JSON format (but occasionally stored as Python dicts, csvs, or other formats). Sometimes this data is incorporated into the tests themselves, making them hard to read. Sometimes they're put in separate files, which is better, but it's still not ideal to have, say, [a 400 line test data file](https://github.com/move-coop/parsons/blob/main/test/test_github/test_data/test_get_repo.json) to test just one connector.

Are there other approaches that might be more readable, easier to maintain, and easier to write? (I know generating the test data is often the most annoying part of writing tests for connectors.)

I'm aware of tools like [Factory Boy](https://factoryboy.readthedocs.io/en/stable/) but that's for Python objects, not really for data. There's [Faker](https://faker.readthedocs.io/en/master/providers/faker.providers.misc.html?highlight=json) which seems more promising. 

Another option might be making use of [Json Schemas](http://json-schema.org/) although ""validate the schema"" isn't a huge part of the tests we're doing.

(I don't love that any of these approaches would involve adding another dependency - maybe it's time to separate out the handful of dev dependencies, like we do the [docs dependencies](https://github.com/move-coop/parsons/blob/main/docs/requirements.txt)?)

Whatever we do, we should make sure to document it really well so that it makes the lives of people writing Parsons tests easier rather than harder and more confusing. 

What do folks think?","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""needs discussion"")]",[],3
357, Add support for deleting person from NGPVAN ,https://api.github.com/repos/move-coop/parsons/issues/834,Sophie Waldman,834,closed,2023-06-06 18:57:15+00:00,2023-06-20 18:47:56+00:00,2023-06-20 18:47:10+00:00,,[],[],7
358,depedency cascade,https://api.github.com/repos/move-coop/parsons/issues/833,elyse-weiss,833,closed,2023-06-06 18:39:22+00:00,2023-06-06 19:02:48+00:00,2023-06-06 19:02:48+00:00,,[],[],0
359,"Revert ""Drop dependencies for cols needs a CASCADE """,https://api.github.com/repos/move-coop/parsons/issues/832,elyse-weiss,832,closed,2023-06-06 14:58:53+00:00,2023-06-06 15:02:39+00:00,2023-06-06 15:02:38+00:00,Reverts move-coop/parsons#819,[],[],0
360,Fix invalid setup.py syntax,https://api.github.com/repos/move-coop/parsons/issues/831,Kathy Nguyen,831,closed,2023-06-06 00:25:25+00:00,2023-06-06 16:03:10+00:00,2023-06-06 14:29:42+00:00,"After c056045d8a4bb76ec11b35958088591a28f74683 was merged, there is an error when running: `pip install git+https://github.com/move-coop/parsons@main#egg=parsons`

```
(test-pywell-ve) kathynguyen@administrators-MacBook-Pro-2 test-pywell % pip install git+https://github.com/move-coop/parsons@main#egg=parsons
Collecting parsons
  Cloning https://github.com/move-coop/parsons (to revision main) to /private/var/folders/d4/td5jnc7d6zd6s1ylg1t74jn40000gp/T/pip-install-2424vdzk/parsons_7b331d3cb8aa43a789e103429d496a28
  Running command git clone --filter=blob:none --quiet https://github.com/move-coop/parsons /private/var/folders/d4/td5jnc7d6zd6s1ylg1t74jn40000gp/T/pip-install-2424vdzk/parsons_7b331d3cb8aa43a789e103429d496a28
  Resolved https://github.com/move-coop/parsons to commit 9e061ba7c314be03025b4fa702cd76a00c50773a
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [1 lines of output]
      error in parsons setup command: 'extras_require' must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

This is because `~=` is not valid syntax in `setup.py`. I changed it to `<2`, which is valid.",[],[],2
361,Enable __len__ dunder method for parsons Table,https://api.github.com/repos/move-coop/parsons/issues/830,Austin Weisgrau,830,closed,2023-06-05 20:57:44+00:00,2023-06-06 14:41:51+00:00,2023-06-06 14:35:00+00:00,This enables the pythonic use of len(Table) rather than needing to use the idiosyncratic Table.num_rows. len(Table) calls Table.num_rows,[],[],0
362, Fix Salesforce query output,https://api.github.com/repos/move-coop/parsons/issues/829,Sophie Waldman,829,closed,2023-06-02 20:10:16+00:00,2023-06-05 21:51:07+00:00,2023-06-05 21:51:07+00:00,"The existing code did not match the method description and did not produce a useful result. Salesforce returns a set of nested dicts, similar to JSON; the new method reformats the nested ordered dicts using the json library, similar to describe_fields.",[],[],1
363,feat(auth0): Add upsert_user function,https://api.github.com/repos/move-coop/parsons/issues/828,,828,closed,2023-05-31 01:04:42+00:00,2023-06-16 18:07:37+00:00,2023-06-16 18:07:34+00:00,Adds `upsert_user` function to the Auth0 connector.,[],[],3
364,added encoding to s3_copy function in redshift class,https://api.github.com/repos/move-coop/parsons/issues/827,,827,closed,2023-05-27 03:48:32+00:00,2024-05-23 18:35:29+00:00,2024-05-23 18:35:28+00:00,,[],[],3
365,Action Builder connector,https://api.github.com/repos/move-coop/parsons/issues/826,,826,closed,2023-05-26 13:07:17+00:00,2023-10-10 19:52:54+00:00,2023-06-30 15:19:16+00:00,"A very initial submission of a class for an Action Builder connection. Woefully bare bones, this is the minimum viable product necessary for a working Action Builder template sync in the TMC environment.","[Label(name=""high priority""), Label(name=""new connector"")]",[],5
366,Specify version of slackclient in setup.py,https://api.github.com/repos/move-coop/parsons/issues/825,Kathy Nguyen,825,closed,2023-05-25 16:55:58+00:00,2023-06-05 21:28:59+00:00,2023-06-05 21:28:59+00:00,"When running `pip install parsons[slack]`, the latest version of slackclient is installed (2.9.4). The version of slackclient specified in `requirements.txt` is 1.3.0: https://github.com/move-coop/parsons/blob/853b27cb14394daaf113b88718facfde1190a572/requirements.txt#L6

In slackclient 2.9.4, the `slackclient` module has been renamed, causing all references to `parsons/notifications/slack.py` to return the error `ModuleNotFoundError: No module named 'slackclient'`. https://github.com/move-coop/parsons/blob/853b27cb14394daaf113b88718facfde1190a572/parsons/notifications/slack.py#L7

This PR specifies the slackclient version in `setup.py` to be the same version specified in `requirements.txt`.",[],[],1
367,Bump requests from 2.25.1 to 2.31.0,https://api.github.com/repos/move-coop/parsons/issues/824,,824,closed,2023-05-23 00:52:12+00:00,2023-06-26 17:50:53+00:00,2023-06-26 17:50:44+00:00,"Bumps [requests](https://github.com/psf/requests) from 2.25.1 to 2.31.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/psf/requests/releases"">requests's releases</a>.</em></p>
<blockquote>
<h2>v2.31.0</h2>
<h2>2.31.0 (2023-05-22)</h2>
<p><strong>Security</strong></p>
<ul>
<li>
<p>Versions of Requests between v2.3.0 and v2.30.0 are vulnerable to potential
forwarding of <code>Proxy-Authorization</code> headers to destination servers when
following HTTPS redirects.</p>
<p>When proxies are defined with user info (<a href=""https://user:pass@proxy:8080"">https://user:pass@proxy:8080</a>), Requests
will construct a <code>Proxy-Authorization</code> header that is attached to the request to
authenticate with the proxy.</p>
<p>In cases where Requests receives a redirect response, it previously reattached
the <code>Proxy-Authorization</code> header incorrectly, resulting in the value being
sent through the tunneled connection to the destination server. Users who rely on
defining their proxy credentials in the URL are <em>strongly</em> encouraged to upgrade
to Requests 2.31.0+ to prevent unintentional leakage and rotate their proxy
credentials once the change has been fully deployed.</p>
<p>Users who do not use a proxy or do not supply their proxy credentials through
the user information portion of their proxy URL are not subject to this
vulnerability.</p>
<p>Full details can be read in our <a href=""https://github.com/psf/requests/security/advisories/GHSA-j8r2-6x86-q33q"">Github Security Advisory</a>
and <a href=""https://nvd.nist.gov/vuln/detail/CVE-2023-32681"">CVE-2023-32681</a>.</p>
</li>
</ul>
<h2>v2.30.0</h2>
<h2>2.30.0 (2023-05-03)</h2>
<p><strong>Dependencies</strong></p>
<ul>
<li>
<p>⚠️ Added support for urllib3 2.0. ⚠️</p>
<p>This may contain minor breaking changes so we advise careful testing and
reviewing <a href=""https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html"">https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html</a>
prior to upgrading.</p>
<p>Users who wish to stay on urllib3 1.x can pin to <code>urllib3&lt;2</code>.</p>
</li>
</ul>
<h2>v2.29.0</h2>
<h2>2.29.0 (2023-04-26)</h2>
<p><strong>Improvements</strong></p>
<ul>
<li>Requests now defers chunked requests to the urllib3 implementation to improve
standardization. (<a href=""https://redirect.github.com/psf/requests/issues/6226"">#6226</a>)</li>
<li>Requests relaxes header component requirements to support bytes/str subclasses. (<a href=""https://redirect.github.com/psf/requests/issues/6356"">#6356</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>
<blockquote>
<h2>2.31.0 (2023-05-22)</h2>
<p><strong>Security</strong></p>
<ul>
<li>
<p>Versions of Requests between v2.3.0 and v2.30.0 are vulnerable to potential
forwarding of <code>Proxy-Authorization</code> headers to destination servers when
following HTTPS redirects.</p>
<p>When proxies are defined with user info (<a href=""https://user:pass@proxy:8080"">https://user:pass@proxy:8080</a>), Requests
will construct a <code>Proxy-Authorization</code> header that is attached to the request to
authenticate with the proxy.</p>
<p>In cases where Requests receives a redirect response, it previously reattached
the <code>Proxy-Authorization</code> header incorrectly, resulting in the value being
sent through the tunneled connection to the destination server. Users who rely on
defining their proxy credentials in the URL are <em>strongly</em> encouraged to upgrade
to Requests 2.31.0+ to prevent unintentional leakage and rotate their proxy
credentials once the change has been fully deployed.</p>
<p>Users who do not use a proxy or do not supply their proxy credentials through
the user information portion of their proxy URL are not subject to this
vulnerability.</p>
<p>Full details can be read in our <a href=""https://github.com/psf/requests/security/advisories/GHSA-j8r2-6x86-q33q"">Github Security Advisory</a>
and <a href=""https://nvd.nist.gov/vuln/detail/CVE-2023-32681"">CVE-2023-32681</a>.</p>
</li>
</ul>
<h2>2.30.0 (2023-05-03)</h2>
<p><strong>Dependencies</strong></p>
<ul>
<li>
<p>⚠️ Added support for urllib3 2.0. ⚠️</p>
<p>This may contain minor breaking changes so we advise careful testing and
reviewing <a href=""https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html"">https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html</a>
prior to upgrading.</p>
<p>Users who wish to stay on urllib3 1.x can pin to <code>urllib3&lt;2</code>.</p>
</li>
</ul>
<h2>2.29.0 (2023-04-26)</h2>
<p><strong>Improvements</strong></p>
<ul>
<li>Requests now defers chunked requests to the urllib3 implementation to improve
standardization. (<a href=""https://redirect.github.com/psf/requests/issues/6226"">#6226</a>)</li>
<li>Requests relaxes header component requirements to support bytes/str subclasses. (<a href=""https://redirect.github.com/psf/requests/issues/6356"">#6356</a>)</li>
</ul>
<h2>2.28.2 (2023-01-12)</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/psf/requests/commit/147c8511ddbfa5e8f71bbf5c18ede0c4ceb3bba4""><code>147c851</code></a> v2.31.0</li>
<li><a href=""https://github.com/psf/requests/commit/74ea7cf7a6a27a4eeb2ae24e162bcc942a6706d5""><code>74ea7cf</code></a> Merge pull request from GHSA-j8r2-6x86-q33q</li>
<li><a href=""https://github.com/psf/requests/commit/302225334678490ec66b3614a9dddb8a02c5f4fe""><code>3022253</code></a> test on pypy 3.8 and pypy 3.9 on windows and macos (<a href=""https://redirect.github.com/psf/requests/issues/6424"">#6424</a>)</li>
<li><a href=""https://github.com/psf/requests/commit/b639e66c816514e40604d46f0088fbceec1a5149""><code>b639e66</code></a> test on py3.12 (<a href=""https://redirect.github.com/psf/requests/issues/6448"">#6448</a>)</li>
<li><a href=""https://github.com/psf/requests/commit/d3d504436ef0c2ac7ec8af13738b04dcc8c694be""><code>d3d5044</code></a> Fixed a small typo (<a href=""https://redirect.github.com/psf/requests/issues/6452"">#6452</a>)</li>
<li><a href=""https://github.com/psf/requests/commit/2ad18e0e10e7d7ecd5384c378f25ec8821a10a29""><code>2ad18e0</code></a> v2.30.0</li>
<li><a href=""https://github.com/psf/requests/commit/f2629e9e3c7ce3c3c8c025bcd8db551101cbc773""><code>f2629e9</code></a> Remove strict parameter (<a href=""https://redirect.github.com/psf/requests/issues/6434"">#6434</a>)</li>
<li><a href=""https://github.com/psf/requests/commit/87d63de8739263bbe17034fba2285c79780da7e8""><code>87d63de</code></a> v2.29.0</li>
<li><a href=""https://github.com/psf/requests/commit/51716c4ef390136b0d4b800ec7665dd5503e64fc""><code>51716c4</code></a> enable the warnings plugin (<a href=""https://redirect.github.com/psf/requests/issues/6416"">#6416</a>)</li>
<li><a href=""https://github.com/psf/requests/commit/a7da1ab3498b10ec3a3582244c94b2845f8a8e71""><code>a7da1ab</code></a> try on ubuntu 22.04 (<a href=""https://redirect.github.com/psf/requests/issues/6418"">#6418</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/psf/requests/compare/v2.25.1...v2.31.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=requests&package-manager=pip&previous-version=2.25.1&new-version=2.31.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

You can trigger a rebase of this PR by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>
> **Note**
> Automatic rebases have been disabled on this pull request as it has been open for over 30 days.
","[Label(name=""dependency update"")]",[],0
368,Add NGPVAN bulk apply suppressions function,https://api.github.com/repos/move-coop/parsons/issues/823,Cody Gordon,823,closed,2023-05-22 18:31:05+00:00,2023-05-22 20:39:35+00:00,2023-05-22 20:39:35+00:00,"Simply adds the `Contacts` -> `Suppressions` bulk import mapping type. 

Note, the `Suppressions` mapping type will need to be enabled for your API key via support request for this to work.

QA'd and working on our end.",[],[],1
369,Pass use_env_token to to_s3_csv,https://api.github.com/repos/move-coop/parsons/issues/822,Cody Gordon,822,closed,2023-05-22 18:28:07+00:00,2023-05-22 18:50:31+00:00,2023-05-22 18:50:31+00:00,"This should extend the fix to functions using `to_s3_csv` (when False is passed) for the error which only appears in Lambda prod (using the same keys as when running locally):

```
S3UploadFailedError: Failed to upload /tmp/[...].zip to ephemeral-jobs-internal/[...].zip: An error occurred (InvalidToken) when calling the PutObject operation: The provided token is malformed or otherwise invalid.
```",[],[],1
370,Update Twilio Package Version,https://api.github.com/repos/move-coop/parsons/issues/821,,821,closed,2023-05-22 15:59:03+00:00,2023-06-26 17:47:07+00:00,2023-06-26 17:47:06+00:00,"Necessary to access the client for methods relating to newer endpoints, especially around 10DLC. Also removes a dependency the previous version had on an old version of PyJWT, which doesn't actually conflict with any current Parsons requirements, but could if we ever built a connector for DocuSign, or any tool with similar authentication.","[Label(name=""enhancement""), Label(name=""low priority"")]",[],0
371,"Add ""deduplicate"" helper method",https://api.github.com/repos/move-coop/parsons/issues/820,Shauna Gordon-McKeon,820,closed,2023-05-18 19:36:51+00:00,2023-06-30 15:16:21+00:00,2023-06-30 15:16:21+00:00,"It would be good to expose basic functionality on the Parsons table to allow people to deduplicate their data. Currently a lot of folks convert the Parsons table to a Pandas dataframe to accomplish this which seems wasteful both in terms of developer times and also could potentially impact performance:

I'm imagining something that looks like this:

`parsons_table.deduplicate(keys, sort)`

Where you can give no keys, one key, or several to deduplicate by, as well as indicating whether the data is already sorted or should be sorted (basically a wrapper on petl's `presorted` parameter.

Essentially it would be a wrapper around the petl function that accomplish this, ie:

```
from petl import etl
from parsons import table

people = people.to_petl()
result = etl.transform.dedup.distinct(people)
Table(result)
```
Medium priority - this seems like it would be very helpful but also it's not blocking anything.","[Label(name=""enhancement""), Label(name=""medium priority"")]",[],1
372,Drop dependencies for cols needs a CASCADE ,https://api.github.com/repos/move-coop/parsons/issues/819,elyse-weiss,819,closed,2023-05-18 15:40:27+00:00,2023-06-06 14:55:48+00:00,2023-06-06 14:55:42+00:00,Closes #818 ,[],[],8
373,[Bug] Drop dependencies for cols needs a CASCADE,https://api.github.com/repos/move-coop/parsons/issues/818,elyse-weiss,818,closed,2023-05-18 14:56:09+00:00,2023-06-06 14:55:44+00:00,2023-06-06 14:55:44+00:00,"We are using `alter_table_cascade=True` in a `rs.upsert()`. It is failing because some of the dependent views have their own dependent views. We need to add CASCADE to this line in the code: https://github.com/move-coop/parsons/blob/main/parsons/databases/redshift/redshift.py#L1090


## Detailed Description


## To Reproduce


## Your Environment


## Additional Context


## Priority
","[Label(name=""bug""), Label(name=""high priority"")]","[NamedUser(login=""elyse-weiss"")]",0
374,[Bug] Parsons should fail gracefully when add_column logic throws exceptions,https://api.github.com/repos/move-coop/parsons/issues/817,,817,open,2023-05-12 22:48:43+00:00,2023-09-28 02:35:29+00:00,,"## Detailed Description

When you call `add_column` and pass in a function that may throw an exception on edge case logic, the column is not evaluated until the Table is materialized, utilizing lazy loading. This is fine, but it causes issues if you try to reuse or correct the column's logic, as now any new operation on the Table, including attempts to `remove` and re-`add` the failing column, throw that exception. Because of the lazy loading, the Parsons table believes it has successfully added the column.

Ideally, when adding a particular column throws an exception, Parsons should fail gracefully and simply not add the column, or else remove the column before throwing the exception.


## To Reproduce

```
tb = parsons.Table([{""data"": 1}, {""data"": 2}, {""data"": 3}])
def conditional_error(x):
    if x.data == 2:
        raise ValueError(""Data can't be 2"")
    return ""Good""
tb.add_column(""new_column"", conditional_error)
tb.remove_column(""new_column"") #This will always fail
```

## Your Environment
* Version of Parsons used (if you know): 1.0
* Environment name and version: replicated on python 3.10 in various environment, although I have been developing using Jupyter lab
","[Label(name=""bug""), Label(name=""medium priority"")]",[],3
375,"Airtable get_records() argument 'field' requires a list, not a single string [Bug]",https://api.github.com/repos/move-coop/parsons/issues/816,,816,closed,2023-05-12 17:05:55+00:00,2023-10-25 18:05:56+00:00,2023-10-25 18:05:56+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
When I pass get_records(field='ID') it splits the 'I' and 'D' and looks for those columns in the airtable table. When I pass it, get_records(field=['ID']), using a list of one, it runs properly. 

We could just updated the documentation but I think this should be fixed.


## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority

Medium 
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.","[Label(name=""bug""), Label(name=""medium priority"")]",[],0
376,DatabaseConnector Interface to Major Release,https://api.github.com/repos/move-coop/parsons/issues/815,Jason,815,closed,2023-05-09 13:21:25+00:00,2023-08-17 19:37:11+00:00,2023-08-17 19:37:11+00:00,"[Here is a spreadsheet with the analysis I did of current feature parity between the four database connectors](https://docs.google.com/spreadsheets/d/1_jbhYk7lRCGwbwA00WKjJK9Ne0Zku_g_C3SuPx0m5-k/edit#gid=0).

Currently there are three methods they share in common that I think are worth representing in the interface:

- table_exists
- query
- copy

Of those, the main difference between the functionality is copy , where Redshift has a ton of extra functionality that the others do not have.

This PR creates the interface ABC, DatabaseConnector, and add the nifty discover_database function. I also do a little cleanup on the existing connectors, mostly adding type hints. These are very useful to make sure that the connector classes align properly with the interface and each other.

The only breaking change is switching the default value of strict_length in the Postgres connector from False to True, to align with the Redshift & MySQL connectors.

This PR only attempts to create the interface ""as is"", documenting the existing behavior shared by the database connectors (apart from the one Postgres default value). Once DatabaseConnector is merged in, we can plan out how we want to address the feature parity.",[],"[NamedUser(login=""Jason94"")]",3
377,Merge main into major-release,https://api.github.com/repos/move-coop/parsons/issues/814,Jason,814,closed,2023-05-08 15:14:43+00:00,2023-05-08 21:04:19+00:00,2023-05-08 21:04:19+00:00,Bring the major-release staging branch up to date with main.,[],[],0
378,Delete docs for non-existing arg in MySql,https://api.github.com/repos/move-coop/parsons/issues/813,Jason,813,closed,2023-05-03 04:30:57+00:00,2023-05-19 18:28:41+00:00,2023-05-19 18:28:41+00:00,The `view` parameter in the MySql class doesn't exist. This PR deletes the documentation for it.,[],[],0
379,[Feature/Addition] Allow boolean columns in csv conversion,https://api.github.com/repos/move-coop/parsons/issues/812,,812,open,2023-04-29 18:33:50+00:00,2023-05-19 19:23:39+00:00,,"## Detailed Description

Petl requires all values to be strings in `to_csv` and `from_csv`. It would be convenient to have an option to be able to load values with booleans instead of having to manually convert them after wards

## Context
A convert_bool, or column_types argument could be used in the `from_csv` function. Could also implement int or float translations at the same time

## Priority
Medium","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""needs discussion"")]",[],2
380,Remove hardcoded version number from conf.py and correct repo link in setup.py,https://api.github.com/repos/move-coop/parsons/issues/811,Soren Spicknall,811,closed,2023-04-24 14:43:18+00:00,2023-05-08 21:01:47+00:00,2023-05-08 21:01:47+00:00,"The hardcoded version ""0.5"" currently gets automatically populated in the title of [the Parsons docs site](https://move-coop.github.io/parsons/html/stable/index.html), no matter which release of the package is selected. This change clears the value entirely.

Additionally, the setup script for the package currently references an old location of the repository, which is corrected in this PR. There may be follow-up necessary to fix the same link [on PyPI](https://pypi.org/project/parsons/) - I'm not sure if that information populates directly from what's provided in setup.py upon the occasion of a new release.",[],[],0
381,chore(Geocoder): Add more informative error handling to geocode #729,https://api.github.com/repos/move-coop/parsons/issues/810,,810,closed,2023-04-22 04:32:07+00:00,2023-05-19 18:27:05+00:00,2023-05-19 18:18:29+00:00,Updates documentation and improves error handling,"[Label(name=""🎉 first PR"")]",[],6
382,Catch File Extensions in S3 Prefix,https://api.github.com/repos/move-coop/parsons/issues/809,Ian,809,closed,2023-04-17 15:03:44+00:00,2023-04-18 19:03:20+00:00,2023-04-18 19:03:20+00:00,"Adding in logic to skip the `prefix -> prefix/` logic here if a file extension is passed in.

Will defer to @KasiaHinkson on what the best solution is here (not sure I totally understand where this is breaking in STW sync)",[],"[NamedUser(login=""IanRFerguson"")]",1
383,[Feature/Addition] Enable redshift unload using parquet format,https://api.github.com/repos/move-coop/parsons/issues/808,Austin Weisgrau,808,open,2023-04-14 17:26:55+00:00,2023-10-10 21:55:39+00:00,,"The `parsons.Redshift().unload()` method currently only will export to CSV format. Parquet format is a better format for many data engineering purposes and the underlying redshift UNLOAD sql command can easily export to parquet.


## Context
CSV format has many issues, including that there is no universal standard for this format, there is no consistent way to set types, record-based storage is non-performant at high volumes, etc.

Parquet solves many of these issues by storing metadata and using column-based storage rather than row-based storage.

[Here's an article](https://towardsdatascience.com/csv-files-for-storage-no-thanks-theres-a-better-option-72c78a414d1d) with more info.

## Possible Implementation
Add a `parquet: bool = False` argument to the `unload()` method. By default, this argument will be set to `False` and default behavior will not change. If `parquet=True`, ignore the CSV specific arguments and unload files as parquet.


## Priority
medium","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],1
384,Add Default Parameter Flexibility,https://api.github.com/repos/move-coop/parsons/issues/807,Ian,807,closed,2023-04-13 17:40:58+00:00,2023-04-13 17:49:53+00:00,2023-04-13 17:49:48+00:00,"My last update did not account for the fact that `prefix = None` is default ... rookie mistake!

This hotfix first confirms that a prefix was supplied, and THEN it checks to see if a trailing `/` character is included",[],"[NamedUser(login=""IanRFerguson"")]",2
385,[Feature/Addition] Bulk delete from VAN/EA,https://api.github.com/repos/move-coop/parsons/issues/806,,806,open,2023-04-12 21:37:26+00:00,2023-05-19 19:09:36+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
VAN/EA API allows for bulk deleting - https://docs.everyaction.com/reference/people-vanid-delete

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->
We should have a function for this!

## Context
<!--- Why is this change important to you? How would you use it? -->
This would be super useful since VAN/EA charges for this normally!
<!--- How can it benefit other users? -->
Saving them money and allowing them to easily remove folks!

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->


## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update"")]",[],1
386,[Bug] Redshift.duplicate_table() drop table kwarg needs a cascade or dependent view option,https://api.github.com/repos/move-coop/parsons/issues/805,Erica Faulkenberry,805,open,2023-04-12 20:19:52+00:00,2023-12-19 22:07:40+00:00,,"The Redshift.duplicate_table() function has an optional parameter that allows the original table to be dropped after creating the duplicate table. This is a very helpful feature, but specifying 'drop' will result in errors if a table has dependent views. 

Ideally, the new options would be: drop-cascade, or a drop-skip-dependent (drop all tables except tables with dependent views). 

An interim suggestion is to update the documentation to include a caveat that Redshift functions that have a drop option do not currently account for dependencies, and perhaps include some example code showcasing suggested workarounds. 

## Priority
Medium","[Label(name=""bug""), Label(name=""medium priority""), Label(name=""needs more info"")]",[],2
387,[Bug] TargetSmart SmartMatch not showing up on documentation page,https://api.github.com/repos/move-coop/parsons/issues/804,elyse-weiss,804,open,2023-04-11 15:07:36+00:00,2023-06-06 22:26:03+00:00,,"## Detailed Description
noticed that we have TS SmartMatch in the code: https://github.com/move-coop/parsons/blob/main/parsons/targetsmart/targetsmart_smartmatch.py
but it’s not on the documentation page: https://move-coop.github.io/parsons/html/stable/targetsmart.html


","[Label(name=""bug""), Label(name=""documentation""), Label(name=""high priority""), Label(name=""needs more info"")]",[],3
388,Standardize S3 Prefix Conventions,https://api.github.com/repos/move-coop/parsons/issues/803,Ian,803,closed,2023-04-10 20:28:33+00:00,2023-04-13 17:36:10+00:00,2023-04-13 14:42:48+00:00,"# Included Here

This PR addresses Issue #784 (albeit incompletely). We included logic to confirm the `prefix/` conventions in `s3.list_keys()` and alert the user via logs and a raised Exception that their API call was rejected due to missing prefix input.

# Future Directions

This is a band-aid and doesn't address the root issue. `boto3` is not built to handle API calls to buckets where permission allowances are incomplete (i.e., we have access to a few prefixes but NOT the whole bucket). The API is somewhat opaque in this way, and attempting to do this gets inefficient very fast. We should revisit but this is sufficient for now.",[],"[NamedUser(login=""IanRFerguson"")]",0
389,Add Redash methods,https://api.github.com/repos/move-coop/parsons/issues/802,Kathy Nguyen,802,closed,2023-04-05 16:38:27+00:00,2023-05-19 18:14:28+00:00,2023-05-19 18:14:28+00:00,Add `get_data_source` and `update_data_source` methods to Redash connector.,[],[],1
390,MoveOn Shopify / AK changes,https://api.github.com/repos/move-coop/parsons/issues/801,Cody Gordon,801,closed,2023-03-24 00:18:44+00:00,2023-04-14 19:48:48+00:00,2023-04-14 19:48:48+00:00,"**1. Add access_token authentication option for Shopify**

**2. Actionkit orders / transactions required to [import Shopify Refunds](https://github.com/MoveOnOrg/fundraising_parsons_scripts/pull/142).** 

Code reviews and additional context:
https://github.com/MoveOnOrg/parsons/pull/38
https://github.com/MoveOnOrg/parsons/pull/39
https://github.com/MoveOnOrg/parsons/pull/41
",[],[],0
391,Actionkit orders / transactions,https://api.github.com/repos/move-coop/parsons/issues/800,Cody Gordon,800,closed,2023-03-23 23:58:25+00:00,2023-03-24 00:15:11+00:00,2023-03-24 00:07:22+00:00,"Internal MoveOn code review and context:
https://github.com/MoveOnOrg/parsons/pull/41",[],[],0
392,Add access_token authentication option for Shopify,https://api.github.com/repos/move-coop/parsons/issues/799,Cody Gordon,799,closed,2023-03-23 23:28:45+00:00,2023-03-24 00:15:10+00:00,2023-03-24 00:07:19+00:00,"Internal MoveOn code reviews and context:
https://github.com/MoveOnOrg/parsons/pull/38
https://github.com/MoveOnOrg/parsons/pull/39",[],[],0
393,[Feature/Addition] Enable Redshift.copy() to use an AWS_SESSION_TOKEN in authorization,https://api.github.com/repos/move-coop/parsons/issues/798,Austin Weisgrau,798,open,2023-03-17 21:09:10+00:00,2023-05-19 18:45:51+00:00,,"A session token is sometimes a necessary component of AWS authorization credentials alongside an access key and secret key. Currently there is no way to pass a session token in to a redshift copy command.

## Detailed Description
A fix would make it possible to pass a session token alongside an AWS access key and secret key using the same two methods:
- through environmental variables
- directly as part of `copy_args`


## Context
I have a situation where I need to assume an IAM role in order to access an S3 bucket. The credentials returned when you assume an AWS IAM role require the inclusion of a session token to be valid. In order to use the Table.to_redshift() method with this IAM role, I need to be able to pass a session token alongside the other AWS credentials.


## Priority
low","[Label(name=""enhancement""), Label(name=""low priority"")]",[],0
394,Redshift.copy() can use AWS Session Token as part of auth variables,https://api.github.com/repos/move-coop/parsons/issues/797,Austin Weisgrau,797,closed,2023-03-17 20:59:00+00:00,2024-06-21 00:23:55+00:00,2024-06-21 00:23:54+00:00,"A session token is sometimes a necessary component of AWS auth
credentials alongside an access key and secret key. This commit
enables a session token to be passed in the same ways that the other
credentials are passed:

- directly as part of copy_args
- through environmental variables

A test is added at `test/test_redshift.py::TestRedshift::test_get_creds_kwargs_with_token` to validate this behavior.
",[],[],3
395,Use black formatting in addition to flake8,https://api.github.com/repos/move-coop/parsons/issues/796,Austin Weisgrau,796,closed,2023-03-17 19:42:10+00:00,2024-01-31 18:47:01+00:00,2023-03-17 20:57:54+00:00,Resolves [#791](https://github.com/move-coop/parsons/issues/791),[],[],0
396,Use black formatting rather than flake8,https://api.github.com/repos/move-coop/parsons/issues/795,Austin Weisgrau,795,closed,2023-03-17 19:19:03+00:00,2023-03-17 19:33:28+00:00,2023-03-17 19:33:27+00:00,Resolves #791 ,[],[],2
397,[Bug] - Updating Dependency Versions,https://api.github.com/repos/move-coop/parsons/issues/794,Adrienne Michelson,794,open,2023-03-13 23:21:53+00:00,2023-12-08 18:08:17+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->

Python Package Conflicts
## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
The following packages are causing conflicts for setting up in Mage.AI to utilize the functions for ActionKit.

pyopenssl 23.0.0 requires cryptography<40,>=38.0.0, but you have cryptography 36.0.2 which is incompatible.
parsons 1.0.0 requires jinja2==3.0.2, but you have jinja2 3.1.2 which is incompatible.
parsons 1.0.0 requires PyJWT==2.4.0, but you have pyjwt 2.6.0 which is incompatible.
parsons 1.0.0 requires python-dateutil==2.8.1, but you have python-dateutil 2.8.2 which is incompatible.
parsons 1.0.0 requires requests==2.25.1, but you have requests 2.27.1 which is incompatible.
parsons 1.0.0 requires SQLAlchemy==1.3.23, but you have sqlalchemy 1.4.46 which is incompatible.

## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->
Utilizing Mage.AI and importing Parsons.

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.

Medium. We may be able to get around it with the guide provided by Shauna on Dependencies. But opening this up to think through dependency management as a whole with the package. https://www.parsonsproject.org/pub/friendly-dependencies/","[Label(name=""bug""), Label(name=""high priority""), Label(name=""needs discussion"")]",[],2
398,Change release version to 1.0,https://api.github.com/repos/move-coop/parsons/issues/793,Jason,793,closed,2023-02-24 17:36:29+00:00,2023-02-24 20:02:12+00:00,2023-02-24 20:01:53+00:00,,[],[],0
399,[Feature/Addition] Census API,https://api.github.com/repos/move-coop/parsons/issues/792,elyse-weiss,792,closed,2023-02-21 21:58:28+00:00,2024-04-30 18:32:45+00:00,2024-04-30 18:32:45+00:00,"People often rely on census data, and it's not always the easiest to navigate. A Parsons connector would be great to help individuals better grab the census data they need, more easily!


## Detailed Description
A list of all available APIs are here: https://www.census.gov/data/developers/data-sets.html

I imagine if we were able to get a class going with one, it'd be a great setup for folks to add additional APIs. I'd recommend perhaps starting with the American Community Survey. 


## Context


## Possible Implementation


## Priority
Low - purely nice to have!","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""new connector"")]","[NamedUser(login=""Charlie-Kramer"")]",8
400,Switch to using Black instead of/in addition to PyLint,https://api.github.com/repos/move-coop/parsons/issues/791,Shauna Gordon-McKeon,791,closed,2023-02-21 19:51:47+00:00,2023-03-17 20:57:56+00:00,2023-03-17 20:57:56+00:00,"Manual linting is time-consuming for contributors. Switching to Black will allow contributors to lint with a single command.

Implementing this is going to involve a massive PR, since Black is very opinionated and will touch a ton of files. But it seems worth it in the long run.
","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""testing"")]","[NamedUser(login=""shaunagm"")]",2
401,Add CI installation + import tests for all supported versions and all major environments,https://api.github.com/repos/move-coop/parsons/issues/790,Shauna Gordon-McKeon,790,closed,2023-02-21 16:44:10+00:00,2023-08-04 21:55:21+00:00,2023-08-04 21:55:20+00:00,"Given that Parsons can be tricky to install, I'd like to add to our tests a step where Parsons is installed on a variety of systems - covering all major OSs and versions. We have also had some issues recently with not testing out all supported versions.

## Possible Implementation

In addition to the work itself, the main question is which versions to test. We can probably poll the community to find the most popular OSs and versions of things.

## Priority

High - this will likely help reliability a lot.","[Label(name=""enhancement""), Label(name=""high priority""), Label(name=""testing"")]",[],1
402,Hubble Connector,https://api.github.com/repos/move-coop/parsons/issues/789,Justin Adkins,789,closed,2023-02-19 05:08:07+00:00,2023-02-21 20:51:14+00:00,2023-02-21 20:51:13+00:00,,[],[],0
403,Airtable Authentication set to change,https://api.github.com/repos/move-coop/parsons/issues/788,Cormac Martinez del Rio,788,closed,2023-02-17 17:55:50+00:00,2024-06-24 16:59:38+00:00,2024-06-24 16:59:38+00:00,"Airtable is changing their authentication methods, moving away from API keys to OAuth integrations and Personal Access Tokens. API keys will not longer work after feb 1 2024. [Details](https://community.airtable.com/t5/announcements/new-api-capabilities-now-in-ga-and-upcoming-api-keys-deprecation/ba-p/141824?utm_ID=recdXE5vJZZ5vR0mh&utm_ID=recdXE5vJZZ5vR0mh&utm_source=lifecycle_team&utm_source=lifecycle_team&utm_medium=email&utm_medium=email&utm_campaign=it_ss_ss_api_deprecation&utm_campaign=it_ss_ss_api_depreciation&utm_content=email-blast_api_1a&utm_content=email-blast_api_key_users)

## Priority
Low. We have 1 year","[Label(name=""low priority""), Label(name=""connector update""), Label(name=""futureproofing"")]","[NamedUser(login=""crayolakat"")]",7
404,Add sample use case of ngpvan printed_list method,https://api.github.com/repos/move-coop/parsons/issues/787,,787,closed,2023-01-19 19:03:22+00:00,2023-06-22 20:42:33+00:00,2023-06-22 20:42:33+00:00,,[],[],1
405,Fix #780 and #781 - Google Sheets documentation,https://api.github.com/repos/move-coop/parsons/issues/786,Ethan Yoo,786,closed,2023-01-10 18:36:55+00:00,2023-02-21 16:14:43+00:00,2023-02-21 16:14:43+00:00,Fixes [issue 780](https://github.com/move-coop/parsons/issues/780) and [issue 781](https://github.com/move-coop/parsons/issues/781),[],[],1
406,Update CircleCI Docs key fingerprint,https://api.github.com/repos/move-coop/parsons/issues/785,Nikki Everett,785,closed,2023-01-10 16:42:55+00:00,2023-01-10 17:43:12+00:00,2023-01-10 17:43:06+00:00,"Per [CircleCI recommendations](https://circleci.com/blog/january-4-2023-security-alert/), I have rotated the two SSH keys stored in the Parsons project settings (the read-only deploy key and the read/write docs key). This PR updates the docs key fingerprint in the CircleCI config.yml.","[Label(name=""high priority"")]","[NamedUser(login=""neverett"")]",0
407,[Bug] S3 utilities in Parsons do not accommodate different subfolder permissions,https://api.github.com/repos/move-coop/parsons/issues/784,elyse-weiss,784,open,2023-01-09 21:54:17+00:00,2023-04-06 15:31:18+00:00,,"Many of the S3 utilities in Parsons aren't able to accommodate setups like TMC's where subfolders of S3 buckets have different permissions from each other - we need to make some adjustments to avoid future breakages, likely by detecting when subfolders are present in a bucket and adding them to a ""prefix"" portion of an AWS API call rather than calling directly into the AWS API with the bucket + subfolder path.


## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know):
* Environment name and version (e.g. Chrome 39, node.js 5.4):
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.","[Label(name=""bug""), Label(name=""high priority""), Label(name=""requires access"")]","[NamedUser(login=""IanRFerguson"")]",3
408,[Feature/Addition] Add polling_interval parameter to Targetsmart Automation match() function,https://api.github.com/repos/move-coop/parsons/issues/783,Nikki Everett,783,open,2023-01-06 22:39:52+00:00,2023-01-09 19:32:01+00:00,,"TS asked us if we could reduce how frequently we monitor their SFTP site for matched files. The current polling interval in the [TS Automation](https://github.com/move-coop/parsons/blob/main/parsons/targetsmart/targetsmart_automation.py) [`match_status()`](https://github.com/move-coop/parsons/blob/main/parsons/targetsmart/targetsmart_automation.py#L230) function is set to 60 seconds, but it would be great if that could be changed by the user. It seems like the easiest way to do that would be to add a polling_interval parameter to the [`match()`](https://github.com/move-coop/parsons/blob/main/parsons/targetsmart/targetsmart_automation.py#L64) function with a default of 60 that is then passed to the match_status() function.

## Context
This change is important to us so we can be more mindful users of TS matching. It will be helpful for other users as it makes the TS Automation code a bit more flexible and configurable to user needs.


## Priority

Medium -- not critical for us, but would be good to have done soon.","[Label(name=""enhancement""), Label(name=""medium priority"")]",[],2
409,"Add docker image references, logo, scytl connector in sidebar",https://api.github.com/repos/move-coop/parsons/issues/782,Shauna Gordon-McKeon,782,closed,2022-12-28 18:55:33+00:00,2023-02-15 18:46:20+00:00,2023-02-15 18:46:19+00:00,"Misc docs improvements, including:

- fixes #671 
- adds scytl connector to sidebar
- adds new Parsons logo to top of docs homepage
- rearranges homepage to be a bit easier to read, including replacing most of the installation details with a link to the installation guide hosted on the website",[],[],0
410,[Bug] Google Sheets initiation instructions insufficient,https://api.github.com/repos/move-coop/parsons/issues/781,elyse-weiss,781,closed,2022-12-14 19:20:13+00:00,2023-02-21 16:14:58+00:00,2023-02-21 16:14:58+00:00,"## Detailed Description
The Google Sheets section includes direction on creating a service account. There is a key piece of information missing! The service account created -- [text]@[project].iam.gserviceaccount.com -- needs to be shared on any Google Sheets or Google Drive Folders being interacted with.

## Priority
High! I couldn't figure this out without asking others.","[Label(name=""bug"")]",[],3
411,[Bug] Documentation Error in Google Sheets Class for createspreadsheet(),https://api.github.com/repos/move-coop/parsons/issues/780,elyse-weiss,780,closed,2022-12-14 17:47:10+00:00,2023-02-21 16:14:44+00:00,2023-02-21 16:14:44+00:00,"## Description
![image](https://user-images.githubusercontent.com/29580051/207669016-f44e1ba9-5787-4a3f-bb57-b02ea5fea3a5.png)

The description says it creates a spreadsheet from a Parsons table, but a Parsons table is not an arg. A Parsons table is also not referenced in the actual code.

## Priority
I would say high in that it is quite misleading!","[Label(name=""bug"")]",[],0
412,Upgrade dependencies,https://api.github.com/repos/move-coop/parsons/issues/779,Kathy Nguyen,779,closed,2022-12-08 19:53:18+00:00,2023-02-21 16:49:18+00:00,2023-02-21 16:49:17+00:00,"I was getting a segmentation fault when running any and all tests locally. OS: macOS Ventura 13.01. Python version: 3.10.8. See below log:
```
(parsons-ve) kathynguyen@administrators-MacBook-Pro parsons % pytest -rf test/test_shopify.py
===================================================================== test session starts =====================================================================
platform darwin -- Python 3.10.8, pytest-7.1.1, pluggy-1.0.0
rootdir: /Users/kathynguyen/Documents/GitHub/parsons, configfile: pytest.ini
plugins: requests-mock-1.5.2, datadir-1.3.0
collected 4 items                                                                                                                                             
test/test_shopify.py ....                                                                                                                               [100%]
====================================================================== warnings summary =======================================================================
../../../.pyenv/versions/3.10.8/envs/parsons-ve/lib/python3.10/site-packages/joblib/my_exceptions.py:21
  /Users/kathynguyen/.pyenv/versions/3.10.8/envs/parsons-ve/lib/python3.10/site-packages/joblib/my_exceptions.py:21: DeprecationWarning: TransportableException is deprecated and will be removed from joblib in 0.16
    warn(""{} is deprecated and will be removed from joblib ""
-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================================================ 4 passed, 1 warning in 1.29s =================================================================
zsh: segmentation fault  pytest -rf test/test_shopify.py
```

After some digging, it was discovered that the root cause is the `google-cloud-bigquery==3.0.1` requirement. This installs `pyarrow==7.0.0` which caused the segmentation fault. After upgrading to `google-cloud-bigquery==3.4.0`, pyarrow will be updated to `pyarrow==10.0.1`, which will fix the issue. `grpcio` is also required to by updated because of the `google-cloud-bigquery` update",[],[],1
413,Add printed list functionality to VAN connector,https://api.github.com/repos/move-coop/parsons/issues/778,,778,closed,2022-11-29 19:58:43+00:00,2022-12-01 20:48:26+00:00,2022-12-01 20:48:14+00:00,,"[Label(name=""good first issue"")]",[],0
414,Add create_transaction method to ActionKit,https://api.github.com/repos/move-coop/parsons/issues/777,Kathy Nguyen,777,closed,2022-11-29 19:12:18+00:00,2022-12-01 22:17:02+00:00,2022-12-01 22:16:19+00:00,,[],[],2
415,merge contacts and test,https://api.github.com/repos/move-coop/parsons/issues/776,Melissa Woods,776,closed,2022-11-22 21:30:32+00:00,2022-11-23 19:53:03+00:00,2022-11-23 19:52:51+00:00,This PR adds the merge_contacts function to the VAN class and includes tests. #772 ,[],"[NamedUser(login=""mkwoods927"")]",3
416,"Updates to ActionKit, S3, and NGPVan",https://api.github.com/repos/move-coop/parsons/issues/775,Kathy Nguyen,775,closed,2022-11-22 18:23:20+00:00,2022-12-07 20:07:55+00:00,2022-12-07 20:07:54+00:00,"1. Add `cancel_orderrecurring` and `update_paymenttoken` functions to ActionKit connector
2. Add `use_env_token` parameter to S3 initializer
3. Add ContactNotes class to NGPVan",[],[],4
417,[Feature/Addition] AP Elections Class,https://api.github.com/repos/move-coop/parsons/issues/774,elyse-weiss,774,open,2022-11-16 14:49:18+00:00,2024-05-02 13:51:53+00:00,,"Proposed new AP Elections class for use with Associated Press API.

## Detailed Description
TMC purchased the Associated Press API in both 2020 and 2022. The code is in this Github gist: https://gist.github.com/elyse-weiss/77385e02430df8b3d2f135c6f9ff470b

I do not have time at this moment to formalize into a Parsons class, but figured someone else might. There are two scripts included:
### ap_election.py
During a new calendar year, the following updates need to be made to `apelection.py`:
- `API_URL`: the AP may upgrade from v3
- `OUTPUT_FIELDS`: the AP may add new fields

### update_election_data.py
`https://api.ap.org/v3/elections/{election_date}?format=json&level={level}&resultstype={test}&statepostal=%{stateid}&minDateTime={timestamp}`
- *election_date*: the Election Date as defined by job parameter
- *level*: levels of reporting as defined by job parameter
    1. ru = reporting unit
    2. district = district
        *** this is only relevant in Presidential years, where you need the Maine 
        electoral votes broken down
- *resultstype*: the type of results as defined by job parameter
    - l = live data
    - t = test data
- *stateid*: this is looped through all 51 states
- *timestamp*: this is generated in code

## Context
At TMC, we chose to push the data to Postgres since we were running the job to hit against the API every 5 minutes.

## Possible Implementation

## Priority
low priority","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""new connector"")]",[],2
418,Donorbox connector,https://api.github.com/repos/move-coop/parsons/issues/773,Shauna Gordon-McKeon,773,closed,2022-11-15 20:37:35+00:00,2022-12-01 20:31:16+00:00,2022-12-01 20:31:16+00:00,"Adds a Donorbox connector implementing the endpoints listed [here](https://github.com/donorbox/donorbox-api). Addresses #718 

Contains tests and docs, although the live tests are currently skipped, as they require a paid account and we don't have one set up with our CI. I've tested them locally, though.","[Label(name=""new connector"")]",[],0
419,[Feature/Addition] Merge Contacts function for EveryAction (VAN),https://api.github.com/repos/move-coop/parsons/issues/772,Melissa Woods,772,closed,2022-11-04 15:30:21+00:00,2023-02-22 16:28:34+00:00,2023-02-22 16:28:33+00:00,"## Detailed Description
The VAN API has a function to merge two contact records in EveryAction. This issue is for adding a method to the VAN class in Parsons to do this!

## Context
Until recently folks who want to de-dupe their EveryAction databases had only two options: to do so manually for every duplicate pair of records or to pay EveryAction. About 8 months ago VAN added this functionality to their API, and adding it to Parsons would save Parsons users a lot of time and money!

## Possible Implementation
[Here is the VAN API documentation](https://docs.ngpvan.com/reference/peoplevanidmergeinto) 
I'm requesting that Parsons API keys have access to this method.

## Priority
Low","[Label(name=""enhancement""), Label(name=""blocked""), Label(name=""low priority""), Label(name=""connector update"")]",[],5
420,[Bug] Hustle Create_leads() does not load custom fields,https://api.github.com/repos/move-coop/parsons/issues/771,,771,open,2022-11-02 15:58:03+00:00,2022-11-17 20:29:31+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
The create leads function does not actually load extra columns as custom fields

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
State should be loaded as a custom field and it is not
![Screen Shot 2022-11-02 at 9 34 35 AM](https://user-images.githubusercontent.com/62267774/199538647-d79c0c85-0971-484b-9e8b-6d2acc6d554b.png)


## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know): the latest
* Environment name and version (e.g. Chrome 39, node.js 5.4): 
* Operating System and version (desktop or mobile):


## Additional Context
Add any other context about the problem here.


## Priority
high!","[Label(name=""bug""), Label(name=""high priority""), Label(name=""connector update"")]",[],0
421,NGPVAN: Saved List Overwrite Bug Fix,https://api.github.com/repos/move-coop/parsons/issues/770,,770,closed,2022-10-26 02:51:26+00:00,2022-11-21 22:28:29+00:00,2022-11-21 22:28:29+00:00,"There was a bug in the code that would not allow you to overwrite a list. Also, cleaned up a copy/paste typo in the logging.","[Label(name=""bug"")]",[],2
422,Add get_aliases() function to Google Admin connector,https://api.github.com/repos/move-coop/parsons/issues/769,Kathy Nguyen,769,closed,2022-10-22 04:16:10+00:00,2022-11-21 23:32:14+00:00,2022-11-21 19:02:48+00:00,Also fix existing tests for them to make more sense,[],[],5
423,Fix GitHub API Docs,https://api.github.com/repos/move-coop/parsons/issues/768,Soren Spicknall,768,closed,2022-10-21 16:11:08+00:00,2022-10-21 16:11:15+00:00,2022-10-21 16:11:15+00:00,,[],[],0
424,GitHub Connector Fix,https://api.github.com/repos/move-coop/parsons/issues/767,Soren Spicknall,767,closed,2022-10-21 16:06:38+00:00,2022-12-06 18:53:00+00:00,2022-10-21 16:16:44+00:00,"This work actually belongs to @AndrewRook, but is being merged via this PR because of a CI configuration issue.",[],[],0
425,Fix GitHub Connector API Docs,https://api.github.com/repos/move-coop/parsons/issues/766,Soren Spicknall,766,closed,2022-10-21 15:58:40+00:00,2022-10-21 16:10:15+00:00,2022-10-21 16:10:15+00:00,,[],[],0
426,Latest Attempt: Fix GitHub Connector API Docs (with working CI),https://api.github.com/repos/move-coop/parsons/issues/765,Soren Spicknall,765,closed,2022-10-21 15:25:00+00:00,2022-10-21 15:57:23+00:00,2022-10-21 15:57:23+00:00,Hoping to overcome CI config issues that stymied testing for #753 and #761,[],[],0
427,Update requirements.txt for joblib security vulnerability,https://api.github.com/repos/move-coop/parsons/issues/764,Soren Spicknall,764,closed,2022-10-21 14:22:25+00:00,2022-10-21 14:26:52+00:00,2022-10-21 14:26:46+00:00,,[],[],0
428,Bump joblib from 0.14.1 to 1.2.0,https://api.github.com/repos/move-coop/parsons/issues/763,,763,closed,2022-10-21 14:16:01+00:00,2022-10-21 14:26:20+00:00,2022-10-21 14:26:10+00:00,"Bumps [joblib](https://github.com/joblib/joblib) from 0.14.1 to 1.2.0.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/joblib/joblib/blob/master/CHANGES.rst"">joblib's changelog</a>.</em></p>
<blockquote>
<h2>Release 1.2.0</h2>
<ul>
<li>
<p>Fix a security issue where <code>eval(pre_dispatch)</code> could potentially run
arbitrary code. Now only basic numerics are supported.
<a href=""https://github-redirect.dependabot.com/joblib/joblib/pull/1327"">joblib/joblib#1327</a></p>
</li>
<li>
<p>Make sure that joblib works even when multiprocessing is not available,
for instance with Pyodide
<a href=""https://github-redirect.dependabot.com/joblib/joblib/pull/1256"">joblib/joblib#1256</a></p>
</li>
<li>
<p>Avoid unnecessary warnings when workers and main process delete
the temporary memmap folder contents concurrently.
<a href=""https://github-redirect.dependabot.com/joblib/joblib/pull/1263"">joblib/joblib#1263</a></p>
</li>
<li>
<p>Fix memory alignment bug for pickles containing numpy arrays.
This is especially important when loading the pickle with
<code>mmap_mode != None</code> as the resulting <code>numpy.memmap</code> object
would not be able to correct the misalignment without performing
a memory copy.
This bug would cause invalid computation and segmentation faults
with native code that would directly access the underlying data
buffer of a numpy array, for instance C/C++/Cython code compiled
with older GCC versions or some old OpenBLAS written in platform
specific assembly.
<a href=""https://github-redirect.dependabot.com/joblib/joblib/pull/1254"">joblib/joblib#1254</a></p>
</li>
<li>
<p>Vendor cloudpickle 2.2.0 which adds support for PyPy 3.8+.</p>
</li>
<li>
<p>Vendor loky 3.3.0 which fixes several bugs including:</p>
<ul>
<li>
<p>robustly forcibly terminating worker processes in case of a crash
(<a href=""https://github-redirect.dependabot.com/joblib/joblib/pull/1269"">joblib/joblib#1269</a>);</p>
</li>
<li>
<p>avoiding leaking worker processes in case of nested loky parallel
calls;</p>
</li>
<li>
<p>reliability spawn the correct number of reusable workers.</p>
</li>
</ul>
</li>
</ul>
<h2>Release 1.1.1</h2>
<ul>
<li>Fix a security issue where <code>eval(pre_dispatch)</code> could potentially run
arbitrary code. Now only basic numerics are supported.
<a href=""https://github-redirect.dependabot.com/joblib/joblib/pull/1327"">joblib/joblib#1327</a></li>
</ul>
<h2>Release 1.1.0</h2>
<ul>
<li>Fix byte order inconsistency issue during deserialization using joblib.load</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/joblib/joblib/commit/5991350e03493fbf27bb596429a935e0c40fb536""><code>5991350</code></a> Release 1.2.0</li>
<li><a href=""https://github.com/joblib/joblib/commit/3fa218887770467695573e37e1c7179fd1b5065d""><code>3fa2188</code></a> MAINT cleanup numpy warnings related to np.matrix in tests (<a href=""https://github-redirect.dependabot.com/joblib/joblib/issues/1340"">#1340</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/cea26ff2080dc4e9b51957e57994f48351086193""><code>cea26ff</code></a> CI test the future loky-3.3.0 branch (<a href=""https://github-redirect.dependabot.com/joblib/joblib/issues/1338"">#1338</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/8aca6f4fc29c36e011201bbfe2da227b58da55e3""><code>8aca6f4</code></a> MAINT: remove pytest.warns(None) warnings in pytest 7 (<a href=""https://github-redirect.dependabot.com/joblib/joblib/issues/1264"">#1264</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/067ed4f7cc88aef0f4160d6ef7155d40767fee08""><code>067ed4f</code></a> XFAIL test_child_raises_parent_exits_cleanly with multiprocessing (<a href=""https://github-redirect.dependabot.com/joblib/joblib/issues/1339"">#1339</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/ac4ebd540840f92f2c12f47ad001b555d2bb1ce2""><code>ac4ebd5</code></a> MAINT add back pytest warnings plugin (<a href=""https://github-redirect.dependabot.com/joblib/joblib/issues/1337"">#1337</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/a23427d1700e32d4fc5d49c16d72e3f3c24f65f9""><code>a23427d</code></a> Test child raises parent exits cleanly more reliable on macos (<a href=""https://github-redirect.dependabot.com/joblib/joblib/issues/1335"">#1335</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/ac0969194aea9c9282a7532cfcda9746bc3b379b""><code>ac09691</code></a> [MAINT] various test updates (<a href=""https://github-redirect.dependabot.com/joblib/joblib/issues/1334"">#1334</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/4a314b152fe0b71b53b6092ed67be528ec81392e""><code>4a314b1</code></a> Vendor loky 3.2.0 (<a href=""https://github-redirect.dependabot.com/joblib/joblib/issues/1333"">#1333</a>)</li>
<li><a href=""https://github.com/joblib/joblib/commit/bdf47e95c7204499397f0cd9ef6b3198c71976ce""><code>bdf47e9</code></a> Make test_parallel_with_interactively_defined_functions_default_backend timeo...</li>
<li>Additional commits viewable in <a href=""https://github.com/joblib/joblib/compare/0.14.1...1.2.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=joblib&package-manager=pip&previous-version=0.14.1&new-version=1.2.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],2
429,Scytl fix browser headers and requests lib w mock,https://api.github.com/repos/move-coop/parsons/issues/762,Adam Greenspan,762,closed,2022-10-21 12:44:31+00:00,2022-10-21 14:52:25+00:00,2022-10-21 14:27:26+00:00,"@shaunagm 

Three issues were occurring that caused the tests to fail:
- First, the zip file request mocks weren't being set up correctly. This sets them as such.
- Second, the outgoing requests using urllib.request tooling was not respecting the mocks, instead going directly to the Scytl server. We didn't notice because the urls and files that were mocked and saved were identical to the ones on the server.
- Third, Scytl started enforcing logic to prevent requests that aren't coming from a browser, causing the 403 Forbidden errors. By adding a UserAgent header, Scytl will now think we are making a request from a browser.

Not sure why it didn't fail when we re-ran the tests last night in CircleCI, but this should be the fix.",[],[],0
430,Fixed GitHub connector API docs (attempt 2),https://api.github.com/repos/move-coop/parsons/issues/761,Andrew Schechtman-Rook,761,closed,2022-10-21 01:43:28+00:00,2022-10-21 15:29:11+00:00,2022-10-21 15:29:11+00:00,This supersedes #753 in an attempt to fix the CI/CD.,[],[],1
431,use requests library,https://api.github.com/repos/move-coop/parsons/issues/760,Adam Greenspan,760,closed,2022-10-20 23:08:52+00:00,2022-10-21 12:45:02+00:00,2022-10-21 12:44:57+00:00,"@shaunagm 

When we merged to main, the outgoing requests started failing with a ""Forbidden"" response, even though it worked both locally and in the pull request.

Let's see if migrating to the requests library improves the situation.",[],[],1
432,Fix assert_matching_tables,https://api.github.com/repos/move-coop/parsons/issues/759,Kathy Nguyen,759,closed,2022-10-18 20:34:23+00:00,2022-10-20 19:51:05+00:00,2022-10-20 19:31:05+00:00,"In `utils.py`, when comparing `assert list(r1) == list(r2)`, if `r1` and `r2` are type`dict`, this only compares if the headers of `r1` and `r2` are equal, when it should be comparing if the entire contents of both dicts are equal.

This is why the `test_google_admin.py` test was passing even though it had a typo and should have failed. This typo is also fixed in this PR",[],[],1
433,Airtable get_records does not return columns when all values are null,https://api.github.com/repos/move-coop/parsons/issues/758,Deleted user,758,open,2022-10-13 20:29:48+00:00,2024-07-15 18:48:33+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
The get_records method only returns a parsons table with columns that are not null for all records. When pulling a view with just a few records, I am not getting certain columns that happen to be blank. I imagine that’s just how the api returns the data but kind of annoying.


## Detailed Description
If you create a view with some records and all of them are missing a value for column ""status"", then status will not appear in the Parsons Table that is returned.


## To Reproduce
Call the get_records method on a table/view with a certain column that have all null values.


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know): 0.21.0
* Environment name and version (e.g. Chrome 39, node.js 5.4): Pycharm CE 2021, Python 3.9
* Operating System and version (desktop or mobile): MacOSX 12.5.1 Desktop


## Additional Context
Add any other context about the problem here.


## Priority
low","[Label(name=""bug""), Label(name=""low priority""), Label(name=""connector update"")]",[],2
434,Strive Connector Class,https://api.github.com/repos/move-coop/parsons/issues/757,brittany bennett,757,open,2022-10-11 17:45:56+00:00,2023-07-24 17:31:44+00:00,,"### Overview

Strive is a popular broadcast SMS tool. [The API developer docs can be found here](https://developers.strivemessaging.org/#tag/p2ps). This PR represents the work to add a new connector class to Parsons.


### To Do
- [x] Determine why we can't pass params/horizontal filtering on get methjods
- [x] Finish all GET requests
- [ ] Add unit tests for all get request methods 
- [ ] Create documentation for new class





","[Label(name=""new connector"")]","[NamedUser(login=""thebbennett"")]",9
435,Update Code of Conduct contact email,https://api.github.com/repos/move-coop/parsons/issues/756,Shauna Gordon-McKeon,756,closed,2022-10-03 16:47:21+00:00,2022-10-13 13:01:24+00:00,2022-10-13 13:01:24+00:00,"This looks like a bigger change than it is because I moved some linebreaks around to preserve matching line lengths. I just replaced ""the project team at justin@movementcooperative.org"" with ""hr@movementcooperative.org"".",[],[],0
436,"[Bug] van.upload_scores() takes a Parsons Table, creating memory issues ",https://api.github.com/repos/move-coop/parsons/issues/755,elyse-weiss,755,open,2022-09-30 15:30:50+00:00,2022-11-17 20:34:15+00:00,,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
`van.upload_scores()` takes a Parsons Table as one of its inputs. When loading scores, you area almost always submitting a full state. We've experienced that this often creates memory issues when actually running scripts using this function. Seems like it may make more sense to have a csv or other file as the input.

## Detailed Description
N/A


## To Reproduce
Try loading scores to Texas or California ;)


## Your Environment
We are running this function within Civis Platform. 


## Additional Context
N/A


## Priority
Medium","[Label(name=""bug""), Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],2
437,"Updates to ActionKit, Braintree, and S3 connectors",https://api.github.com/repos/move-coop/parsons/issues/754,Kathy Nguyen,754,closed,2022-09-27 20:59:11+00:00,2022-10-14 16:08:14+00:00,2022-10-14 16:08:14+00:00,"Congregate improvements to the ActionKit, Braintree, and S3 connectors",[],[],1
438,Fixed GitHub connector API docs,https://api.github.com/repos/move-coop/parsons/issues/753,Andrew Schechtman-Rook,753,closed,2022-09-26 12:34:03+00:00,2022-10-21 01:45:02+00:00,2022-10-21 01:45:01+00:00,"Even though `_wrap_method` is not _explicitly_ used as a decorator with the @ syntax, `decorate_methods` uses it in [functionally the same way](https://github.com/move-coop/parsons/blob/4775b5dfbcf5560a7dc2a617ed7847e41abdf450/parsons/github/github.py#L31). So adding `@wraps` to the inner function of `_wrap_method` seems to do the trick to propagate the docstrings.

I built the docs locally and it seems to work:
![image](https://user-images.githubusercontent.com/923682/192277128-fccc7000-68b2-498a-8ed3-f7e9060dfc86.png)


I also ran all the unit tests and nothing seems to have broken. 

Closes #742. ",[],[],4
439,Update cache keys for CircleCI configuration,https://api.github.com/repos/move-coop/parsons/issues/752,Soren Spicknall,752,closed,2022-09-20 21:16:38+00:00,2022-09-22 17:56:01+00:00,2022-09-21 14:42:44+00:00,An attempt to fix CircleCI configuration currently causing false negatives on new PR builds.,[],[],0
440,Code-generating NGPVAN API client. ,https://api.github.com/repos/move-coop/parsons/issues/751,Alex Merose,751,open,2022-09-19 00:59:14+00:00,2022-11-17 20:35:01+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
NGPVAN appears to use the [Swaggar / OpenAPI](https://www.openapis.org/) spec to generate their documentation with readme.com. This means that if we could find access to the swaggar.json (e.g. the spec for their docs), we could use code generation tools ([1], [2], [3]) to automatically produce a Python API client for NGPVAN. 

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->

I spent some time seeing if there was some way that I could scrape the NGPVAN API from the website. Using Chome Devtools, I was able to find snippets of JSON of the API spec per each endpoint. Due to a cross-site-origin header, I was unable to script out a way to get the specs en mass. 

However, I noticed a strange pattern in their readme generated docs – namely this code: 
```
const sdk = require('api')('@ngpvan/v1.0#5r6sg7sl74xoc30');
```

Looking further, I found that [`api` is an npm library](https://www.npmjs.com/package/api) that makes use of the swagger api spec for the node SDK. By reverse engineering this library, I could probably get access to the swagger spec that NGPVAN has not made available. 

Indeed, this line of code reveals what that junk is at the end of the string above: 

https://github.com/readmeio/api/blob/a9c6ce28b3bcb6981e2135180c19f39ef6f025af/packages/api/src/fetcher.ts#L24

Thus, the URL for the swagger spec for NGPVAN is: https://dash.readme.com/api/v1/api-registry/5r6sg7sl74xoc30

## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
Code-generating an Python API Client has several benefits: 
- Saves on manually having to write a connector for every endpoint of the NGPVAN API
- If the API changes, all we have to do is re-run the code generation system to get the latest API client. 

It's worth saying, code-generation comes with tradeoffs, too: 
- Short of writing your own tool, using a generic code-generator often leaves you with opinions / cruft that you don't need or want. 
- Extra work would be required to bridge between Parson's tables and the code-generated output. It's likely that extra coding would be needed to adapt between the two paradigms, meaning more work beyond ""just regeneration"". 

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->

1. https://swagger.io/tools/swagger-codegen/
2. https://github.com/swagger-api/swagger-codegen
3. https://github.com/OpenAPITools/openapi-generator

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
low priority -- this is more of an R&D project :)


[1]: https://swagger.io/tools/swagger-codegen/
[2]: https://github.com/swagger-api/swagger-codegen
[3]: https://github.com/OpenAPITools/openapi-generator","[Label(name=""enhancement""), Label(name=""low priority"")]",[],2
441,APIConnector should handle retry logic in a standard way.,https://api.github.com/repos/move-coop/parsons/issues/750,Alex Merose,750,closed,2022-09-18 01:09:19+00:00,2022-11-17 20:36:44+00:00,2022-11-17 20:36:44+00:00,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
Requests dispatched with `APIConnector` should centrally handle retries with exponential backoff and jitter.

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->

As described in this comment (https://github.com/move-coop/parsons/pull/670#discussion_r973633500), retrying request errors is non-trivial. Further, this is a concern for every single API request (that would be nice to not have to put back on to the user). Since the `APIConnector` class is pending a refactor #736, I recommend that retry logic be included. 

## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
I noticed the second example of a error-prone retry pattern in the `mysql_to_googlesheets.py` script: 
https://github.com/move-coop/parsons/blob/1411927f818133e485e6f9571f1ef0fabb8ca9ff/useful_resources/sample_code/mysql_to_googlesheets.py#L43

Ideally, we'd include a 3p dependency to handle retries so we wouldn't need to reinvent the wheel (pun intended). Users could add a decorator to their functions to add retry logic throughout the codebase, but especially on the APIConnector. 


## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->
This implementation looks good to me: https://pypi.org/project/backoff/


## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
medium","[Label(name=""enhancement"")]",[],1
442,Fix #707 - Parsing of code blocks for Mobilize America documentation,https://api.github.com/repos/move-coop/parsons/issues/749,Ethan Yoo,749,closed,2022-09-17 13:43:00+00:00,2022-09-22 16:44:20+00:00,2022-09-22 16:44:20+00:00,Fixes [issue 707](https://github.com/move-coop/parsons/issues/707),[],[],0
443,A custom logging handler that writes data to standard log tables.,https://api.github.com/repos/move-coop/parsons/issues/748,Alex Merose,748,open,2022-09-17 04:33:45+00:00,2022-11-17 20:38:30+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
Create a `parsons` logging handler that writes `logging.log` messages to a database, using standard Python interfaces.

## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->

In the Mobilize to ActionNetwork sample, I noticed [a logging pattern](https://github.com/cmdelrio/parsons_etl_trainings/blob/main/Mobilize_to_ActionNetwork.py#L34) that has a few issues. Namely, what happens if there's a crash before log messages are written to the log table? Furthermore, wouldn't it be nice to get to use `logger.info` etc and have those messages appear in your data warehouse? Without duplicating code, you could capture all the console logs to a db. 

The standard way to implement this in Python is to create a custom [Handler class](https://docs.python.org/3/howto/logging.html#handlers). The interface could look something like: 
```
db = Redshift()
handler = ParsonsHandler(db, table_name='mobilize_schema.log_messages')
logger = logging.getLogger(__name__)
logger.addHandler(handler)
```


## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
- Code deduplication (uses standard logging for all log destinations)
- Less error prone
- Encourage logging practice

## Possible Implementation
<!--- Not obligatory, but suggest an idea for implementing addition or change -->
(rough sketch)
```
@dataclasses.dataclass
class ParsonsHandler(logging.Handler):
    db: BaseTable
    table_name: str

    def emit(record) -> None:
        self.db.copy(Table(vars(record)), table_name=self.table_name, **other_kwargs)

    def close() -> None:
        self.db.close()
```

Better still: you may be able to subclass and directly use this: https://docs.python.org/3/library/logging.handlers.html#logging.handlers.HTTPHandler !
## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
Low Priority – this is a nice-to-have.","[Label(name=""enhancement""), Label(name=""parsons core""), Label(name=""low priority"")]",[],0
444,Typo fixes for ETL best practices guide.,https://api.github.com/repos/move-coop/parsons/issues/747,Alex Merose,747,closed,2022-09-17 03:35:54+00:00,2022-09-22 16:40:30+00:00,2022-09-22 16:40:30+00:00,,[],[],0
445,Unit tests for #685,https://api.github.com/repos/move-coop/parsons/issues/746,Alex Merose,746,closed,2022-09-17 00:05:55+00:00,2022-11-22 22:35:15+00:00,2022-11-22 22:35:15+00:00,"I wrote unit tests to try to programmatically reproduce the issue described in #685. However, I was unable to identify the issue. ",[],[],1
446,Add use_env_token option to S3 object,https://api.github.com/repos/move-coop/parsons/issues/745,Sophie Waldman,745,closed,2022-09-16 19:54:18+00:00,2022-09-22 16:56:03+00:00,2022-09-22 16:56:03+00:00,"Previously this was only settable in AWSConnection, but you can't pass an AWSConnection to the S3 object.",[],[],1
447,installation issues with Parsons 3.10 on M1 Mac,https://api.github.com/repos/move-coop/parsons/issues/744,Shauna Gordon-McKeon,744,closed,2022-09-15 19:45:40+00:00,2022-09-22 16:31:18+00:00,2022-09-22 16:30:52+00:00,"Parsons doesn't work with Python 3.10 (on M1 Mac and probably elsewhere). Specifically seems to be an issue with new directory structures in the Collections standard library in 3.10 conflicting with older dependencies.  See [this StackOverflow page](https://stackoverflow.com/questions/69381312/in-vs-code-importerror-cannot-import-name-mapping-from-collections).  The packages we ran into trouble with were urllib3, botocore, and gspread.

## To Reproduce

Using a Mac (or possibly other OS) and Python 3.10, try installing Parsons. It should install, but when trying to use it and importing anything from Parsons you'll get an issue like `ImportError: cannot import name 'Mapping' from 'collections'`. 

Sorry I don't have a full stacktrace, but these are errors from dependencies, not a fix that can be made in Parsons itself. Found issues with urllib3, botocore and gspread before giving up and switching to Python 3.9.

## Your Environment

Wasn't mine, but: Mac with M1 chip, using Python 3.10, Parsons 0.21.0.

## Priority

High priority (or lower the priority but remove statements saying we support 3.10)","[Label(name=""bug""), Label(name=""low priority""), Label(name=""needs more info"")]",[],10
448,Update version number for 0.21.0 release,https://api.github.com/repos/move-coop/parsons/issues/743,Soren Spicknall,743,closed,2022-09-15 16:14:07+00:00,2022-09-15 16:25:58+00:00,2022-09-15 16:25:56+00:00,,[],[],0
449,Github autodocs not displaying,https://api.github.com/repos/move-coop/parsons/issues/742,Shauna Gordon-McKeon,742,closed,2022-09-13 15:33:28+00:00,2022-11-17 20:34:34+00:00,2022-11-17 20:34:34+00:00,"The Github connector has plenty of public methods, but not are appearing in the docs. 

## Detailed Description

If you look at the Github connector there's [plenty of public methods](https://github.com/move-coop/parsons/blob/main/parsons/github/github.py#L112) but they don't appear in the docs, which just shows the class itself:

![Screenshot from 2022-09-13 11-26-49](https://user-images.githubusercontent.com/1179362/189942772-f335f439-70d1-4027-8f7b-a3374a06f359.png)

## Additional Context

The [wrapgithub404 decorator](https://github.com/move-coop/parsons/blob/main/parsons/github/github.py#L36) may be interfering with autodoc? That's my best guess. May be interacting with the specific directives given [here](https://github.com/move-coop/parsons/blob/main/docs/github.rst#api). I found [this StackOverflow question](https://stackoverflow.com/questions/3687046/python-sphinx-autodoc-and-decorated-members) but don't have the time to chase down further.

## Priority

Medium priority. This affects usability of the Github connector, but OTOH I think it's a sparsely used connector. But maybe it's sparsely used because the docs don't work.  ¯\_(ツ)_/¯","[Label(name=""bug""), Label(name=""documentation""), Label(name=""medium priority"")]",[],0
450,Slack as_user deprecated,https://api.github.com/repos/move-coop/parsons/issues/741,,741,closed,2022-09-07 17:21:57+00:00,2024-07-22 01:29:22+00:00,2024-07-22 01:29:22+00:00,"According to this [Slack API documentation](https://api.slack.com/methods/chat.postMessage#arg_as_user), new Slack apps may not use the `as_user` argument. 

However, the weird thing about my experience is that I get an error when I set `as_user=False` but when I set it to `True`, it works (but doesn't impersonate my user, although that could be an App scopes issue).

I'm guessing we should keep the argument in our method for backwards compatibility, but maybe we should remove it from the API call if it's set to `False` as a default?

We might also consider making it possible to surface the entirety of `resp`, since the method only returns `resp['error']` as a message, which in my case only contained `invalid_arguments`. I had to uncover the fullness of `resp` to see a `deprecated_argument` item that pointed me in the right direction.","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update"")]",[],0
451, Don't auto-grab session token if it's not passed in,https://api.github.com/repos/move-coop/parsons/issues/740,Sophie Waldman,740,closed,2022-09-01 15:12:57+00:00,2022-09-16 20:10:00+00:00,2022-09-06 16:19:09+00:00,"It turns out that trying to grab a session token from the environment variable can break things when used in concert with Zappa asynchronous processing - the access key and secret key passed in may not match the temporary token generated for the asynchronous process. Thus, this change requires the token to be passed in, even if it's set as an environment variable. 

As described [here](https://github.com/zappa/Zappa#asynchronous-task-execution), with asynchronous task execution, Zappa spins up a completely separate Lambda instance. The session token is generated as part of that process, and (as far as I can tell) is based on the execution role of the Lambda function.
So when the asynchronous task is running, it has the `aws_session_token` set even though there wasn't one originally. I'm not sure why it isn't just ""None"".
In any case, the token doesn't match the access key ID and secret access key that I was being passed in, resulting in an InvalidTokenError: `An error occurred (InvalidToken) when calling the PutObject operation: The provided token is malformed or otherwise invalid`. It's not that the token is actually invalid, but it doesn't match the credentials being passed in.

I'd like to be able to force aws_session_token to None even if the environment variable is set.",[],[],6
452,mention build prereqs for psycopg2,https://api.github.com/repos/move-coop/parsons/issues/739,Ty Schlichenmeyer,739,closed,2022-08-31 21:26:36+00:00,2022-08-31 21:56:21+00:00,2022-08-31 21:52:04+00:00,"During a fresh dev setup on WSL (Ubuntu) i received an error while trying to install dependencies due to `psycopg2`'s [build prerequisites](https://www.psycopg.org/docs/install.html#build-prerequisites).  I added some clarifying documentation, but there may be other/better ways around this error than this PR which is a little bit clutter-y.  Feedback from the core team welcome.

Also not sure if this is an error other OSes might run into -- IIRC OSX comes with a lot of these libraries preinstalled.

",[],[],4
453,Enhancement: Allow Subfolders for Temp S3 Bucket in Redshift Operations,https://api.github.com/repos/move-coop/parsons/issues/738,Soren Spicknall,738,closed,2022-08-31 15:07:17+00:00,2022-08-31 19:17:54+00:00,2022-08-31 19:03:14+00:00,"This small change to the Redshift connector will allow users to direct the temporary S3 file storage used for Redshift copy operations to simulated S3 subfolders, rather than just a top-level bucket.","[Label(name=""enhancement""), Label(name=""connector update"")]",[],0
454,feat: scytl connector,https://api.github.com/repos/move-coop/parsons/issues/737,Adam Greenspan,737,closed,2022-08-30 23:57:36+00:00,2022-10-20 22:39:50+00:00,2022-10-20 22:35:16+00:00,"[Scytl](https://www.scytl.com/resources-and-references/customers/u-s-elections/), or Clarity Elections, is a company that creates a tool for publishing election results in real-time. It's used in the U.S. by several states and over 800 counties, including Georgia, Colorado, Arkansas, and Dallas County, Texas.

For each participating election administrator, they publish a site with results that can be published on election night. Unfortunately, while that site contains downloadable data, that data is formatted in a complex way, making it difficult for developers to fetch election results. In general, their results either come zipped in either an unformatted text file or a complex XML document. Summary results come as a zipped CSV, but just contain top-line results. The JSON that powers the site results is even more complicated.

This connector provides methods to download the latest election results from their site and formats them into readable lists of dictionaries, which can easily be converted into a Parsons Table or Pandas dataframe.

Because this connector is can be useful for live reporting, it also contains a polling feature. As long as the class is instantiated, it will only fetch results that are new since the previous fetch of that method. To skip this feature, set force_update to true on any of the fetch methods.

Note: please do not confuse with [Clarity Campaign Labs](https://www.claritycampaigns.com/)

Example microsite: https://results.enr.clarityelections.com/CO/105975/web.276603/","[Label(name=""new connector"")]",[],14
455,[Refactor] Improve ulitity of the APIConnector class,https://api.github.com/repos/move-coop/parsons/issues/736,Jason,736,open,2022-08-30 15:28:46+00:00,2023-08-03 19:40:12+00:00,,"The `APIConnector` class is a largely internal-to-Parsons class that is supposed to make writing connectors easier. Querying endpoints and parsing the results is a task most connectors need to do repeatedly. Making this easier allows the Parsons community to develop new connectors faster and maintain existing connectors with less effort. This is the goal of the `APIConnector` class, but it doesn't succeed in this yet.


## Detailed Description
The API Connector does not do much to actually make using API's easier. Here are a few examples:

If you create a connector that has a `data_key`, the APIConnector does not unwrap that data for you when you make a request:
``` python
client = APIConnector(my_url, data_key='data')

# Note how you still need to grab the data out of the 'data' key
data = client.get_request('orders')['data']
# There is a built in method in the client, but it's actually more typing to use
data = client.data_parse(client.get_request('orders'))
```

The `pagination_key` is also mostly unused.

If an API returns a payload like:
```
{
    data: { ... <data here> ... },
    next: <url you should call to get next 20 results>
}
```

then querying the API with your `APIConnector` like this:
``` python
client = APIConnector(my_url, pagination_key='next')
data = client.get_request('orders')
```
still only returns the first 20 results.


## Possible Implementation
([See notes for the 8/25/2022 contributor meeting](https://docs.google.com/document/d/1ovkdOJhBjpaC3sRzZwFFDvlfgYw1vuP_HsPiIdzYtPk/edit#)) Since `APIConnector` is used by most of the connectors in Parsons, we need to be careful to not break existing connectors while we refactor the class. This is the plan we came up with:

Create an initial PR doing the actual refactor:
1. Copy the existing class into a new `APIConnector` class in another module.
2. Refactor the new version of the class, leaving the old version in place. This way existing connectors won't be affected.
3. Submit for review and get feedback on the made changes and other potential improvements.

When this PR is merged, we can create more PR's migrating each of the existing connectors to the new `APIConnector.`

When all of the connectors are migrated, we can delete the old `APIConnector`.


## Priority
This is not time sensitive. But the sooner we update it, the fewer connectors will get written with the old version and need to be re-written.","[Label(name=""enhancement""), Label(name=""parsons core""), Label(name=""high priority""), Label(name=""needs discussion"")]","[NamedUser(login=""jafayer"")]",10
456,Simple docs for the SMTP connector,https://api.github.com/repos/move-coop/parsons/issues/735,Andrew Schechtman-Rook,735,closed,2022-08-27 13:22:52+00:00,2022-09-22 22:36:34+00:00,2022-09-22 16:51:29+00:00,"I added some really simple documentation for the STMP class. It's based on the docs for the Gmail connector, with tweaks where appropriate. I have fairly limited knowledge of both SMTP and the class itself, so if I'm missing anything obvious please let me know!

This, along with #703 and #717, closes #635. ",[],[],1
457,Add upsert_person method to ActionNetwork,https://api.github.com/repos/move-coop/parsons/issues/734,Shauna Gordon-McKeon,734,closed,2022-08-25 17:58:59+00:00,2022-09-26 18:45:24+00:00,2022-08-31 19:01:23+00:00,"Also adds deprecation warnings to update_person and add_person.

Addresses #706 

cc @maggiedp",[],[],0
458,Update Mobilize connector,https://api.github.com/repos/move-coop/parsons/issues/733,Jason,733,closed,2022-08-24 19:50:41+00:00,2022-09-07 15:20:09+00:00,2022-09-07 15:20:08+00:00,"This is a PR addressing issue #714. I did some research and the only connector method calling a defunct endpoint is `get_events_organization`, so I update that. In testing, I also found that for large results it throws a stack exception. This is fixed by inserting `Table.materialize()` in the loop building up the results. I also made this change to `get_events`, since it uses the same algorithm.

I decided not to refactor the class to use APIConnector for two reasons:
1. APIConnector assumes you will use the same header for all requests. This isn't true for this class, so it would need two separate instances.
2. Given that APIConnector doesn't handle pagination or data extraction for you, it didn't seem like it would simplify the code, even (1) notwithstanding. We're discussing this during the 8/25/2022 contributors meeting, so hopefully that will change soon and we can revisit this connector then.","[Label(name=""breaking change"")]",[],1
459,Always use utf-8 encoding writing to RS s3,https://api.github.com/repos/move-coop/parsons/issues/732,Jason,732,closed,2022-08-24 19:17:09+00:00,2022-09-09 14:52:54+00:00,2022-09-09 14:52:53+00:00,"I think I figured out why Redshift.copy gives an encoding error very frequently, like this:
```
Traceback (most recent call last):
  File ""C:\Users\JasonWalker\Programming\Workspace\one-off-scripts\mobilize\events.py"", line 28, in <module>
    main()
  File ""C:\Users\JasonWalker\Programming\Workspace\one-off-scripts\mobilize\events.py"", line 24, in main
    rs.copy(events, 'indivisible_test.org_events', if_exists='drop', compupdate=False)
  File ""c:\users\jasonwalker\programming\lib\parsons\parsons\databases\redshift\redshift.py"", line 539, in copy
    key = self.temp_s3_copy(tbl, aws_access_key_id=aws_access_key_id,
  File ""c:\users\jasonwalker\programming\lib\parsons\parsons\databases\redshift\rs_copy_table.py"", line 144, in temp_s3_copy
    local_path = tbl.to_csv(temp_file_compression='gzip')
  File ""c:\users\jasonwalker\programming\lib\parsons\parsons\etl\tofrom.py"", line 137, in to_csv
    petl.tocsv(self.table,
  File ""C:\Users\JasonWalker\Programming\lib\petl\petl\io\csv.py"", line 108, in tocsv
    tocsv_impl(table, source=source, encoding=encoding, errors=errors,
  File ""C:\Users\JasonWalker\Programming\lib\petl\petl\io\csv_py3.py"", line 44, in tocsv_impl
    _writecsv(table, source=source, mode='wb', **kwargs)
  File ""C:\Users\JasonWalker\Programming\lib\petl\petl\io\csv_py3.py"", line 62, in _writecsv
    writer.writerow(row)
  File ""C:\Python310\lib\encodings\cp1252.py"", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\u2028' in position 757: character maps to <undefined>
```
Redshift.copy winds up calling petl.to_csv , the result of which is copied to s3 and then transferred to Redshift. The acceptinvchars argument to Redshift.copy (@Soren Spicknall has pointed this out past times I've had this problem) does insert the acceptinvchars statement to the SQL to load the CSV into Redshift. However, the encoding for writing the temp .csv is not specified, so it fails if it encounters non-ascii characters. ",[],[],2
460,Update from 2.8.5 to 2.9.3,https://api.github.com/repos/move-coop/parsons/issues/731,Shauna Gordon-McKeon,731,closed,2022-08-23 19:10:17+00:00,2022-09-26 18:45:24+00:00,2022-09-09 14:53:29+00:00,"Opening this PR on behalf of @Jason94. We've had tons of trouble with psycopg2-binary, maybe updating to the most recent version will fix some of them.",[],[],1
461,Cormac mobilecommons connector,https://api.github.com/repos/move-coop/parsons/issues/730,Cormac Martinez del Rio,730,closed,2022-08-18 17:01:00+00:00,2023-10-25 18:06:52+00:00,2023-10-25 18:06:52+00:00,Thank you Daniel! Let me know if I can clarify anything. ,"[Label(name=""new connector"")]",[],3
462,[Bug] Census Geocode API Not Working,https://api.github.com/repos/move-coop/parsons/issues/729,,729,closed,2022-08-15 22:23:57+00:00,2023-05-19 18:24:48+00:00,2023-05-19 18:24:47+00:00,"<!--- Provide a general summary of the bug you're experiencing in the Title above -->
When I attempted to use the Census Geocode API a few weeks ago I was unable to due to an error about the field names but the field names I used matched the documentation exactly 

## Detailed Description
<!--- Provide a detailed description of the bug you are experiencing - tell us what you expected to happen, and what happened instead of that -->
I created a parsons table to pass to the`geocode_address_batch()` function and I received the following error. 
<img width=""1440"" alt=""Screen Shot 2022-08-05 at 11 39 21 AM"" src=""https://user-images.githubusercontent.com/62267774/184727244-f8328843-b4ca-4be9-aa73-3cd66c11aaeb.png"">

## To Reproduce
<!--- Describe the steps that lead to experiencing the bug -->
Here is the code I ran:
```
from parsons import Table, Redshift
from canalespy import setup_environment, logger
import os

CG=CensusGeocoder()
rs=Redshift()
tb = Table()

to_geocode=tb.from_redshift(""""""
select * from [schema].[table]"""""") 

CG.geocode_address_batch(to_geocode)
```

The table in redshift looks like this 
![Screen Shot 2022-08-15 at 4 18 53 PM](https://user-images.githubusercontent.com/62267774/184728208-304c6ed2-b0a7-4635-b1fe-421f864e9ab7.png)


## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in, such as the following bullet points -->
* Version of Parsons used (if you know): latest
* Environment name and version (e.g. Chrome 39, node.js 5.4): I dont know
* Operating System and version (desktop or mobile): Desktop


## Additional Context
Add any other context about the problem here.


## Priority
Please indicate whether fixing this bug is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you need it addressed by.
Medium! It was VERY time consuming to geocode all those records via other means","[Label(name=""bug""), Label(name=""medium priority""), Label(name=""connector update"")]",[],3
463,"Convert None to """" in convert_columns_to_str",https://api.github.com/repos/move-coop/parsons/issues/728,Jason,728,closed,2022-08-12 13:53:15+00:00,2022-08-24 19:18:34+00:00,2022-08-24 19:18:34+00:00,"The function `parsons.Table.convert_columns_to_str()` converts `None` to `""None""`. I think that converting it to the empty string is more intuitive and more helpful. It does make it harder to deserialize the result of `convert_columns_to_str`, but the `str` function is not generally deserializable anyway.","[Label(name=""breaking change"")]",[],3
464,Addition: Accept Civis default naming conventions for environmental variables ,https://api.github.com/repos/move-coop/parsons/issues/727,brittany bennett,727,open,2022-08-11 17:28:01+00:00,2022-11-17 20:46:43+00:00,,"## Detailed Description
Parsons expects certain naming conventions for it's environmental variables. Take the Redshift class. It expects the following environmental variables: REDSHIFT_USERNAME, REDSHIFT_PASSWORD, REDSHIFT_HOST, REDSHIFT_DB, and REDSHIFT_PORT.  

However, I am one of the many Civis users in this community. When you set up the ""Redshift"" environmental variable in Civis as a databse cred, it creates the following environmental variables: REDSHIFT_ID, REDSHIFT_NAME, REDSHIFT_HOST, REDSHIFT_CREDENTIAL_ID, REDSHIFT_CREDENTIAL_USERNAME, REDSHIFT_CREDENTIAL_PASSWORD., REDSHIFT_DATABASE, REDSHIFT_PORT


There are a few differences here between the environmental variables names that Parsons is expecting and the default names in Civis. Namely:

| Parsons Expects This | Civis Creates This           |
|----------------------|------------------------------|
| REDSHIFT_DB          | REDSHIFT_DATABASE            |
| REDSHIFT_USERNAME    | REDSHIFT_CREDENTIAL_USERNAME |
| REDSHIFT_PASSWORD    | REDSHIFT_CREDENTIAL_PASSWORD |


## Context
Because of this, I have to write some weird code in my Parsons scripts that I expect to run as a container script in Civis. Namely:

```
# If running on container, load this env
try:
    os.environ[""REDSHIFT_DB""] = os.environ[""REDSHIFT_DATABASE""]
    os.environ[""REDSHIFT_USERNAME""] = os.environ[""REDSHIFT_CREDENTIAL_USERNAME""]
    os.environ[""REDSHIFT_PASSWORD""] = os.environ[""REDSHIFT_CREDENTIAL_PASSWORD""]
    os.environ[""S3_TEMP_BUCKET""] = os.environ[""S3_TEMP_BUCKET""]
    van_key = os.environ[""VAN_PASSWORD""]

# If running locally, load this env
except KeyError:
    van_key = os.environ[""VAN_API_KEY""]

```


## Possible Implementation
We could edit the init method of Redshift here

https://github.com/move-coop/parsons/blob/main/parsons/databases/redshift/redshift.py

```
try:

self.username = username or os.environ['REDSHIFT_USERNAME'] or os.environ['REDSHIFT_CREDENTIAL_USERNAME'] 

self.password = password or os.environ['REDSHIFT_PASSWORD'] or os.environ['REDSHIFT_CREDENTIAL_PASSWORD'] 

self.host = host or os.environ['REDSHIFT_HOST']

self.db = db or os.environ['REDSHIFT_DB'] or os.environ['REDSHIFT_DATABASE']

self.port = port or os.environ['REDSHIFT_PORT']

```
## Priority
This may only be important to me but it would make my code a lot cleaner. Thank you <3 ","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""low priority"")]",[],1
465,overwrite argument in van.upload_saved_list_rest() not evaluated?,https://api.github.com/repos/move-coop/parsons/issues/726,Lydia-Rose Kesich,726,open,2022-08-11 02:46:11+00:00,2022-11-17 20:47:10+00:00,,"van.upload_saved_list_rest() fails for me when overwriting an existing list, even when the overwrite argument is populated with the list id. The lists in question are owned by the API user and were originally created with van.upload_saved_list_rest().

I can successfully overwrite lists by writing a script for the VAN REST API that doesn't use the Parsons function, so I don't think there's an issue on the VAN end.

This code (minus the overwrite argument) was used successfully to create the list. I didn't change anything except adding the overwrite argument and populating it. It fails with the error ""Saved list already exists. Set overwrite argument to list ID or change list name"".

```
van.upload_saved_list_rest(
    tbl,
    url_type='S3',
    folder_id=3071,
    list_name='2022 Mobilization Expansion',
    description='household members of uncontacted mobi targets',
    callback_url=os.environ[""CALLBACK_URL""],
    columns=[""VANID""],
    id_column='VANID',
    delimiter='csv',
    header=True,
    overwrite=723800,
    bucket=os.environ[""S3_TEMP_BUCKET""]
    )
```

This code successfully overwrites a saved list. (Not included--Table.to_s3_csv() to create a zipped csv in S3 bucket, and a boto3 function to get the signed url to pass to the VAN API)

```
payload = {
    ""description"": ""update van saved lists"",
    ""file"": {
        ""fileName"": file,
        ""hasHeader"":True,
        ""sourceUrl"":source_url,
        ""columnDelimiter"":""CSV"",
        ""columns"":[{""name"": ""VANID""}]
    },
    ""actions"":
    [
        {
            ""actionType"": ""LoadSavedListFile"",
            ""listName"": list_name,
            ""listDescription"": list_desc,
            ""personIdType"": ""VANID"",
            ""personIdColumn"": ""VANID"",
            ""folderId"":""3071"",
            ""overwriteExistingListId"":list_id
        }
    ],
    ""listeners"":
    [
        {
            ""type"": ""URL"",
            ""value"": os.environ[""CALLBACK_URL""]
        }
    ]
}
headers = {
    ""Accept"": ""application/json"",
    ""Content-Type"": ""application/json"",
    ""Authorization"": ""Basic REDACTED""
}

response = requests.post(url, json=payload, headers=headers)
```
","[Label(name=""bug""), Label(name=""medium priority""), Label(name=""connector update"")]",[],0
466,Don't auto-grab session token if it's not passed in,https://api.github.com/repos/move-coop/parsons/issues/725,Sophie Waldman,725,closed,2022-08-10 20:09:06+00:00,2022-09-01 15:13:23+00:00,2022-09-01 15:13:23+00:00,"It turns out that trying to grab a session token from the environment variable can break things when used in concert with Zappa asynchronous processing - the access key and secret key passed in may not match the temporary token generated for the asynchronous process. Thus, this change forces the token to be explicitly set instead of automatically pulling from the environment variable.",[],[],8
467,New helper module with some reusable functions,https://api.github.com/repos/move-coop/parsons/issues/724,Shauna Gordon-McKeon,724,closed,2022-07-28 20:11:02+00:00,2023-06-06 19:48:57+00:00,2023-06-06 19:48:56+00:00,"I would like for Parsons to include more reusable code which helps users with transformations and analysis. I have placed some examples in a new `etl_helpers` folder. 

I am looking for feedback on:

1) the overall goal of providing more helper code for users
2) the architecture here. is `etl_helpers` a decent name? I wanted to distinguish from the existing `utilities` which is primarily for code that helps Parsons contributors enhance Parsons connectors.
3) the specific helper functions I added

Once the overall approach is approved, and I have feedback on the specific helper functions, I will write some tests for them as well as incorporate them into the documentation.

Note: I copied two datetime functions from `utilities/datetime.py` which I didn't even know were there before. Eventually with enough notice we should remove the `utilities/datetime.py` file.

Checklist:

- [x] initial PR for review
- [ ] confirm architecture
- [ ] confirm specific functions
- [ ] add tests
- [ ] add docs",[],[],4
468,parsons.Table 'select_row' method [Feature/Addition],https://api.github.com/repos/move-coop/parsons/issues/723,Jason,723,open,2022-07-28 19:36:35+00:00,2022-11-17 20:47:26+00:00,,"We should add a method to select a single row from a Table and return it as a dictionary.

## Detailed Description
In my experience, writing code like this is very common:
`table.select_rows(lambda r: r['col'] == value).to_dicts()[0]`

For general usability and ease-of-use for new users, this deserves a method on the Table class. 

## Possible Implementation
A few implementation questions exist:

**What should happen if no values are found?**
It could return `None`, an empty dictionary, or raise an error.

I think that returning None is the most obvious. But the advantage of returning an empty dictionary is you can write something like this:
`table.select_row('name', user_name).get('phone')`
and it will pass even if `user_name` is not found, returning `None` for the phone number. This is also an acceptable solution because `{}` is False in Python's boolean logic. So this code snippet would print ""not found"" if `user_name` is not in the table:

```
if table.select_row('name', user_name):
    print(""Found the user"")
else:
    print(""Not found"")
```

**What should happen if multiple values are found?**

IMO raising an error makes the most sense. We would note in the documentation that it is up to the user to make sure that their column is unique. Returning a list of multiple dictionaries would negate the purpose of this method. Another option would be to return the one that is found first in the table.

**What should the function signature be?**

I propose a signature like this, allowing a caller to match a single column and value, or specifying multiple columns that must match the corresponding value:

`def select_row(column_specifier: str | List[str], value_specifier: str | List[str]) -> dict`

Example usages:
`tbl.select_row('first_name', 'Steve')`

`tbl.select_row(['phone', 'email'], ['123-123-1212', 'benjobango@email.com'])`

## Priority
Low. I am willing to work on this. I'm mostly posting the issue so we can have a discussion about the signature of the method.","[Label(name=""enhancement""), Label(name=""low priority"")]","[NamedUser(login=""Jason94"")]",3
469,Direct Users to Parsons Community Resources on Sync Failures [Feature/Addition],https://api.github.com/repos/move-coop/parsons/issues/722,Jason,722,open,2022-07-28 19:22:29+00:00,2023-04-06 20:03:54+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->
**General Summary:**
We should provide a way for any Parsons user to easily tell us about an issue when they run into a broken connector call. This can be easily achieved by adding a default error message to all of our connectors that has an encouraging message and a link to our GitHub issues page, Slack, website, or something.

**Detailed Description & Possible Implementation:**
With the way that connectors are written, they will almost always raise an error if they encounter an issue with the API they are trying to call. We can add default error wrapping to all of our connector methods that:

1. Catches an error raised during the call
2. Prints an encouraging message and helpful link to the log or stdout
3. Re-raises the error that was caught to return it to the usual Python error handling. (We definitely need this step, or connectors will break the usual Python error handling flow.)

For a specific implementation, most connectors use APIConnector. If we add this to the methods in APIConnector, that should suffice for those. For the older connectors that don't use APIConnector, this will need to be added separately. I think that the easiest way to do this is via a Python decorator that does the above steps around the wrapped method. This could be quickly added to any methods in APIConnector that independently make API requests and api-requesting methods of any rogue-connectors.

**Context:**
Many of the Parsons connectors were created to address specific use cases, and aren't broadly used by the Parsons community or TMC. We want to support these connectors. However, without regular use by active members of the Parsons community, it is difficult for us to know whether a connector is being used by non-community Parsons users. (A recent example is the partially broken Mobilize connector.) Since these users are not already active on our Slack, GitHub, or other communication forums, they are less likely to let us know that something is broken. Reaching out to those Parsons users is a bigger project, but one thing we can do right now is make it easier to engage with the Parsons community for assistance when you find that something is broken.

**Priority:**
Low priority, but also probably low effort.","[Label(name=""enhancement""), Label(name=""medium priority"")]",[],1
470,HOTFIX Sendmail tests linting,https://api.github.com/repos/move-coop/parsons/issues/721,Soren Spicknall,721,closed,2022-07-28 17:13:25+00:00,2022-08-31 19:17:51+00:00,2022-07-28 19:07:25+00:00,"A previous PR, #717, didn't kick off all three rounds of testing in our CI when filed for some reason. I'll look into configuration to resolve that, but in the meantime these fixes are necessary to bring the project back into pep8 compliance. No functionality differences, just linting. Should be a quick review!",[],[],0
471,Bugfix for Redshift upsert alter_varchar_columns cascade issue,https://api.github.com/repos/move-coop/parsons/issues/720,Soren Spicknall,720,closed,2022-07-27 20:15:09+00:00,2022-08-31 19:17:53+00:00,2022-07-28 14:17:19+00:00,"We have a utility parameter on the Redshift `copy` and `upsert` functions that allows you to automatically cascade changes onto dependent views when varchar columns are widened. By default it's set to False, but I discovered that when set to True it was cascading on _every_ `copy` and `upsert` call using that parameter, not just the ones where column widening was actually needed. This changes the behavior to nest the cascade operation within the loop that determines which (if any) columns need to be widened.",[],[],0
472,Create a CallHub Connector [Feature/Addition],https://api.github.com/repos/move-coop/parsons/issues/719,Melissa Woods,719,open,2022-07-26 16:16:22+00:00,2023-05-04 02:24:17+00:00,,"<!--- Provide a general summary of the proposed Parsons addition in the Title above -->


## Detailed Description
<!--- Provide a detailed description of the change or addition you are proposing -->
CallHub is a political texting and phonebanking tool. They offer [integrations](https://callhub.io/app-directory/) to many commonly used tools in the progressive community already, like VAN and Action Network, but it would be helpful to have a Parsons connector for groups that may need something more tailored.

[Here is CallHub's website](https://callhub.io/)
[Here is the API documentation](https://developer.callhub.io/reference/create-new-contact).

## Context
<!--- Why is this change important to you? How would you use it? -->
<!--- How can it benefit other users? -->
One specific use case for this connector would be maintaining a Do Not Call phonebook in Callhub with data from community sources that is not housed within an organization's other outreach tools, such as data provided to an organization from coalition partners or data exchanges.

## Priority
<!--- Please indicate whether adding this feature is high, medium, or low priority for you. If the issue is time-sensitive for you, please let us know when you want it addressed by. -->
Medium","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""new connector"")]","[NamedUser(login=""mkwoods927"")]",2
473,[Feature/Addition] Create Donorbox Class,https://api.github.com/repos/move-coop/parsons/issues/718,elyse-weiss,718,closed,2022-07-21 20:58:18+00:00,2024-01-31 21:16:55+00:00,2024-01-31 21:16:55+00:00,"A TMC member asked about syncing Donorbox data. I figured it would therefore be good to put a low priority issue in Parsons for class creation.

## Detailed Description
https://github.com/donorbox/donorbox-api

## Context

## Possible Implementation

## Priority
Low priority","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""new connector"")]","[NamedUser(login=""shaunagm"")]",0
474,Sendmail connector is now an abstract base class,https://api.github.com/repos/move-coop/parsons/issues/717,Andrew Schechtman-Rook,717,closed,2022-07-19 22:11:09+00:00,2022-07-28 14:17:09+00:00,2022-07-28 14:17:09+00:00,"After the work I did in #703 and a [subsequent Slack discussion](https://tmc-parsons.slack.com/archives/C018UUEMPJ5/p1657762357561899), here is a PR that makes the Sendmail connector an abstract base class. It also removes the connector from the parsons global `init`, since users aren't supposed to call it directly. I _think_ this covers all the usages of it, and the tests all passed locally, but if I missed something please don't hesitate to point it out.",[],[],0
475,"Fix small bug with postal addresses in add_person, plus under-documented tags param",https://api.github.com/repos/move-coop/parsons/issues/716,Shauna Gordon-McKeon,716,closed,2022-07-13 21:51:29+00:00,2022-09-26 18:45:43+00:00,2022-07-27 20:57:08+00:00,"Fixes two things which tripped me working with the Action Network connector and with `add_person` specifically. The first is just an underdocumented param, the second is an actual bug which causes the user to be unable to add postal addresses when adding people to AN.",[],[],0
476,Remove leading and trailing whitespace from table and schema name in RS utility method table_exists_with_connection(),https://api.github.com/repos/move-coop/parsons/issues/715,Nikki Everett,715,closed,2022-07-11 13:06:24+00:00,2022-09-26 18:45:43+00:00,2022-07-12 21:03:54+00:00,"I ran into a very weird bug where splitting the Redshift table name string passed to an rs.copy() call over two lines in order to make my code PEP 8 compliant meant the table name had a leading whitespace character when passed to table_exists_to_connection(), meaning that the latter check failed, because no table with a leading whitespace in the name exists. This in turn caused rs.copy() to think it needed to create a new table even though one already existed, which caused a database error.

I think this small change should ensure that even if the table name string is split over two lines, the table name will not contain extra whitespace characters that will cause the lookup in pg_tables to fail.","[Label(name=""bug"")]","[NamedUser(login=""neverett"")]",0
477,Mobilize America connector needs refactoring,https://api.github.com/repos/move-coop/parsons/issues/714,Shauna Gordon-McKeon,714,closed,2022-07-09 20:39:32+00:00,2022-09-07 15:22:32+00:00,2022-09-07 15:22:32+00:00,"As mentioned in https://github.com/move-coop/parsons/pull/708, the Mobilize connector has been partly broken for at least a little while. I've fixed some issues but plenty remain, largely endpoints no longer working because the Mobilize API has been changed (for example, `get_events_organization` is now supposed to hit `organizations/<org_id>/events`, instead of hitting `events/` with org_id passed in as a parameter (that old use was [deprecated](https://github.com/mobilizeamerica/api#deprecated-list-all-public-events).)

I recommend re-writing the connector to use the `APIConnector()` utility and double-checking all existing endpoints/updating tests.

## To Reproduce

To reproduce the changed endpoint issue, set up authentication to Mobilize (I'm using a staging account) and call `mobilize.get_events_organization(organization_id=your_org_id`. It should give you an error:

```
   line 66, in _request_paginate
    while r.json()['next']:
      KeyError: 'next'
```

At least for me, what's happening is that the old endpoint no longer returns the [expected format](https://github.com/mobilizeamerica/api#format) with 'next' and 'previous' parameters. Switching the endpoint to `organizations/<org_id>/events` gets it working.

## Your Environment


* Version of Parsons used (if you know): parsons installed from github main (so 0.19 but whatever the commit history is as of today, 7/9/2022)
* Environment name and version (e.g. Chrome 39, node.js 5.4): python 3.8.10
* Operating System and version (desktop or mobile): Ubuntu 20.04


## Priority
High priority - it seems pretty important to have a working Mobilize connector","[Label(name=""bug""), Label(name=""high priority""), Label(name=""connector update"")]","[NamedUser(login=""Jason94"")]",2
478,Update setup.py for new release: 0.20.0,https://api.github.com/repos/move-coop/parsons/issues/713,Shauna Gordon-McKeon,713,closed,2022-07-09 18:00:49+00:00,2022-09-26 18:45:44+00:00,2022-07-12 14:47:56+00:00,,[],[],0
479,Update docker image version referenced in testing,https://api.github.com/repos/move-coop/parsons/issues/712,Soren Spicknall,712,closed,2022-07-08 20:28:07+00:00,2022-07-22 20:59:15+00:00,2022-07-08 20:46:53+00:00,"As a fix for errors in CircleCI, we previously updated the Docker image type referenced to a newer setup. We attempted to update our baseline for testing to Python 3.10 alongside that shift, but some references to 3.7 remain in the CI config file. This PR fixes that issue.",[],[],0
480,HOTFIX Sendmail pep8 Compliance,https://api.github.com/repos/move-coop/parsons/issues/711,Soren Spicknall,711,closed,2022-07-08 19:26:35+00:00,2022-07-22 20:59:15+00:00,2022-07-08 20:14:12+00:00,"Due to failing CI configuration, I didn't catch in #703 that a few lines exceeded pep8 guidelines. This fixes the issue so that the main branch of the repo can return to a passing state.",[],[],0
481,Remove leading and trailing whitespace from table and schema name in RS utility method table_exists_with_connection(),https://api.github.com/repos/move-coop/parsons/issues/710,Nikki Everett,710,closed,2022-07-07 20:03:21+00:00,2022-07-11 13:03:27+00:00,2022-07-11 13:02:41+00:00,"I ran into a very weird bug where splitting the Redshift table name string passed to an `rs.copy()` call over two lines in order to make my code PEP 8 compliant meant the table name had a leading whitespace character when passed to `table_exists_to_connection()`, meaning that the latter check failed, because no table with a leading whitespace in the name exists. This in turn caused `rs.copy()` to think it needed to create a new table even though one already existed, which caused a database error.

I think this small change should ensure that even if the table name string is split over two lines, the table name will not contain extra whitespace characters that will cause the lookup in `pg_tables` to fail.","[Label(name=""bug"")]","[NamedUser(login=""neverett"")]",1
482,Fix Redshift column alteration bug,https://api.github.com/repos/move-coop/parsons/issues/709,Soren Spicknall,709,closed,2022-07-07 19:23:24+00:00,2022-07-22 20:59:17+00:00,2022-07-22 20:58:02+00:00,"Under current behavior, if you try to copy or upsert a Parsons table to Redshift with alter_table set to True, it'll take the incoming column widths and send them to Redshift as alteration queries. This will fail when a varchar column coming in from the Parsons table exceeds Redshift's maximum column width of 65535.

It's desirable to use the alter_table argument in combination with truncatecolumns to enact a sequence as follows:

1. Incoming Parsons table has values that are too wide for the current column width _and_ too wide for Redshift varchar constraints
2. Parsons resizes the destination columns to take up the maximum allowed width in Redshift, and then
3. Parsons successfully pushes the table to Redshift, truncating the column values that exceed 65535 characters.

As it stands, step 2 fails in this sequence because an invalid column width is sent to Redshift within an alteration query. This caps the allowable length of that alteration query. With this change, copy and upsert calls with truncatecolumns=False would still behave as expected, but calls with that argument set to true _and_ alter_table=True would exhibit the new behavior outlined above.",[],[],0
483,"Update Mobilize connector URI, fix display of errors, fix auth issue",https://api.github.com/repos/move-coop/parsons/issues/708,Shauna Gordon-McKeon,708,closed,2022-07-06 21:22:51+00:00,2022-09-26 18:45:44+00:00,2022-07-09 17:54:17+00:00,"1 -  Most importantly, this updates the base URI for the Mobilize connector, which was no longer working. **WARNING** this changes from HTTP to HTTPS which may cause authentication issues? I don't know. The Mobilize Connector has its own custom requests code and doesn't use APIConnector, which I assume would handle the change fine.

2 - Also fixed an issue where the auth information wasn't getting passed from `request_paginate` to `request`, which lead to auth errors.

3 -  Also (and this is the original reason I opened this PR):

The Mobilize connector uses the following logic to check for request errors:

`if 'error' in r.json():`

Unfortunately, for most unsuccessful API calls (including 404 file not found, 401 authentication, etc) no json is returned. Because we try to parse it anyway, the user sees an unreadable JSONDecodeError, like `simplejson.errors.JSONDecodeError: Expecting value: line 1 column 1 (char 0)` instead of the more helpful error message that requests would otherwise return.

This fix uses the helper method provided by requests, `r.raise_for_status()` to surface most errors, before moving on to the line `if 'error' in r.json():` (which may still be helpful in a subset of cases).

The better fix here is to rewrite the Mobilize connector to use the APIConnector utility, which was created precisely so we wouldn't have to duplicate this kind of general request logic within each connector, but that's a bigger change I don't have time for right now.",[],[],0
484,Mobilize America quickstart guide is missing,https://api.github.com/repos/move-coop/parsons/issues/707,Shauna Gordon-McKeon,707,closed,2022-07-06 18:28:30+00:00,2022-09-22 16:44:21+00:00,2022-09-22 16:44:21+00:00,"The text for the Mobilize connector suggests there's a quickstart guide (""as seen below"") but there's no code example as there is with other connectors.

I'd expect something like, for example, what NGPVAN has:

![Screenshot from 2022-07-06 14-26-33](https://user-images.githubusercontent.com/1179362/177618074-3d621959-cfe2-43cf-9e02-a8ac8636d792.png)

But instead we see this for Mobilize:

![Screenshot from 2022-07-06 14-25-07](https://user-images.githubusercontent.com/1179362/177617958-2fb7ad9d-bb7a-415d-b2f4-903ee39be10c.png)

If you look at the [raw file](https://raw.githubusercontent.com/move-coop/parsons/main/docs/mobilize_america.rst), the code blocks are there, but for some reason they're not showing up. Probably the best way to figure out the issue is to compare to a code block that is showing up.

This is medium priority, since it's not a bug but it's good to have good documentation.","[Label(name=""bug""), Label(name=""documentation""), Label(name=""good first issue"")]",[],0
485,ActionNetwork.update_person() does not update the person,https://api.github.com/repos/move-coop/parsons/issues/706,Maggie Dart-Padover,706,closed,2022-07-01 19:49:32+00:00,2022-09-22 16:29:43+00:00,2022-09-22 16:29:43+00:00,"When I run update_person(), it returns the person but does not actually update the value for that person.

## To Reproduce
```
from parsons import ActionNetwork
an = ActionNetwork()
person_id='d52040b6-0168-401f-8e7a-b49b2b4e15a1'
person_update=an.update_person(person_id, custom_fields={'2022Letter': 'test'})
person_update=an.update_person(person_id, family_name='test')
person_update['family_name']
person_update['custom_fields']['2022Letter']

```
person_update['family_name'] returns 'Charles' and person_update['custom_fields']['2022Letter'] returns a keyerror because 2022Letter was not added to the custom fields. It does successfully return the json for the correct person, it just doesn't update the person.

## Your Environment
* Version of Parsons used (if you know): parsons==0.19.0
* Operating System and version (desktop or mobile): Ubuntu 21.10
* Is there information about my Action Network group that would be helpful here? I'm just using the key from the API page

## Additional Context
I'm using the lower level function api.get_request() with the Action Network connector and it's working fine. 

## Priority
I think this is a high priority, since it's a bug that makes this function not work but I'm not sure how popular this function is. I'm working on workarounds that don't use parsons for updating people in AN.","[Label(name=""bug""), Label(name=""high priority""), Label(name=""connector update"")]",[],7
486,add phone to ngpvan apply canvass result,https://api.github.com/repos/move-coop/parsons/issues/705,Binny Zupnick,705,closed,2022-06-30 00:22:39+00:00,2022-07-07 20:13:14+00:00,2022-07-07 20:13:14+00:00,"This is a fix for issue #525.

It's pretty straightforward in that it creates a new names param `phone` for `apply_canvass_result` which continues to send that to `apply_response`. In `apply_response` there is some logic to ensure that the `phone` param was provided if the `canvassTypeId` is either `1` or `37`. If it wasn't provided, throw an error, if it was provided, append it to the request as the API needs.",[],[],0
487,Fix error in VAN Target Export method.,https://api.github.com/repos/move-coop/parsons/issues/704,CM Lubinski,704,closed,2022-06-24 21:15:20+00:00,2022-07-08 16:08:46+00:00,2022-07-08 16:08:46+00:00,"It looks like this method assumes VAN return a *list* of exported files
rather than a single target, but VAN currently only returns individual
targets: https://docs.ngpvan.com/reference/targetexportjobsexportjobid

They might've changed the API since this was last updated? In any event,
these tweaks return correct data in my recent testing!",[],[],1
488,Added tests for SendMail notification connector,https://api.github.com/repos/move-coop/parsons/issues/703,Andrew Schechtman-Rook,703,closed,2022-06-17 15:02:24+00:00,2022-07-09 13:22:40+00:00,2022-07-08 16:07:07+00:00,"Hi!

I added tests for the `SendMail` class, as part of addressing #635. I installed pytest-cov and got the following coverage results (with the tests added):

```bash
$ pytest test/ --cov=parsons/notifications/ --cov-report=term-missing
<full output snipped>
Name                                Stmts   Miss  Cover   Missing
-----------------------------------------------------------------
parsons/notifications/__init__.py       0      0   100%
parsons/notifications/gmail.py         34     15    56%   29, 32, 41-42, 67-85
parsons/notifications/sendmail.py     116      0   100%
parsons/notifications/slack.py         73     11    85%   28, 114, 145-147, 195-197, 220-224, 229
parsons/notifications/smtp.py          34      8    76%   37-41, 59-62
-----------------------------------------------------------------
```

I was going to add documentation for `SendMail` as discussed in #635, but when I looked at it closely it seems like this class is more like an abstract base class than something that users should use directly. You have to inherit from it and override `_send_message` in order for it to work, for one thing. So I figured I'd just PR the tests and then ask — should there be user-facing documentation for this class? ",[],[],2
489,More flexible importing in parsons/__init__.py,https://api.github.com/repos/move-coop/parsons/issues/702,CM Lubinski,702,closed,2022-06-11 18:42:06+00:00,2022-07-18 16:59:12+00:00,2022-07-18 16:59:11+00:00,"This implements the try-imports method @schuyler1d [described](https://github.com/move-coop/parsons/pull/632#issuecomment-1065494121) in a sustainable way, thereby allowing
```python
from parsons import ...
```
for users without all of the dependencies.

I'm a little torn on the ""Table"" callout -- it's definitely cooky to
make circular dependencies /easy/, but I figured this is the least
invasive approach.

I won't update the docs until this is on pypi!",[],[],2
490,Add missing check to distinguish between path string and json string in Google connector utility,https://api.github.com/repos/move-coop/parsons/issues/701,Shauna Gordon-McKeon,701,closed,2022-06-09 18:52:41+00:00,2022-06-10 15:34:44+00:00,2022-06-10 15:34:44+00:00,"_Re-doing this PR due to deleted fork weirdness._

I think I figured out the issue that I and others were having with Google authentication. A previous PR changed the way the [utilities.py](https://github.com/move-coop/parsons/blob/v0.19.0/parsons/google/utitities.py) file worked and all strings, including path strings, were being interpreted as json strings. So when we set a path as the environmental variable, setup_google_application_credentials saved it as though it was itself the json credentials file, which of course led to parsing errors down the line.

I'd like to get this reviewed and a new release cut relatively quickly, as until this is released on pypi anyone trying to set up the google connector is going to run into errors.

",[],[],0
491,Attempt to filter expand_fields before stringifying them,https://api.github.com/repos/move-coop/parsons/issues/700,Shauna Gordon-McKeon,700,closed,2022-06-08 22:15:02+00:00,2022-06-10 15:35:15+00:00,2022-06-10 15:35:15+00:00,"[Two years ago](https://github.com/move-coop/parsons/commit/fb54f5b849198cdb3dbb2a2e33603ce6e493fa2a), filter logic was added to the `get_person` VAN method to remove fields that didn't exist on particular databases. Unfortunately, it was added after the logic which turns `expand_fields` from an array into a string, leading to weirdness like this:

![Screenshot from 2022-06-08 18-13-06](https://user-images.githubusercontent.com/1179362/172726490-a99713a4-89fa-480b-98e7-abc49bd79545.png)

This in turn leads to 404 errors. Has `get_person` just always returned 404 errors for the last two years? Is no one using the `expand_fields` parameter? Have people been silently working around this? I am deeply curious.

Note: this PR also makes it so the print statement displays `vanid` instead of none when a vanid is passed in.",[],[],1
492,Add missing check to distinguish between path string and json string in Google connector utility,https://api.github.com/repos/move-coop/parsons/issues/699,Shauna Gordon-McKeon,699,closed,2022-06-08 18:48:56+00:00,2022-06-09 18:53:14+00:00,2022-06-09 18:53:13+00:00,"I think I figured out the issue that I and others were having with Google authentication. A previous PR changed the way the [utilities.py](https://github.com/move-coop/parsons/blob/v0.19.0/parsons/google/utitities.py) file worked and all strings, including path strings, were being interpreted as json strings. So when we set a path as the environmental variable, `setup_google_application_credentials` saved it as though it was itself the json credentials file, which of course led to parsing errors down the line.

I'd like to get this reviewed and a new release cut relatively quickly, as until this is released on pypi anyone trying to set up the google connector is going to run into errors.",[],[],3
493,Update docs to account for wheels.,https://api.github.com/repos/move-coop/parsons/issues/698,CM Lubinski,698,closed,2022-06-06 21:08:57+00:00,2022-06-09 14:23:04+00:00,2022-06-09 14:23:04+00:00,"Parsons is hosted on pypi as both a source distribution and a binary
wheel package. The later is faster, but skips over configurations in
setup.py (more specifically: whatever environment vars were set on the
building machine are the ones that stuck). To allow
`PARSONS_LIMITED_DEPENDENCIES` to work, we need to tell pip that it
should skip the wheel for Parsons.",[],[],0
494,Fix issue templates,https://api.github.com/repos/move-coop/parsons/issues/697,Shauna Gordon-McKeon,697,closed,2022-06-02 18:44:35+00:00,2022-06-02 21:39:06+00:00,2022-06-02 21:35:45+00:00,"In #668 the template metadata was accidentally removed. This PR adds them back and, hopefully, gets the templates working.",[],[],1
495,VAN: Fix codes bugs.,https://api.github.com/repos/move-coop/parsons/issues/696,,696,closed,2022-06-01 18:01:55+00:00,2022-06-07 17:49:29+00:00,2022-06-07 17:49:29+00:00,"Two small bug fixes:

- `get_code` : The method for the underlying VANConnector class was misnamed.
- `apply_code`: The _is_applicable_ parameter was misnamed and thus, not applying.

Furthermore, I might want to rework the _supported_entities parameter. It requests constructing a dict, which is sort of unnecessary and not keeping with the common sense abstraction of Parsons. But, that would come in another PR.","[Label(name=""bug"")]",[],3
496,Big Query NoneType Mapping Bug,https://api.github.com/repos/move-coop/parsons/issues/695,,695,closed,2022-05-31 19:54:45+00:00,2022-06-23 17:10:01+00:00,2022-06-23 16:48:45+00:00,"To ensure that columns with different data types are stored accordingly after running the copy function to big query, we have added a mapping line to the python to Big Query data type map `'NoneType': 'String'`.

The other alternative was a set of codes that filtered out NoneType from the get_column_stats function and kept the first type that was available other than NoneType, i.e., string, int, float.  

We landed on the addition below as it will provide more flexibility when multiple data types are present in the column. ",[],[],3
497,added materialization methods to docs,https://api.github.com/repos/move-coop/parsons/issues/694,Andrew Schechtman-Rook,694,closed,2022-05-26 23:06:37+00:00,2022-06-02 18:52:42+00:00,2022-06-02 18:52:38+00:00,"Hi 👋!

I took a stab at adding the missing documentation to close #662. I added a sentence to the lazy loading section as well as a table with the two methods. I also added _just_ the two materialize methods from `Table` to the API section at the end. I built the docs locally and things look ok.

Hopefully that's the right place! If not, let me know and I can try again. Thanks!
",[],[],1
498,Create Auth0 connector,https://api.github.com/repos/move-coop/parsons/issues/693,Kathy Nguyen,693,closed,2022-05-26 19:00:39+00:00,2022-06-23 17:35:58+00:00,2022-06-23 16:47:45+00:00,Auth0 connector with a few methods,[],[],2
499,Bump pyjwt from 2.0.1 to 2.4.0,https://api.github.com/repos/move-coop/parsons/issues/692,,692,closed,2022-05-25 05:31:36+00:00,2022-05-26 17:51:03+00:00,2022-05-26 17:50:55+00:00,"Bumps [pyjwt](https://github.com/jpadilla/pyjwt) from 2.0.1 to 2.4.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/jpadilla/pyjwt/releases"">pyjwt's releases</a>.</em></p>
<blockquote>
<h2>2.4.0</h2>
<h2>Security</h2>
<ul>
<li>[CVE-2022-29217] Prevent key confusion through non-blocklisted public key formats. <a href=""https://github.com/jpadilla/pyjwt/security/advisories/GHSA-ffqj-6fqr-9h24"">https://github.com/jpadilla/pyjwt/security/advisories/GHSA-ffqj-6fqr-9h24</a></li>
</ul>
<h2>What's Changed</h2>
<ul>
<li>Add support for Python 3.10 by <a href=""https://github.com/hugovk""><code>@​hugovk</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/699"">jpadilla/pyjwt#699</a></li>
<li>Don't use implicit optionals by <a href=""https://github.com/rekyungmin""><code>@​rekyungmin</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/705"">jpadilla/pyjwt#705</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/708"">jpadilla/pyjwt#708</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/710"">jpadilla/pyjwt#710</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/711"">jpadilla/pyjwt#711</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/712"">jpadilla/pyjwt#712</a></li>
<li>documentation fix: show correct scope for decode_complete() by <a href=""https://github.com/sseering""><code>@​sseering</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/661"">jpadilla/pyjwt#661</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/716"">jpadilla/pyjwt#716</a></li>
<li>Explicit check the key for ECAlgorithm by <a href=""https://github.com/estin""><code>@​estin</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/713"">jpadilla/pyjwt#713</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/720"">jpadilla/pyjwt#720</a></li>
<li>api_jwk: Add PyJWKSet.<strong>getitem</strong> by <a href=""https://github.com/woodruffw""><code>@​woodruffw</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/725"">jpadilla/pyjwt#725</a></li>
<li>Update usage.rst by <a href=""https://github.com/guneybilen""><code>@​guneybilen</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/727"">jpadilla/pyjwt#727</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/728"">jpadilla/pyjwt#728</a></li>
<li>fix: Update copyright information by <a href=""https://github.com/kkirsche""><code>@​kkirsche</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/729"">jpadilla/pyjwt#729</a></li>
<li>Docs: mention performance reasons for reusing RSAPrivateKey when encoding by <a href=""https://github.com/dmahr1""><code>@​dmahr1</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/734"">jpadilla/pyjwt#734</a></li>
<li>Fixed typo in usage.rst by <a href=""https://github.com/israelabraham""><code>@​israelabraham</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/738"">jpadilla/pyjwt#738</a></li>
<li>Add detached payload support for JWS encoding and decoding by <a href=""https://github.com/fviard""><code>@​fviard</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/723"">jpadilla/pyjwt#723</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/740"">jpadilla/pyjwt#740</a></li>
<li>Raise DeprecationWarning for jwt.decode(verify=...) by <a href=""https://github.com/akx""><code>@​akx</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/742"">jpadilla/pyjwt#742</a></li>
<li>Don't mutate options dictionary in .decode_complete() by <a href=""https://github.com/akx""><code>@​akx</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/743"">jpadilla/pyjwt#743</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/748"">jpadilla/pyjwt#748</a></li>
<li>Replace various string interpolations with f-strings by <a href=""https://github.com/akx""><code>@​akx</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/744"">jpadilla/pyjwt#744</a></li>
<li>Update CHANGELOG.rst by <a href=""https://github.com/hipertracker""><code>@​hipertracker</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/751"">jpadilla/pyjwt#751</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/hugovk""><code>@​hugovk</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/699"">jpadilla/pyjwt#699</a></li>
<li><a href=""https://github.com/rekyungmin""><code>@​rekyungmin</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/705"">jpadilla/pyjwt#705</a></li>
<li><a href=""https://github.com/sseering""><code>@​sseering</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/661"">jpadilla/pyjwt#661</a></li>
<li><a href=""https://github.com/estin""><code>@​estin</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/713"">jpadilla/pyjwt#713</a></li>
<li><a href=""https://github.com/woodruffw""><code>@​woodruffw</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/725"">jpadilla/pyjwt#725</a></li>
<li><a href=""https://github.com/guneybilen""><code>@​guneybilen</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/727"">jpadilla/pyjwt#727</a></li>
<li><a href=""https://github.com/dmahr1""><code>@​dmahr1</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/734"">jpadilla/pyjwt#734</a></li>
<li><a href=""https://github.com/israelabraham""><code>@​israelabraham</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/738"">jpadilla/pyjwt#738</a></li>
<li><a href=""https://github.com/fviard""><code>@​fviard</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/723"">jpadilla/pyjwt#723</a></li>
<li><a href=""https://github.com/akx""><code>@​akx</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/742"">jpadilla/pyjwt#742</a></li>
<li><a href=""https://github.com/hipertracker""><code>@​hipertracker</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/751"">jpadilla/pyjwt#751</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/jpadilla/pyjwt/compare/2.3.0...2.4.0"">https://github.com/jpadilla/pyjwt/compare/2.3.0...2.4.0</a></p>
<h2>2.3.0</h2>
<h2>What's Changed</h2>
<ul>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/700"">jpadilla/pyjwt#700</a></li>
<li>Add exception chaining by <a href=""https://github.com/ehdgua01""><code>@​ehdgua01</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/702"">jpadilla/pyjwt#702</a></li>
<li>Revert &quot;Remove arbitrary kwargs.&quot; by <a href=""https://github.com/auvipy""><code>@​auvipy</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/701"">jpadilla/pyjwt#701</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/jpadilla/pyjwt/blob/master/CHANGELOG.rst"">pyjwt's changelog</a>.</em></p>
<blockquote>
<h2><code>v2.4.0 &lt;https://github.com/jpadilla/pyjwt/compare/2.3.0...2.4.0&gt;</code>__</h2>
<p>Security</p>
<pre><code>
- [CVE-2022-29217] Prevent key confusion through non-blocklisted public key formats. https://github.com/jpadilla/pyjwt/security/advisories/GHSA-ffqj-6fqr-9h24
<p>Changed</p>
<pre><code>
- Explicit check the key for ECAlgorithm by @estin in https://github.com/jpadilla/pyjwt/pull/713
- Raise DeprecationWarning for jwt.decode(verify=...) by @akx in https://github.com/jpadilla/pyjwt/pull/742

Fixed
~~~~~

- Don't use implicit optionals by @rekyungmin in https://github.com/jpadilla/pyjwt/pull/705
- documentation fix: show correct scope for decode_complete() by @sseering in https://github.com/jpadilla/pyjwt/pull/661
- fix: Update copyright information by @kkirsche in https://github.com/jpadilla/pyjwt/pull/729
- Don't mutate options dictionary in .decode_complete() by @akx in https://github.com/jpadilla/pyjwt/pull/743

Added
~~~~~

- Add support for Python 3.10 by @hugovk in https://github.com/jpadilla/pyjwt/pull/699
- api_jwk: Add PyJWKSet.__getitem__ by @woodruffw in https://github.com/jpadilla/pyjwt/pull/725
- Update usage.rst by @guneybilen in https://github.com/jpadilla/pyjwt/pull/727
- Docs: mention performance reasons for reusing RSAPrivateKey when encoding by @dmahr1 in https://github.com/jpadilla/pyjwt/pull/734
- Fixed typo in usage.rst by @israelabraham in https://github.com/jpadilla/pyjwt/pull/738
- Add detached payload support for JWS encoding and decoding by @fviard in https://github.com/jpadilla/pyjwt/pull/723
- Replace various string interpolations with f-strings by @akx in https://github.com/jpadilla/pyjwt/pull/744
- Update CHANGELOG.rst by @hipertracker in https://github.com/jpadilla/pyjwt/pull/751

`v2.3.0 &amp;lt;https://github.com/jpadilla/pyjwt/compare/2.2.0...2.3.0&amp;gt;`__
-----------------------------------------------------------------------

Fixed
~~~~~

- Revert &amp;quot;Remove arbitrary kwargs.&amp;quot; `[#701](https://github.com/jpadilla/pyjwt/issues/701) &amp;lt;https://github.com/jpadilla/pyjwt/pull/701&amp;gt;`__

Added
~~~~~

- Add exception chaining `[#702](https://github.com/jpadilla/pyjwt/issues/702) &amp;lt;https://github.com/jpadilla/pyjwt/pull/702&amp;gt;`__

`v2.2.0 &amp;lt;https://github.com/jpadilla/pyjwt/compare/2.1.0...2.2.0&amp;gt;`__
-----------------------------------------------------------------------

&amp;lt;/tr&amp;gt;&amp;lt;/table&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;... (truncated)&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Commits&lt;/summary&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/83ff831a4d11190e3a0bed781da43f8d84352653&quot;&gt;&lt;code&gt;83ff831&lt;/code&gt;&lt;/a&gt; chore: update changelog&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/4c1ce8fd9019dd312ff257b5141cdb6d897379d9&quot;&gt;&lt;code&gt;4c1ce8f&lt;/code&gt;&lt;/a&gt; chore: update changelog&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/96f3f0275745c5a455c019a0d3476a054980e8ea&quot;&gt;&lt;code&gt;96f3f02&lt;/code&gt;&lt;/a&gt; fix: failing advisory test&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/9c528670c455b8d948aff95ed50e22940d1ad3fc&quot;&gt;&lt;code&gt;9c52867&lt;/code&gt;&lt;/a&gt; Merge pull request from GHSA-ffqj-6fqr-9h24&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/24b29adfebcb4f057a3cef5aaf35653bc0c1c8cc&quot;&gt;&lt;code&gt;24b29ad&lt;/code&gt;&lt;/a&gt; Update CHANGELOG.rst (&lt;a href=&quot;https://github-redirect.dependabot.com/jpadilla/pyjwt/issues/751&quot;&gt;#751&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/31f5acb8fb3ec6cdfe2b1b0a4a8f329b5f3ca67f&quot;&gt;&lt;code&gt;31f5acb&lt;/code&gt;&lt;/a&gt; Replace various string interpolations with f-strings (&lt;a href=&quot;https://github-redirect.dependabot.com/jpadilla/pyjwt/issues/744&quot;&gt;#744&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/5581a31c21de70444c1162bcfa29f7e0fc86edda&quot;&gt;&lt;code&gt;5581a31&lt;/code&gt;&lt;/a&gt; [pre-commit.ci] pre-commit autoupdate (&lt;a href=&quot;https://github-redirect.dependabot.com/jpadilla/pyjwt/issues/748&quot;&gt;#748&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/3d4d82248f1120c87f1f4e0e8793eaa1d54843a6&quot;&gt;&lt;code&gt;3d4d822&lt;/code&gt;&lt;/a&gt; Don't mutate options dictionary in .decode_complete() (&lt;a href=&quot;https://github-redirect.dependabot.com/jpadilla/pyjwt/issues/743&quot;&gt;#743&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/1f1fe15bb41846c602b3e106176b2c692b93a613&quot;&gt;&lt;code&gt;1f1fe15&lt;/code&gt;&lt;/a&gt; Add a deprecation warning when jwt.decode() is called with the legacy verify=...&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jpadilla/pyjwt/commit/35fa28e59d99b99c6a780d2a029a74d6bbba8b1e&quot;&gt;&lt;code&gt;35fa28e&lt;/code&gt;&lt;/a&gt; [pre-commit.ci] pre-commit autoupdate (&lt;a href=&quot;https://github-redirect.dependabot.com/jpadilla/pyjwt/issues/740&quot;&gt;#740&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Additional commits viewable in &lt;a href=&quot;https://github.com/jpadilla/pyjwt/compare/2.0.1...2.4.0&quot;&gt;compare view&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;

&lt;br /&gt;
</code></pre>


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyjwt&package-manager=pip&previous-version=2.0.1&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
500,Bug in GoogleBigQuery Copy NoneType Handling,https://api.github.com/repos/move-coop/parsons/issues/691,,691,closed,2022-05-18 21:58:01+00:00,2022-06-23 17:13:12+00:00,2022-06-23 17:12:55+00:00,"The [internal method](https://github.com/move-coop/parsons/blob/main/parsons/google/google_bigquery.py#L289), `_generate_schema()`, uses the Parsons Table method `get_columns_type_stats()` and parses it with the `BIGQUERY_TYPE_MAP` reference, rather than going through the whole `detect_data_type()` [process](https://github.com/move-coop/parsons/blob/main/parsons/databases/database/database.py#L140) the other database types use.

Unfortunately, `BIGQUERY_TYPE_MAP` doesn't account for columns with NoneType under certain circumstances. AS a result, `copy()` can fail if there are any non-string columns that also contain NoneType values.

It's possible that the better long-term solution would be to incorporate the BigQuery process. But a quick workaround is also possible that filters out NoneTypes before any application of `BIGQUERY_TYPE_MAP`","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update"")]","[NamedUser(login=""ydamit""), NamedUser(login=""scmora"")]",2
501,GoogleBigQuery columns property erroring on empty tables,https://api.github.com/repos/move-coop/parsons/issues/690,,690,open,2022-05-18 19:47:14+00:00,2022-05-23 15:00:13+00:00,,"The core of the issue is that `GoogleBigQuery`'s `query()` method [returns](https://github.com/move-coop/parsons/blob/main/parsons/google/google_bigquery.py#L248) `None` if it queries a table with no rows of data, even if that table otherwise exists.

I propose a solution where `query()` checks `INFORMATION_SCHEMA.COLUMNS` for data on the table if `batch` is empty and `got_results` is still False. If found, it would be relatively straightforward to return a table with headers only instead of `None`. This should fix the `columns` property and be applicable in other contexts as well.","[Label(name=""bug""), Label(name=""medium priority""), Label(name=""connector update"")]","[NamedUser(login=""ydamit""), NamedUser(login=""scmora"")]",2
502,GoogleBigQuery Copy Bug,https://api.github.com/repos/move-coop/parsons/issues/689,,689,closed,2022-05-18 19:28:38+00:00,2022-05-18 19:30:11+00:00,2022-05-18 19:30:11+00:00,"The `GoogleBigQuery` connector can be set up with proper credentials such that the `query()` method seems to work fine, but `copy()` throws a credentials error.

Further investigation revealed that the issue revolves around [the credentials-handling utility function](https://github.com/move-coop/parsons/blob/main/parsons/google/utitities.py), `setup_google_application_credentials()` not being set up to be called multiple times in the same session (especially an issue with the `DBSync` class, as it chunks large data sets).

### Proposed Solution
There may be a better solution out there, but the override that worked for me was to have the function stop doing anything if it's already found a .json file, by adding an `elif` to the function's condition:
```
if (type(credentials) is dict):
    credentials = json.dumps(credentials)
elif credentials.endswith('.json'):
    return True
```

### Deeper Investigation
[Initializing](https://github.com/move-coop/parsons/blob/main/parsons/google/google_bigquery.py#L85) the `GoogleBigQuery` class calls `setup_google_application_credentials()` immediately. This function saves the Google credential into a .json file and then stores the filename as the `GOOGLE_APPLICATION_CREDENTIALS` environment variable - which is also the default env var it checks for the credential itself.

The problem is that the `copy()` method [instantiates](https://github.com/move-coop/parsons/blob/main/parsons/google/google_bigquery.py#L157) the `GoogleCloudStorage` class, which _also_ calls `setup_google_application_credentials()` when [initialized](https://github.com/move-coop/parsons/blob/main/parsons/google/google_cloud_storage.py#L37). The result is that the `GoogleCloudStorage` object is looking for a credential but instead finds a file path & name instead.

This is fine the _first_ time it happens, since the Google's clients actually _want_ to [read credentials from a file](https://cloud.google.com/storage/docs/reference/libraries#setting_up_authentication). But because `setup_google_application_credentials()` is always saving what it finds in the `GOOGLE_APPLICATION_CREDENTIALS` env var into a file and saving that file's path back into the same env var, we now have a .json file that is just the path to another .json file.","[Label(name=""bug""), Label(name=""medium priority""), Label(name=""connector update"")]","[NamedUser(login=""ydamit""), NamedUser(login=""scmora"")]",0
503,updated parakimo to latest,https://api.github.com/repos/move-coop/parsons/issues/688,Binny Zupnick,688,closed,2022-05-15 19:26:16+00:00,2022-05-17 18:46:10+00:00,2022-05-17 18:46:09+00:00,updating the `parakimo` package to `2.10.4` to get a fix of theirs to solve issue https://github.com/move-coop/parsons/issues/683,[],[],3
504,Re-named Phone2Action to Capitol Canary,https://api.github.com/repos/move-coop/parsons/issues/687,Binny Zupnick,687,closed,2022-05-15 17:34:44+00:00,2022-06-21 19:20:17+00:00,2022-06-21 19:20:17+00:00,"This is a PR for issue https://github.com/move-coop/parsons/issues/669.

I couldn't find API docs anywhere so this PR assumes all of the URLs and environment variables maintain the `phone2action` branding. ",[],[],2
505,HOTFIX language classifiers in setup.py for PyPI release,https://api.github.com/repos/move-coop/parsons/issues/686,Soren Spicknall,686,closed,2022-05-11 18:38:54+00:00,2022-06-09 16:36:10+00:00,2022-05-11 18:43:25+00:00,Fixes an issue with missing commas in a recent change to supported Python versions in setup.py,[],[],0
506,Bug in Double application of Google Creds utility,https://api.github.com/repos/move-coop/parsons/issues/685,,685,closed,2022-05-11 14:00:32+00:00,2024-07-22 01:32:05+00:00,2024-07-22 01:32:05+00:00,"The `setup_google_application_credentials()` utility is applied twice in any use of the `GoogleBigQuery` connector that uses `copy()` or any other method that also instantiates the `GoogleCloudStorage` class. 

Both of these classes make use of the `setup_google_application_credentials()` utility, which works fine the first time it's applied. The result under most conditions is that the `GOOGLE_APPLICATION_CREDENTIALS` env var is populated with the path to the temporary .json file holding the credentials.

The problem is that the _second_ time this is applied, a temporary file is created whose contents are merely the path to the _first_ temporary file, and the various clients involved don't know what to do with that. They try to parse that nested file path as a JSON string, and of course it throws an error.

I believe this might be most easily fixed by adding another condition to the `if` block to handle this edge case:
```
elif credentials.endswith('.json'):
    return True
```","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update"")]","[NamedUser(login=""ydamit""), NamedUser(login=""scmora"")]",3
507,Silent Alter Table Failure on Case Mismatch,https://api.github.com/repos/move-coop/parsons/issues/684,,684,open,2022-05-10 20:42:51+00:00,2022-05-13 17:13:08+00:00,,,"[Label(name=""bug""), Label(name=""low priority""), Label(name=""connector update"")]",[],1
508,paramiko error:  blowfish has been deprecated,https://api.github.com/repos/move-coop/parsons/issues/683,lizette,683,closed,2022-05-09 16:41:06+00:00,2022-05-17 18:48:07+00:00,2022-05-17 18:48:07+00:00,"updated parsons package and when attempted to load `Redshift`, encountered this error in notebook: 

`CryptographyDeprecationWarning: Blowfish has been deprecated`
<img width=""1207"" alt=""Screen Shot paramiko error"" src=""https://user-images.githubusercontent.com/46300917/167457122-bafca593-9974-4cd3-a85b-8f1b5a2f9fad.png"">

Did some investigation and found this thread in the paramiko git. 
https://github.com/paramiko/paramiko/issues/2038
","[Label(name=""bug""), Label(name=""dependency update""), Label(name=""high priority""), Label(name=""futureproofing"")]",[],2
509,Update simple-salesforce version,https://api.github.com/repos/move-coop/parsons/issues/682,Shauna Gordon-McKeon,682,closed,2022-05-06 19:15:01+00:00,2022-05-12 19:00:31+00:00,2022-05-12 18:20:09+00:00,"Addresses #642 by bumping [simple-salesforce](https://github.com/simple-salesforce/simple-salesforce) version from 0.74.3 to 1.11.6. 

0.74.3 was published July 4 2019, so we've got about 3 years of [changes](https://github.com/simple-salesforce/simple-salesforce/blob/master/CHANGES). I went through and looked at the tests to make sure that the client interfaces hadn't introduced breaking changes and I don't believe they have, but I don't have a Salesforce account so I couldn't actually test that. 

I did note that we have two empty tests in `test_salesforce.py` and I considered trying to add them but I don't know enough about the API to mock the data.",[],[],2
510,Bump to version 0.19.0.,https://api.github.com/repos/move-coop/parsons/issues/681,CM Lubinski,681,closed,2022-05-05 01:34:12+00:00,2022-05-11 15:57:21+00:00,2022-05-11 15:57:20+00:00,"Since we tweaked which versions of Python we actively support (but are
still at major version zero), updating the minor version made sense to
me. This'll let us publish a new version to pypi!",[],[],0
511,"Add ""Intro to ETL"" training guide",https://api.github.com/repos/move-coop/parsons/issues/680,Shauna Gordon-McKeon,680,closed,2022-05-04 21:24:42+00:00,2022-05-12 18:06:10+00:00,2022-05-12 18:06:10+00:00,,[],[],0
512,Standardize how clients are referred to,https://api.github.com/repos/move-coop/parsons/issues/679,Shauna Gordon-McKeon,679,open,2022-05-03 16:57:09+00:00,2022-05-06 14:54:23+00:00,,"I haven't done an exhaustive review, but for example:

- The Salesforce connector stores the client info as a private variable `_client` ([ref](https://github.com/move-coop/parsons/blob/main/parsons/salesforce/salesforce.py#L43)), then gets the client via a method with property descriptor `def client()` ([ref](https://github.com/move-coop/parsons/blob/main/parsons/salesforce/salesforce.py#L200)). 
- The [ActBlue connector](https://github.com/move-coop/parsons/blob/main/parsons/actblue/actblue.py#L43), [Phone2Action](https://github.com/move-coop/parsons/blob/main/parsons/phone2action/p2a.py) and [Quickbase connector](https://github.com/move-coop/parsons/blob/main/parsons/quickbase/quickbase.py) have APIConnectors they all just set as attribute `client`.
- The [Action Network connector](https://github.com/move-coop/parsons/blob/main/parsons/action_network/action_network.py#L27) has an APIConnector it calls `api`. 
- [Van](https://github.com/move-coop/parsons/blob/main/parsons/ngpvan/van_connector.py#L31) also calls its client `api`. 
- I'm not sure what [PDI](https://github.com/move-coop/parsons/tree/main/parsons/pdi) is doing - it looks like it doesn't have a client at all (not even a Parsons helper client) but instead is handling requests/auth/session management itself.

The `client` terminology and the method of just setting it as `self.client` in `__init__` is definitely the most common, enough that I assumed it was already standard, but here's definitely lots of exceptions.

It would take a good bit of work, but I think we should pick a client method for all connectors, re-write existing connectors to use it, and write it up in the docs for future use. We could leave the wrappers/stubs of old methods to not break backwards compatibility (ie create the standard attribute `client` but keep an `api` attribute that points to `client`, for those that were using `api`).

Not super high priority, but having this level of consistency would make Parsons easier to use and contribute to.","[Label(name=""documentation""), Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""low priority""), Label(name=""connector update"")]",[],0
513,[DOC] Added Table of Contents to ReadMe,https://api.github.com/repos/move-coop/parsons/issues/678,Tomiwa Sanusi,678,closed,2022-04-28 13:01:09+00:00,2022-05-06 19:24:06+00:00,2022-05-06 19:24:06+00:00,Added Table of Contents to ReadMe docs to make navigation easier.,[],[],0
514,Update auth info for Google Sheets,https://api.github.com/repos/move-coop/parsons/issues/677,Shauna Gordon-McKeon,677,closed,2022-04-27 17:05:07+00:00,2022-05-17 18:39:11+00:00,2022-05-17 18:39:11+00:00,"I've been working with the Google Sheets connector a bit recently and found the authentication info was out of date. I've updated the guidance here as well as tweaked the intro to the Google connectors as a whole, including fixing a link and using slightly broader language.

Google Admin, BigQuery and Cloud Storage all have somewhat similar instructions to the old Google Sheets instructions. I wonder if they might be out of date too?",[],[],0
515,NGPVAN/EveryAction - Changed Entity Fails If No Data Found,https://api.github.com/repos/move-coop/parsons/issues/676,,676,open,2022-04-27 15:56:27+00:00,2022-11-17 20:49:01+00:00,,"Currently the [changed entity](https://github.com/move-coop/parsons/blob/main/parsons/ngpvan/changed_entities.py#L45) method fails there is no data that matches the dates provided. This happens quite frequently if run on daily basis.

I propose one of two solutions:

1. Return `None` if no data is found. 

2. Return an empty table with column headers if no data is found. This second solution is slightly tricky in that it would rely because it would only return the standard column headers and not any custom fields that were requested.

Thoughts?","[Label(name=""medium priority""), Label(name=""connector update""), Label(name=""needs discussion"")]",[],0
516,Remove VAN method workaround that stripped query parameters from URL,https://api.github.com/repos/move-coop/parsons/issues/675,Shauna Gordon-McKeon,675,closed,2022-04-27 15:43:35+00:00,2022-04-28 19:07:26+00:00,2022-04-28 19:07:26+00:00,"Updates  'upload_saved_list_rest' to remove the workaround.

Addresses https://github.com/move-coop/parsons/issues/513",[],[],3
517,TargetSmart 2022: SmartMatch,https://api.github.com/repos/move-coop/parsons/issues/674,Ben Stroud,674,closed,2022-04-27 15:14:05+00:00,2022-08-30 18:05:04+00:00,2022-08-30 18:05:04+00:00,"Adds support for performing TargetSmart SmartMatch list matching workflow executions. SmartMatch matches contact list records to Voterbase, TargetSmart's national database of voting age individuals.

Additional code review of the previously implemented TargetSmart routines has been done to modernize for 2022, including doc updates.",[],[],3
518,NGPVAN -- update_person_json function doesn't update person record,https://api.github.com/repos/move-coop/parsons/issues/673,harnoor singh,673,open,2022-04-27 00:04:43+00:00,2023-09-28 02:19:20+00:00,,"The update_person_json() function does not seem to function as the documentation indicates. When I use the function to attempt to update a contact (in the demo EA site using a sandbox the EA team setup for me) in the following way:

`van = VAN(db='MyCampaign')
van.update_person_json(id=101991312, match_json={'codes':None})`

I get a strange JSON output that is completely different from the actual person's record (it is shown below)

```
{'vanId': 101991312,
 'firstName': 'joe',
 'lastName': 'jack',
 'middleName': None,
 'suffix': None,
 'title': None,
 'sourceFileTitle': None,
 'sourceFileFirstName': None,
 'sourceFileMiddleName': None,
 'sourceFileLastName': None,
 'sourceFileSuffix': None,
 'contactMode': None,
 'organizationContactCommonName': None,
 'organizationContactOfficialName': None,
 'salutation': 'joe',
 'formalSalutation': 'joe jack',
 'additionalSalutation': None,
 'preferredPronoun': None,
 'pronouns': None,
 'envelopeName': 'joe jack',
 'formalEnvelopeName': 'joe jack',
 'additionalEnvelopeName': None,
 'contactMethodPreferenceCode': None,
 'nickname': None,
 'website': None,
 'professionalSuffix': None,
 'party': None,
 'employer': None,
 'occupation': None,
 'jobTitle': None,
 'sex': None,
 'dateOfBirth': None,
 'selfReportedRace': None,
 'selfReportedEthnicity': None,
 'selfReportedRaces': None,
 'selfReportedEthnicities': None,
 'selfReportedGenders': None,
 'selfReportedSexualOrientations': None,
 'selfReportedLanguagePreference': None,
 'emails': None,
 'phones': None,
 'addresses': None,
 'recordedAddresses': None,
 'identifiers': None,
 'codes': None,
 'customFields': None,
 'primaryCustomField': None,
 'contributionSummary': None,
 'suppressions': None,
 'caseworkCases': None,
 'caseworkIssues': None,
 'caseworkStories': None,
 'notes': None,
 'scores': None,
 'customProperties': None,
 'electionRecords': None,
 'membershipStatus': None,
 'organizationRoles': None,
 'districts': None,
 'surveyQuestionResponses': None,
 'finderNumber': None,
 'biographyImageUrl': None,
 'primaryContact': None}
```

But if I run  

`van.get_person(id=101991312)`

 I get the following which proves that the above JSON is not reflecting the updated record of the person

```
people INFO Getting person with  of 101991312 at url people/101991312

{'vanId': 101991312,
 'firstName': 'joe',
 'lastName': 'jack',
 'middleName': None,
 'suffix': None,
 'title': None,
 'sourceFileTitle': None,
 'sourceFileFirstName': None,
 'sourceFileMiddleName': None,
 'sourceFileLastName': None,
 'sourceFileSuffix': None,
 'contactMode': None,
 'organizationContactCommonName': None,
 'organizationContactOfficialName': None,
 'salutation': 'joe',
 'formalSalutation': 'joe jack',
 'additionalSalutation': None,
 'preferredPronoun': None,
 'pronouns': None,
 'envelopeName': 'joe jack',
 'formalEnvelopeName': 'joe jack',
 'additionalEnvelopeName': None,
 'contactMethodPreferenceCode': None,
 'nickname': None,
 'website': None,
 'professionalSuffix': None,
 'party': None,
 'employer': None,
 'occupation': None,
 'jobTitle': None,
 'sex': None,
 'dateOfBirth': None,
 'selfReportedRace': None,
 'selfReportedEthnicity': None,
 'selfReportedRaces': None,
 'selfReportedEthnicities': None,
 'selfReportedGenders': None,
 'selfReportedSexualOrientations': None,
 'selfReportedLanguagePreference': None,
 'emails': [{'type': 'P',
   'email': 'kate+test@sink.sendgrid.net',
   'dateCreated': '2022-04-11T10:39:00Z',
   'isPreferred': True,
   'isSubscribed': True,
   'subscriptionStatus': 'S'}],
 'phones': [],
 'addresses': [],
 'recordedAddresses': [],
 'identifiers': [],
 'codes': [{'codeId': 1002611,
   'parentCodeId': None,
   'name': 'test source code',
   'codeType': 'SourceCode'}],
 'customFields': [],
 'primaryCustomField': None,
 'contributionSummary': None,
 'suppressions': [],
 'caseworkCases': [],
 'caseworkIssues': [],
 'caseworkStories': [],
 'notes': [],
 'scores': None,
 'customProperties': [],
 'electionRecords': [],
 'membershipStatus': None,
 'organizationRoles': [],
 'districts': [{'districtFieldId': 0,
   'name': 'State',
   'parentFieldId': None,
   'isCustomDistrict': False,
   'districtFieldValues': [{'id': 'FL',
     'name': 'Florida',
     'parentId': None}]}],
 'surveyQuestionResponses': None,
 'finderNumber': None,
 'biographyImageUrl': None,
 'primaryContact': None}
```

When I walked through this issue with Shauna from the Parsons team she pointed out that this command doesn't appear to hit an endpoint for the API that would delete or update a contact's record according to Parson's documentation for the update_person_json() method. It appears that the documentation is incorrect or this function is not working exactly as it should. 

Just for reference, this is the [API's page of documentation](https://docs.ngpvan.com/reference/peoplevanidcodescodeid) that provides code which successfully (I have tested it and it works) deletes codes off of contacts' records, and this is the [page of the documentation]( for the API that is linked for the update_person_json() parson method. ","[Label(name=""bug""), Label(name=""high priority"")]",[],9
519,"""Create your own sandbox"" section missing from ""How to build a connector"" doc",https://api.github.com/repos/move-coop/parsons/issues/672,Nikki Everett,672,closed,2022-04-26 21:11:49+00:00,2022-04-29 16:05:20+00:00,2022-04-29 16:05:20+00:00,"The ""[Create your own sandbox](https://github.com/move-coop/parsons/blob/main/docs/build_a_connector.rst#create-your-own-sandbox)"" section is missing from the ""[How to build a connector](https://move-coop.github.io/parsons/html/stable/build_a_connector.html)"" doc page.","[Label(name=""documentation"")]",[],1
520,Add Parsons Docker Image to Documentation,https://api.github.com/repos/move-coop/parsons/issues/671,brittany bennett,671,closed,2022-04-26 20:09:54+00:00,2023-02-15 18:46:21+00:00,2023-02-15 18:46:21+00:00,"Hello,

Occasionally someone will ask about docker image requirements to run Parsons. TMC maintains an image `movementcooperative/parsons` that could be opened to non-TMC members. If so, we should add a note about the usage of this image in the Parsons documentation. 

Cheers ","[Label(name=""documentation""), Label(name=""good first issue""), Label(name=""medium priority"")]",[],1
521,Opt out phones in EA use case and sample script,https://api.github.com/repos/move-coop/parsons/issues/670,Melissa Woods,670,closed,2022-04-22 17:46:45+00:00,2022-12-28 18:23:33+00:00,2022-12-28 18:23:33+00:00,This PR adds a new use case and sample script for opting out phone numbers in EveryAction.,"[Label(name=""sample script"")]",[],9
522,Re-name Phone2Action to Capitol Canary,https://api.github.com/repos/move-coop/parsons/issues/669,elyse-weiss,669,closed,2022-04-21 21:08:20+00:00,2022-06-21 19:54:35+00:00,2022-06-21 19:54:35+00:00,"""now is the time to retire the Phone2Action name and move forward with a new, more focused identity as we continue to transform how policy is won. Call us Capitol Canary.""

we should probably re-name the connector? check out their API docs to confirm?","[Label(name=""good first issue""), Label(name=""connector update""), Label(name=""futureproofing"")]",[],3
523,Create bug and addition issue templates,https://api.github.com/repos/move-coop/parsons/issues/668,Soren Spicknall,668,closed,2022-04-14 19:44:09+00:00,2022-06-09 16:36:11+00:00,2022-05-09 20:49:53+00:00,,[],[],0
524,Fix for Google Dependencies,https://api.github.com/repos/move-coop/parsons/issues/667,Soren Spicknall,667,closed,2022-04-14 15:30:22+00:00,2022-06-09 16:36:11+00:00,2022-04-14 18:14:49+00:00,Corrects issues with Google dependencies that resulting in a build bug described in the Parsons Slack [here](https://tmc-parsons.slack.com/archives/C018CQD3G4E/p1649818619946079).,[],[],0
525,Update main branch references in README,https://api.github.com/repos/move-coop/parsons/issues/666,Soren Spicknall,666,closed,2022-04-13 14:47:08+00:00,2022-06-09 16:36:12+00:00,2022-04-13 15:17:37+00:00,,[],[],0
526,Remove unused function.,https://api.github.com/repos/move-coop/parsons/issues/665,CM Lubinski,665,closed,2022-04-13 02:45:23+00:00,2022-04-27 18:32:57+00:00,2022-04-27 18:32:57+00:00,"This function has a typo in it (missing a comma on 329), but since it's not
referenced, we can just delete it!",[],[],4
527,Support Python 3.9 and 3.10; drop 3.6,https://api.github.com/repos/move-coop/parsons/issues/664,CM Lubinski,664,closed,2022-04-12 18:57:01+00:00,2022-04-22 16:00:06+00:00,2022-04-22 16:00:06+00:00,"Attempting to `import parsons` from Python 3.10 currently errors out due to outdated dependencies, so I bumped up the facebook dep and some testing libraries. Unfortunately I don't have a Facebook ads account to verify no breaking changes, but the changelog _seemed_ innocuous. I've ran tests in Python 3.7 through 10 without problem.

It drops support for Python 3.6, which had completely different errors (including `google-cloud-storage` no longer supporting 3.6).

This also prevents pip from installing parsons on Python <3.7 or >3.10. We could set this to just >= 3.7, but it seems better to me to proactively claim support (particularly considering 3.10 support was broken).",[],[],0
528,VAN: Activist Code Apply - Omit Contact History,https://api.github.com/repos/move-coop/parsons/issues/663,,663,closed,2022-04-12 16:49:09+00:00,2022-04-22 15:34:05+00:00,2022-04-22 15:34:05+00:00,"The ``van.apply_activist_code`` method currently applies a contact response (e.g. canvass status) when it is run. This is problematic in that many use cases for applying activist codes are not the result of contacts. Instead they are arbitrary flags applied to records. This can be a pain as it introduces fake contact attempts which muck up reporting for organizations (i.e. me).

Here is an example of what happens when you currently apply an activist code.

![ss3](https://user-images.githubusercontent.com/5530043/163013340-3ae9e198-4bfc-4ba6-b16b-156491f7f9ad.png)

There was an old issue #445 which lifted up, but it seems like in the meantime, VAN has addressed it. This PR adds in a kwarg ``omit_contact`` that omits, by default, a contact response being added when an activist code is applied. 

I have coded this such that the default is to omit the contact, which, while it would represent a change in functionality compared to the previous version of the method, I believe is representative of the desired defaults of most use cases.","[Label(name=""enhancement"")]",[],1
529,Table.materialize() and Table.materialize_to_file() methods not showing up in docs,https://api.github.com/repos/move-coop/parsons/issues/662,Nikki Everett,662,closed,2022-04-08 21:41:51+00:00,2022-06-02 18:52:38+00:00,2022-06-02 18:52:38+00:00,It looks the `materialize()` and `materialize_to_file()` methods defined in https://github.com/move-coop/parsons/blob/main/parsons/etl/table.py do not appear in the docs.,"[Label(name=""bug""), Label(name=""documentation""), Label(name=""good first issue"")]",[],0
530,NGPVAN Class - Reflect EveryAction in Function Names,https://api.github.com/repos/move-coop/parsons/issues/661,elyse-weiss,661,open,2022-04-06 20:32:47+00:00,2022-04-08 17:00:26+00:00,,"Currently, our NGPVAN connector has a DB parameter where you denote whether you are hitting `MyVoters` or `EveryAction`. 

While in years gone by, it really did feel like MyVoters and MyCampaign (the EA ancestor) were two sides of the same coin, this does not reflect today's reality. I actually think we should split this connector into a VAN connector and EA connector. There are features that only exist in one and not the other, so the current setup may sow confusion. ","[Label(name=""medium priority""), Label(name=""connector update""), Label(name=""futureproofing"")]",[],0
531,Fix bug in 'make html',https://api.github.com/repos/move-coop/parsons/issues/660,Shauna Gordon-McKeon,660,closed,2022-04-06 17:15:24+00:00,2022-04-08 17:05:31+00:00,2022-04-08 17:05:31+00:00,"When I added versioning in #639 I intended to leave it so `make html` still works to quickly check your docs build, but I added a variable to the templates that only gets created when you `make deploy_docs`. This PR is a simple fix which checks that the variable is actually there and skips that template if not.",[],[],0
532,Add Azure to Parsons DBSync,https://api.github.com/repos/move-coop/parsons/issues/659,elyse-weiss,659,open,2022-04-06 13:53:59+00:00,2022-11-17 21:01:14+00:00,,"Given EveryAction Cloud is being set up in Azure, it would probably behoove lots of users to be able to DBSync using Parsons from Azure to BigQuery or Redshift. 
","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""needs more info"")]",[],1
533,Update README license to match docs.,https://api.github.com/repos/move-coop/parsons/issues/658,CM Lubinski,658,closed,2022-04-05 01:38:38+00:00,2022-04-05 20:37:07+00:00,2022-04-05 20:37:07+00:00,Also point to `main` instead of `master`.,[],[],0
534,run docs build on any change to master/main,https://api.github.com/repos/move-coop/parsons/issues/657,Shauna Gordon-McKeon,657,closed,2022-04-01 17:56:37+00:00,2022-04-01 19:58:13+00:00,2022-04-01 19:58:12+00:00,"Oversight in #639 - right now CircleCI only runs when there's a change to a release branch, we want to now build the docs every time, since we'll need to update 'latest'.",[],[],0
535,Remove built docs from git,https://api.github.com/repos/move-coop/parsons/issues/656,Shauna Gordon-McKeon,656,closed,2022-03-31 18:01:16+00:00,2022-04-01 20:15:16+00:00,2022-04-01 20:15:16+00:00,"I think the built docs started getting included in git because the relevant line in the gitignore was commented out and wrong. Fixed this, and untracked the currently tracked files. I *think* this should work? One can never be sure with git.",[],[],0
536,Action Network Docs Update,https://api.github.com/repos/move-coop/parsons/issues/655,Tomiwa Sanusi,655,closed,2022-03-31 09:26:30+00:00,2022-04-05 20:36:36+00:00,2022-04-05 20:36:36+00:00,Fixes #653 ,[],[],2
537,Updates to PDI Wrapper,https://api.github.com/repos/move-coop/parsons/issues/654,Sophia Alice,654,closed,2022-03-30 23:27:35+00:00,2022-04-14 15:22:06+00:00,2022-04-14 15:22:06+00:00,"Additional endpoint coverage of the PDI API, added automatic session token reauth when token expires, ability to pull more than 2000 values, documentation of endpooints",[],[],0
538,Fix Action Network connector docs,https://api.github.com/repos/move-coop/parsons/issues/653,Shauna Gordon-McKeon,653,open,2022-03-30 21:45:03+00:00,2023-02-21 16:59:03+00:00,,"A few fairly minor issues with the docs:

- [x] The authentication info note at the top is just slightly too vague. I think it would be more helpful to say: ""Only ActionNetwork accounts of the partner tier are able to access their API. You can generate your key from the API & Sync page, located in the Start Organizing menu, under ""Details""."" This is would replace ""in the right column"", the current end to that sentence, to ""under 'Details'"".

- [x] The quickstart section references a non-existent method `get_people_list`, we should switch it to `get_people`.

- [ ] There's a Python code-block towards the end of the document that is not displaying properly. You can search the page for  `code-block:: python` in order to find it or look under the method `create_event` and parameter `location`.
","[Label(name=""documentation""), Label(name=""good first issue""), Label(name=""low priority"")]",[],1
539,Bump paramiko from 2.7.2 to 2.10.1,https://api.github.com/repos/move-coop/parsons/issues/652,,652,closed,2022-03-29 21:56:29+00:00,2022-04-08 21:44:31+00:00,2022-04-08 21:44:23+00:00,"Bumps [paramiko](https://github.com/paramiko/paramiko) from 2.7.2 to 2.10.1.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/paramiko/paramiko/commit/286bd9f0374922341d48923b0c3ef09aab57919f""><code>286bd9f</code></a> Cut 2.10.1</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/4c491e299c9b800358b16fa4886d8d94f45abe2e""><code>4c491e2</code></a> Fix CVE re: PKey.write_private_key chmod race</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/aa3cc6fa3e9f1df72d4ffd2d5fc02ae734a6cba4""><code>aa3cc6f</code></a> Cut 2.10.0</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/e50e19f7d26665fd60f143320cce2e9c03f27c80""><code>e50e19f</code></a> Fix up changelog entry with real links</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/02ad67eaec68bacc18838158b902ccaade8f5dc8""><code>02ad67e</code></a> Helps to actually leverage your mocked system calls</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/29d7bf43f4a8beabcfe14cdc969d6a370e57ecf8""><code>29d7bf4</code></a> Clearly our agent stuff is not fully tested yet...</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/5fcb8da16d4b33fa52880c1c3e848654a698d34d""><code>5fcb8da</code></a> OpenSSH docs state %C should also work in IdentityFile and Match exec</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/1bf3dce7255ff2055dcdbc4d29454fb0184dfaf7""><code>1bf3dce</code></a> Changelog enhancement</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/f6342fc5f00b48e679e7c2c3579b1b27d94ffc1f""><code>f6342fc</code></a> Prettify, add %C as acceptable controlpath token, mock gethostname</li>
<li><a href=""https://github.com/paramiko/paramiko/commit/3f3451fd46353fa173f6c083b1c38438d04a68ea""><code>3f3451f</code></a> Add to changelog</li>
<li>Additional commits viewable in <a href=""https://github.com/paramiko/paramiko/compare/2.7.2...2.10.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=paramiko&package-manager=pip&previous-version=2.7.2&new-version=2.10.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
540,upload_saved_list_rest needs 'bucket' argument,https://api.github.com/repos/move-coop/parsons/issues/651,Elliot L. Richardson,651,open,2022-03-28 17:14:18+00:00,2023-02-21 16:59:54+00:00,,"Hello! I’m attempting to use the `upload_saved_list_rest` function in the Saved List class of the NGPVAN connector, and I’m getting this error: “to_s3_csv() missing 1 required positional argument: ‘bucket’“. It seems like `upload_saved_list_rest` relies on `post_file` which calls `to_s3_csv` but no ‘bucket’ arg is passed. I don't think I know the functions well enough to submit a pull request but I was hoping it could be fixed pretty quickly so if that would help, let me know. ","[Label(name=""documentation""), Label(name=""good first issue""), Label(name=""medium priority"")]",[],3
541,Van connector docs,https://api.github.com/repos/move-coop/parsons/issues/650,Tomiwa Sanusi,650,closed,2022-03-26 16:44:52+00:00,2022-04-01 17:58:59+00:00,2022-04-01 17:58:50+00:00,Fixes #426 ,[],[],6
542,Only unpack dicts in P2A campaigns response if the response has data,https://api.github.com/repos/move-coop/parsons/issues/649,Soren Spicknall,649,closed,2022-03-24 21:00:11+00:00,2022-06-09 16:36:13+00:00,2022-05-02 21:03:22+00:00,"This fixes a bug that caused late-loading failures in `get_campaigns` responses when an account doesn't have any campaigns associated with it. The previous version of this code attempted to expand a column that doesn't exist when there aren't campaigns returned. It does not impact the type of returned object under empty conditions, so is backwards compatible with existing workarounds that people might have privately composed.",[],[],0
543,Sau w,https://api.github.com/repos/move-coop/parsons/issues/648,Saurabh ,648,closed,2022-03-24 20:56:17+00:00,2022-03-25 19:33:07+00:00,2022-03-24 21:04:46+00:00,,[],[],0
544,fixed the issues with tables.rst,https://api.github.com/repos/move-coop/parsons/issues/647,Saurabh ,647,closed,2022-03-24 16:20:23+00:00,2022-03-30 20:02:10+00:00,2022-03-30 19:56:05+00:00,"hey @shaunagm  I gave it a shot.
please let me know if any changes are required,",[],[],10
545,Added Pandas df vs Parsons table,https://api.github.com/repos/move-coop/parsons/issues/646,Tomiwa Sanusi,646,closed,2022-03-24 10:35:04+00:00,2022-03-25 16:00:02+00:00,2022-03-25 16:00:01+00:00,Added the pandas df vs parsons table to docs,[],[],2
546,Documentation of Parsons Table's to/from methods is incorrect,https://api.github.com/repos/move-coop/parsons/issues/645,Shauna Gordon-McKeon,645,closed,2022-03-22 22:05:51+00:00,2022-03-30 21:41:24+00:00,2022-03-30 21:41:24+00:00,"If you compare the methods [in the code](https://github.com/move-coop/parsons/blob/master/parsons/etl/tofrom.py) to [the list in the docs](https://move-coop.github.io/parsons/html/table.html#from-parsons-table) it looks incorrect, specifically the 'from' table:

![Screenshot from 2022-03-22 18-03-08](https://user-images.githubusercontent.com/1179362/159583873-1359d6c5-ac17-429f-8b78-a6cbdc0a02e4.png)

Looking at the [corresponding docs page](https://github.com/move-coop/parsons/blob/master/docs/table.rst) I think the issue is just that some of the links are wrong - like `from_csv` somehow got used in place of `from_redshift` in the redshift row. Should be straightforward to fix, someone just needs to go through this carefully.","[Label(name=""documentation""), Label(name=""good first issue""), Label(name=""high priority"")]",[],5
547,Create Google Admin connector,https://api.github.com/repos/move-coop/parsons/issues/644,Kathy Nguyen,644,closed,2022-03-22 21:37:31+00:00,2022-04-11 20:26:05+00:00,2022-04-11 20:26:05+00:00,,[],[],2
548,Add staging API URLs to docs,https://api.github.com/repos/move-coop/parsons/issues/643,Nikki Everett,643,open,2022-03-22 19:47:45+00:00,2023-02-21 17:01:38+00:00,,"It seems like it'd be helpful for contributors using the sandbox API keys if we added staging API URLs for connectors with staging-specific APIs to the ""[Connector-specific guidance](https://move-coop.github.io/parsons/html/build_a_connector.html#connector-specific-guidance)"" section of the docs. I figure we can make a list and do them all in one sweep, so feel free to comment with more.

* Mobilize: http://staging-api.mobilize.us/v1","[Label(name=""documentation""), Label(name=""medium priority"")]",[],0
549,Switch dependency to newer simple-salesforce version ,https://api.github.com/repos/move-coop/parsons/issues/642,Cormac Martinez del Rio,642,closed,2022-03-22 15:18:26+00:00,2022-06-22 20:34:46+00:00,2022-06-22 20:34:46+00:00,"I've been getting emails that salesforce is retiring old versions of their api. The Parsons simple-salesforce dependency is a super outdated version. It's not clear in their documentation which salesforce api version they use, but to be safe I think we need to update to a newer version. ","[Label(name=""high priority""), Label(name=""connector update"")]",[],4
550,Add filtering to Action Network connector,https://api.github.com/repos/move-coop/parsons/issues/641,Joe Irving,641,closed,2022-03-22 14:08:14+00:00,2022-04-05 20:34:00+00:00,2022-04-05 20:33:39+00:00,"As laid out in the [Searching and filtering (OData)](https://actionnetwork.org/docs/v2/) section of the Action Network documentation, you can filter resource lists by fields like `modified_date`, which is needed for any kind of database sync and avoids unnecessary API calls.

The `filter` argument has been added to `_get_page`, and then `_get_entry_list`, `get_people` and `get_tags`. 

This is my first pull request to parsons, but looking forward to making more! (Also be nice if I have made any silly mistake please) ",[],[],4
551,Debugging + add get_events,https://api.github.com/repos/move-coop/parsons/issues/640,Cormac Martinez del Rio,640,closed,2022-03-18 23:21:30+00:00,2022-03-22 19:30:28+00:00,2022-03-21 14:42:12+00:00,Did some debugging and added get_events!,[],[],0
552,Multiversion docs,https://api.github.com/repos/move-coop/parsons/issues/639,Shauna Gordon-McKeon,639,closed,2022-03-18 18:53:57+00:00,2022-04-01 16:47:57+00:00,2022-04-01 16:47:57+00:00,"Here's a first stab at setting up multiversioning for the docs using https://github.com/Holzhaus/sphinx-multiversion

The conf.py file takes a manually specified set of version tags to generate docs for, plus `stable` and `latest`. These are not ""real"" branches, but instead are built by the `deploy_docs.sh` script. 

Speaking of... deployment is now complicated enough that I've put the steps in a `deploy_docs.sh` script. I couldn't figure out the Makefiles so I left them alone for now. The steps are:

- checkout ""latest"" branch off of main/master
- checkout ""stable"" branch off of the latest tagged version (copied approach from https://github.com/librosa/librosa/pull/1447/files)
- actually build the docs
- create an index page at the top level that redirects to stable (I wish we could just have the stable at the top level, but there's no obvious way to do that)

`_templates/versions.html` specifies what the little version box should actually look like.

Drawbacks to this approach:
- need to update `DOCUMENTED_VERSIONS` in `conf.py` when we want to add or remove a version to document
- nothing at the base level of the docs, everything's under a version name or stable or latest directory. this means breaking all current links to documentation, although I think we could probably do some kind of systemic redirect like we're doing with index.",[],[],1
553,misc connector docs updates,https://api.github.com/repos/move-coop/parsons/issues/638,Shauna Gordon-McKeon,638,closed,2022-03-16 18:20:39+00:00,2022-03-21 16:08:49+00:00,2022-03-21 16:08:48+00:00,"- add ActionKit to list of connectors we have a sandbox with credentials for
- create new section for connectors that have create-your-own sandbox options
- add Airtable, Braintree, Github, Salesforce and Twilio to new section
- update Bluelink intro to be in third person and more in line with other connector intros
- fix typo in airtable intro",[],[],0
554,Update Slack notification code to use up to date Slack SDK,https://api.github.com/repos/move-coop/parsons/issues/637,Nikki Everett,637,open,2022-03-15 17:22:14+00:00,2023-02-21 17:04:51+00:00,,"The [Slack notification code](https://github.com/move-coop/parsons/blob/master/parsons/notifications/slack.py) currently uses a legacy version of the Python Slack SDK, which is causing issues for a few of TMC's internal scripts. This code should probably use the `slack_sdk` module instead of `slackclient` and have its methods in the Slack class updated as needed to accommodate the most up to date version of the SDK.

A full-fledged Slack connector is probably the better long-term solution here, but I think fixing this code for now should be pretty straightforward.

(This is not exactly a connector update, so I just marked it as a bug for now. I'm also happy to take on this fix.)","[Label(name=""bug""), Label(name=""low priority""), Label(name=""connector update"")]",[],0
555,Add live tests to our CI,https://api.github.com/repos/move-coop/parsons/issues/636,Shauna Gordon-McKeon,636,open,2022-03-14 17:25:21+00:00,2024-08-08 19:56:22+00:00,,"We have credentials for sandbox accounts for a small-ish percentage of our connectors - hopefully more soon! Also for several of our connectors we have tests that get skipped because they lack credentials to do live tests.

Ideally we'd have this for all of our connectors, but for now, we should look at the tests that are getting skipped and, for any that we have or can create sandbox accounts for, add the credentials for those sandbox accounts to CI and un-skip the tests.

This issue will be considered done when:

- [ ] we've added credentials and un-skipped the tests for all connectors that have live tests already written and sandbox accounts available
- [ ] we've documented the process of adding sandbox account credentials to CI so that in the future it's easier to add live tests (you'd still have to write the tests, but the process of adding credentials is straightforward)","[Label(name=""medium priority""), Label(name=""testing"")]",[],0
556,Add docs for SMTP and Sendmail connectors,https://api.github.com/repos/move-coop/parsons/issues/635,Shauna Gordon-McKeon,635,closed,2022-03-11 19:58:57+00:00,2022-09-22 16:51:30+00:00,2022-09-22 16:51:30+00:00,"In working on PR #634 I noticed that there aren't any docs for our SMTP and Sendmail notifications connectors, which means they're pretty undiscoverable. We should add docs here: https://github.com/move-coop/parsons/blob/master/docs/notifications.rst

Note that SMTP has a test file but Sendmail doesn't; we may want to also check that both connectors have good test coverage before adding their docs and making them discoverable.","[Label(name=""documentation""), Label(name=""good first issue""), Label(name=""testing"")]",[],0
557,Update init shortcuts,https://api.github.com/repos/move-coop/parsons/issues/634,Shauna Gordon-McKeon,634,closed,2022-03-11 18:31:40+00:00,2022-03-17 20:35:28+00:00,2022-03-17 20:35:28+00:00,"When attempting to do a docs update I ran into an error with the Shopify connector, which wasn't added to the handy `__init__.py` shortcut. In the contrib meeting we talked about updating the tests to always reference the shortcut so we can more easily catch when we've forgotten to add connectors to it. This PR:

- updates all the tests to import directly from Parsons, ie `from parsons import Shopify` instead of `from parsons.shopify.shopify import Shopify`
- adds missing connectors to `__init__.py` - in addition to Shopify we were also missing Redash, Braintree, Bluelink, SMTP and Sendmail

I'm not entirely sure SMTP and Sendmail are actively used/maintained - it looks like Sendmail doesn't have tests and neither have docs. But I added them anyway.",[],[],0
558,No dependencies DO NOT MERGE,https://api.github.com/repos/move-coop/parsons/issues/633,Bella Wang,633,closed,2022-03-09 16:53:24+00:00,2022-03-09 17:28:27+00:00,2022-03-09 17:28:26+00:00,"parsons with fewer dependencies, and not reading in the aws lambda creds",[],[],1
559,Prototype for modular dependencies.,https://api.github.com/repos/move-coop/parsons/issues/632,CM Lubinski,632,closed,2022-03-08 22:32:36+00:00,2022-04-13 14:17:20+00:00,2022-04-13 14:17:20+00:00,"For #517 , I wanted to propose a backwards-compatible solution!

Currently, setup.py declares that Parson requires a ton of pinned
dependencies. This is problematic for incorportating Parsons into a
larger code base for a few reasons:
* Extra dependencies increase the attack surface, which is a bad
trade-off if they aren't ever used
* Extra dependencies increase the build size, which, in turn increases
testing and deployment time
* Pinning to specific versions often causes conflicts (dependency hell)

The docs recommend installing parsons separately from the rest of an
app's dependencies, using the ""--no-deps"" flag, but that doesn't play
well with package locking tools like pipenv and poetry.

This prototype tries to give us a backwards-compatible offramp.
Following the process used in python-slugify 1.x.x, it changes which
dependencies are available at install time, dependending on an
environment variable. So,

```sh
pip install parsons  # retain current behavior
PARSONS_LIMITED_DEPENDENCIES=true pip install parsons  # core dependencies only
PARSONS_LIMITED_DEPENDENCIES=true pip install parsons[ngpvan]  # core + ngpvan(suds)
PARSONS_LIMITED_DEPENDENCIES=true pip install parsons[google,ngpvan]  # core + google + ngpvan
PARSONS_LIMITED_DEPENDENCIES=true pip install parsons[all]  # core + all optional deps
```

In some future, major release we can remove the guard and recommend
`parsons[all]` where appropriate.

This does *not* attempt to find all optional dependencies -- I wanted to
throw the proof of concept up for review first.",[],[],7
560,Add 'getting started with parsons' training guide,https://api.github.com/repos/move-coop/parsons/issues/631,Shauna Gordon-McKeon,631,closed,2022-03-07 18:48:17+00:00,2022-03-15 15:19:50+00:00,2022-03-15 15:19:50+00:00,This PR contains content created for the 'Getting Started With Parsons' Parsons Party written up as a first training guide.,[],[],1
561,Use case and sample scripts,https://api.github.com/repos/move-coop/parsons/issues/630,Melissa Woods,630,closed,2022-03-04 18:24:58+00:00,2022-03-21 15:33:11+00:00,2022-03-21 15:32:12+00:00,This PR contains a use case and corresponding sample script for posting Civis job status alerts to Slack. It also contains a sample script for querying a MYSQL database and saving the results in Google Sheets. The use case for this script is coming soon!,"[Label(name=""sample script"")]",[],10
562,Update Parsons version number for 0.18.1 release,https://api.github.com/repos/move-coop/parsons/issues/629,Soren Spicknall,629,closed,2022-03-03 20:59:45+00:00,2022-06-09 16:36:13+00:00,2022-03-03 21:03:54+00:00,,[],[],0
563,Update reference to license,https://api.github.com/repos/move-coop/parsons/issues/628,Shauna Gordon-McKeon,628,closed,2022-03-03 20:11:52+00:00,2022-03-03 20:56:18+00:00,2022-03-03 20:56:18+00:00,Wording of previous license attribution is slightly confusing.,[],[],0
564,Remove unused file.,https://api.github.com/repos/move-coop/parsons/issues/627,CM Lubinski,627,closed,2022-03-02 20:27:22+00:00,2022-03-03 19:05:21+00:00,2022-03-03 19:05:21+00:00,"This was added in #581, but doesn't appear to be used by it. Probably
not super sensitive, but let's just not track it.",[],[],0
565,Add sandbox instructions,https://api.github.com/repos/move-coop/parsons/issues/626,Shauna Gordon-McKeon,626,closed,2022-03-02 19:44:58+00:00,2022-03-03 19:02:54+00:00,2022-03-03 19:02:54+00:00,This PR contains the content that @neverett and I collectively wrote to help contributors access and use our sandboxes.,[],[],0
566,Change branch name from `master` to `main`,https://api.github.com/repos/move-coop/parsons/issues/625,brittany bennett,625,closed,2022-02-28 17:56:27+00:00,2022-06-22 20:54:34+00:00,2022-06-22 20:54:34+00:00,I am proposing we switch the branch called `master` to be named `main` in keeping with progressive conventions. [This guide may be helpful] (https://github.com/github/renaming). Soren indicated that renaming this branch will necessitate changing a lot of documentation. ,"[Label(name=""housekeeping/meta"")]",[],5
567,Add Slack Class,https://api.github.com/repos/move-coop/parsons/issues/624,brittany bennett,624,open,2022-02-28 15:47:55+00:00,2023-02-21 17:04:37+00:00,,"Data staff at Sunrise are interested in add a new connector class to Parsons to bring in data from Slack's API. We are hoping that this could bring insight into our Slack analytics and therefore the engagement of our base. We are interested in the following data:

- Number of total members in the Slack.
- The ability to track Slack membership over time. 
- Number of messages sent per day.
- Number of messages sent per day per channel. 
","[Label(name=""medium priority""), Label(name=""connector update"")]",[],2
568,Add ActBlue and QuickBase to docs sidebar,https://api.github.com/repos/move-coop/parsons/issues/623,Shauna Gordon-McKeon,623,closed,2022-02-14 20:38:47+00:00,2022-02-14 23:06:07+00:00,2022-02-14 23:06:07+00:00,"Noticed that these didn't actually get linked to sidebar and are currently invisible to users.

Plus I moved shopify to after sftp because that's how the alphabet works.",[],[],0
569,first draft of sample script & use case docs,https://api.github.com/repos/move-coop/parsons/issues/622,Shauna Gordon-McKeon,622,closed,2022-02-09 21:59:39+00:00,2022-02-25 18:23:31+00:00,2022-02-25 18:23:31+00:00,Here's a first stab at the docs for writing up use cases and adding sample scripts. Also a placeholder file to specify where the use cases will live.,"[Label(name=""documentation"")]",[],3
570,npgvan.People json methods do not work as documented,https://api.github.com/repos/move-coop/parsons/issues/621,Jason,621,open,2022-02-09 21:51:29+00:00,2024-09-05 19:30:24+00:00,,"The NGPVAN people API ([documented here](https://docs.everyaction.com/reference/people#common-models)) uses camel case for the search parameters. Keeping with Python style, the Parsons VAN API uses underscore casing ([which I learned today is called snake case](https://en.wikipedia.org/wiki/Snake_case)). The people methods that take parameters, such as `find_person`, do store these arguments in a camel-cased dictionary at [`ngpvan.people.py:L262`](https://github.com/move-coop/parsons/blob/master/parsons/ngpvan/people.py#L262).

However, for the methods that take JSON, such as `find_person_json`, the dictionary passed in is not transformed in any way. So if you pass a dictionary with keys like `firstName` you will be fine. However if you pass keys like `first_name`, it will not work because the API is expecting camel case. [This is made worse because our documentation for these methods explicitly state that you must include the snake-cased versions of the parameters.](https://move-coop.github.io/parsons/html/ngpvan.html#parsons.ngpvan.van.People.find_person_json)

In the process of investigating this, I also found some irregularities with the `email` field in particular. The API expects emails to be wrapped in a data structure. Again, if you use the `find_person` method this works fine. The email argument is wrapped in the necessary structure at [`ngpvan.people.py:L267`](https://github.com/move-coop/parsons/blob/master/parsons/ngpvan/people.py#L267). However, no such wrapping occurs if you pass a plain `email` key in your dictionary for the `find_person_json` method. If you duplicate that structure in your dictionary, then it does work fine when it gets to the VAN API.

I have attached a script I wrote where you can quickly verify these problems, and the results of one time that I ran it. I expect that the same problems could be found with other multi-word arguments to the API, such as `additionalEnvelopeName`, although I haven't tested with any others.

The two solutions I can see are:

1. Change the documentation so passing camel-cased fields is called out explicitly. Also document how more complex data like emails must be passed. Change the `People._valid_search` method so that it fails when a JSON object with snake case keys was passed.
2. Change the `People._people_search` method so that it transforms snake case dictionaries to camel case dictionaries.

Personally I think 2 would be more consistent and has the benefit of not changing the external API.

```python
import os
import requests
from urllib.error import HTTPError
from parsons import VAN

# Load your VAN API key here
VAN_API_KEY = os.getenv('EA_TRAINING_PASSWORD')
# Set to MyCampaign/EveryAction/etc.
VAN_DB = 'EveryAction'

def main():
	van = VAN(api_key=VAN_API_KEY, db=VAN_DB)

	test_person = {
		'first_name': 'Jason',
		'last_name': 'Walker',
		'email': 't@test.com'
	}

	print(""Test Json:"")
	try:
		test_json = van.find_person_json(test_person)
	except requests.HTTPError:
		test_json = ""Not found.""
	print(test_json)

	print(""Test args:"")
	try:
		test_args = van.find_person(
			first_name=test_person['first_name'],
			last_name=test_person['last_name'],
			email=test_person['email']
		)
	except requests.HTTPError:
		test_args = ""Not found.""
	print(test_args)

	print(""Test camel json:"")
	try:
		test_camel_json = van.find_person_json({
			'firstName': test_person['first_name'],
			'lastName': test_person['last_name'],
			'email': test_person['email']
		})
	except requests.HTTPError:
		test_camel_json = ""Not found.""
	print(test_camel_json)

	print(""Test email wrapped, camel json:"")
	try:
		fixed_json = van.find_person_json({
			'firstName': test_person['first_name'],
			'lastName': test_person['last_name'],
			'emails': [{'email': test_person['email']}]
		})
	except requests.HTTPError:
		fixed_json = ""Not found.""
	print(fixed_json)


if __name__ ==  '__main__':
	main()
```

[van_search_email_test.txt](https://github.com/move-coop/parsons/files/8036449/van_search_email_test.txt)","[Label(name=""bug""), Label(name=""documentation""), Label(name=""high priority""), Label(name=""connector update"")]","[NamedUser(login=""rdhyee"")]",6
571,adds Controlshift connector to main init file,https://api.github.com/repos/move-coop/parsons/issues/620,Chris Cuellar,620,closed,2022-02-04 17:16:53+00:00,2022-02-09 16:30:40+00:00,2022-02-09 16:30:40+00:00,Forgot to do this in the earlier Controlshift PR #598 ,"[Label(name=""bug"")]",[],0
572,Updates version number and docs,https://api.github.com/repos/move-coop/parsons/issues/619,Chris Cuellar,619,closed,2022-01-28 23:44:06+00:00,2022-01-31 16:22:34+00:00,2022-01-31 16:22:31+00:00,Updates doc description of the Parsons license. ,[],[],0
573,Create ActBlue Connector.,https://api.github.com/repos/move-coop/parsons/issues/618,Tori Marbois,618,closed,2022-01-28 20:47:40+00:00,2022-02-09 16:31:14+00:00,2022-02-09 16:31:14+00:00,"The ActBlue Parsons connector allows the user to access the ActBlue CSV API. Authentication requires two keys: an `ACTBLUE_CLIENT_UUID` and an `ACTBLUE_CLIENT_SECRET`, which can be passed into the class constructor or set as environment variables. The user then should use the main helper function, `get_contributions()`, to make the request to the API and get the results within the Parsons table format.
This PR includes the code for the connector class, testing suite, documentation, and sample code.

The PR also adds '202' as a success code for returned status codes within the parsons/utilities/api_connector.py file, and fixes a bug preventing the provision of Google credentials via JSON file in the Google Sheets connector.",[],[],0
574,Request: Add option to return results in JSON format,https://api.github.com/repos/move-coop/parsons/issues/617,Flambé,617,closed,2022-01-26 22:12:47+00:00,2023-02-21 17:20:42+00:00,2023-02-21 17:20:42+00:00,"Our team has been exploring using Parsons to build custom source connectors in Airbyte. One major challenge we have faced is that Airbyte's SDK requires that data be returned in raw JSON format.

It would be super helpful if parsons had the ability to return data either as a Table object or as a JSON (eg, as an option when calling a function).","[Label(name=""documentation""), Label(name=""enhancement""), Label(name=""parsons core"")]",[],2
575,Request: Resolve incompatible dependencies with Google Composer (Airflow),https://api.github.com/repos/move-coop/parsons/issues/616,Flambé,616,closed,2022-01-26 22:06:15+00:00,2023-02-21 17:22:35+00:00,2023-02-21 17:22:34+00:00,"Parsons is currently incompatible with deployment in Google Composer, which is necessary to use parsons in Composer Airflow instances.

The specific offenders appear to be two packages:
- pyjwt (parsons requires 2.0.1, while no versions of Composer support pyjwt 2 or higher)
- google-cloud-storage (parsons requires 1.17.0, while Composer requires <2.0.0,>=1.30)

This is the error we get when attempting to install parsons in Composer:
![image](https://user-images.githubusercontent.com/47834372/151253814-041979fc-c21f-4ce4-930f-bd6190141223.png)

It would be amazing if we could get these dependencies resolved so that it would be possible to use parsons in Airflow using Google Composer. That would be a major unlock to enable all kinds of fancy orchestrated workflows.

(This request is similar to other existing issues requesting lighter-weight parsons to resolve similar compatibility issues, e.g. https://github.com/move-coop/parsons/issues/595)","[Label(name=""bug""), Label(name=""parsons core""), Label(name=""dependency update"")]",[],5
576,Add more documentation on virtualenvs and install to the quickstart.,https://api.github.com/repos/move-coop/parsons/issues/615,Jason,615,closed,2022-01-25 21:42:39+00:00,2022-02-02 20:45:35+00:00,2022-02-02 20:45:35+00:00,"@shaunagm and I have been talking about adding more documentation for installing Parsons and making virtual environments, particularly for Windows. This PR adds a ""virtual environments"" section and fleshes out the installation section to the [index page of the documentation](https://move-coop.github.io/parsons/html/index.html).

I have only used Parsons on Windows, so I haven't added virtual environment instructions for Linux/Mac OS. If anyone would like to volunteer, I'd be happy to merge some changes into this branch before we merge the PR. Otherwise we could merge this PR and add that section later.","[Label(name=""documentation"")]",[],1
577,Redshift: only drop dependent views if they exist,https://api.github.com/repos/move-coop/parsons/issues/614,Soren Spicknall,614,closed,2022-01-25 20:51:49+00:00,2022-06-09 16:36:15+00:00,2022-01-27 22:45:45+00:00,"Previously, `drop_dependencies_for_cols` would attempt to run an empty query if no dependent views were found, resulting in an error. This commit fixes that bug.",[],[],0
578,Only deploy Parson docs on branches tagged as releases,https://api.github.com/repos/move-coop/parsons/issues/613,Chris Cuellar,613,closed,2022-01-21 21:49:09+00:00,2022-01-28 23:30:52+00:00,2022-01-28 23:30:50+00:00,"Should also now skip build for the auto-generated gh-pages branch

For more info, see: https://circleci.com/docs/2.0/workflows/#executing-workflows-for-a-git-tag",[],[],0
579,"Parsons docs should target releases, and should only be updated with new releases.",https://api.github.com/repos/move-coop/parsons/issues/612,Chris Cuellar,612,closed,2022-01-21 21:22:14+00:00,2022-03-17 16:56:32+00:00,2022-03-17 16:56:32+00:00,"Currently, docs are updated on pushes to the main branch, which means the docs are slightly ahead of what's available via PyPi. ","[Label(name=""documentation""), Label(name=""enhancement"")]",[],1
580,Updates license to Apache V2 with Author Attribution NOTICE,https://api.github.com/repos/move-coop/parsons/issues/611,Chris Cuellar,611,closed,2022-01-19 18:09:36+00:00,2022-01-20 21:49:17+00:00,2022-01-20 21:49:14+00:00,,[],[],0
581,"Create PDI Functions Relating to Events, Event Invitations, and Contacts",https://api.github.com/repos/move-coop/parsons/issues/610,Cormac Martinez del Rio,610,closed,2022-01-19 16:07:39+00:00,2023-02-21 18:55:39+00:00,2023-02-21 18:55:39+00:00,"The following end points are currently being added as functions to the PDI class
POST /calendars/{id}/events
GET /calendars
POST /eventActivities
GET /contacts
POST /contacts
POST /contacts/{contactId}/emails
POST /contacts/{contactId}/phones
POST /events/{eventid}/invitations
PUT /events/{eventid}/invitations/{id}
PUT /events/{id}","[Label(name=""enhancement""), Label(name=""connector update"")]","[NamedUser(login=""cmdelrio"")]",3
582,Updates Sphinx dependencies,https://api.github.com/repos/move-coop/parsons/issues/609,Chris Cuellar,609,closed,2022-01-14 23:26:39+00:00,2022-01-18 22:03:37+00:00,2022-01-18 22:03:33+00:00,"This also replaces the problematic markdown parser we were using (see #608) with the recommended extension for parsing markdown in the latest version of Sphinx. 

Also experimenting with adding a new CI job to test building our docs on Pull Request submissions, so we can hopefully catch issues with our docs-builder before stuff gets merged into the main branch. 

closes #608 
","[Label(name=""dependency update"")]",[],1
583,Sphinx docs builder no longer working,https://api.github.com/repos/move-coop/parsons/issues/608,Chris Cuellar,608,closed,2022-01-14 23:20:45+00:00,2022-01-18 22:03:33+00:00,2022-01-18 22:03:33+00:00,"See for example the latest attempt at building our docs pages: https://app.circleci.com/pipelines/github/move-coop/parsons/3333/workflows/c0274c59-fc74-4808-9df7-0656794c97ae/jobs/3996

I did some digging into this and it appears the issue has to do with a known and unresolved issue with the `m2r` dependency. See https://github.com/miyakogi/m2r/issues/66 for more info.","[Label(name=""bug"")]",[],0
584,feat(Redshift): Add option to alter table cascade when upserting a table,https://api.github.com/repos/move-coop/parsons/issues/607,Nikki Everett,607,closed,2022-01-14 22:24:35+00:00,2022-01-19 19:04:12+00:00,2022-01-19 16:39:13+00:00,"Similar to #511, except adds option to alter table cascade when upserting. When I tested with `alter_table=True` and `alter_table_cascade=False` when upserting to a test table with a dependent view, the test failed with the expected error `DETAIL:  rule _RETURN on view neverett_dev.upsert_cascade_test_view depends on column ""col1""`, and when I tested with `alter_table=True` and `alter_table_cascade=True`, the dependent view was dropped and the test table was updated with new values as expected.

","[Label(name=""enhancement"")]","[NamedUser(login=""neverett"")]",0
585,Quickbase Materialization Intervention,https://api.github.com/repos/move-coop/parsons/issues/606,Soren Spicknall,606,closed,2022-01-10 16:36:58+00:00,2022-01-11 15:52:46+00:00,2022-01-11 15:52:38+00:00,"In the Quickbase query_records function, materializes the target table between API calls to avoid issues with lazy loading potentially running afoul of Python recursion depth limits.",[],[],0
586,Resume work on Mobile Commons connector,https://api.github.com/repos/move-coop/parsons/issues/605,usha yeruva,605,closed,2021-12-27 00:34:30+00:00,2023-11-21 21:20:22+00:00,2023-11-21 21:20:22+00:00,"Mobile Commons updated their API this year, so I'd love to help to help get this connector off the ground. I haven't done much parsons dev work before, but I'm a user and am fairly familiar with the MC API. ","[Label(name=""enhancement""), Label(name=""high priority""), Label(name=""new connector"")]","[NamedUser(login=""cmdelrio"")]",10
587,Add support for temporary S3 credentials.,https://api.github.com/repos/move-coop/parsons/issues/604,Jason,604,closed,2021-12-21 15:30:29+00:00,2022-01-15 01:01:42+00:00,2022-01-15 01:01:42+00:00,"The S3 API supports creating temporary credentials for one-off operations, such as pushing a file to a particular key in a particular bucket. For example in our use case, the Mapbox API uses this to take new data for website map plugins. When S3 returns a set of temporary credentials it also returns a _session token_ that needs to be included with the standard credentials for them to be accepted.

The Parsons S3 connector uses the Boto library under the hood, which supports such session tokens. All we need to do to add support in Parsons is look for it in the env variables at the right time and pass it along to the Boto object.

Amazon documentation on session tokens & temporary credentials:
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html","[Label(name=""documentation"")]",[],2
588,Don't move columns on Table.fill_column and Table.fillna_column,https://api.github.com/repos/move-coop/parsons/issues/603,Jason,603,closed,2021-12-16 22:04:36+00:00,2021-12-17 21:47:53+00:00,2021-12-17 21:47:53+00:00,"Fix for [issue 537](https://github.com/move-coop/parsons/issues/537).

The existing `Table.fill_column` and `Table.fillna_column` methods create a temp column with the new value, drop the old column, and then rename the existing column. This results in the ""old"" column being moved to the end of the table.

I changed both methods to use the petl `convert` and `update` functions to update the column in-place. This has the added benefit of allowing a calculated value to be passed to `Table.fillna_column`, which previously could only handle a fixed value. This does change the API of the function, but only in the case that someone wanted to store a reference to a function object in a Parsons `Table`.

An alternative, less invasive solution would have been to note the index of the old column before it was dropped, and then move the new column to that index. I believe that is not a trivial operation, so I think this solution will be more performant.",[],[],1
589,Add a Sendgrid Notification connector,https://api.github.com/repos/move-coop/parsons/issues/602,Chris Cuellar,602,open,2021-12-13 16:45:59+00:00,2024-05-23 18:06:43+00:00,,"For sending transactional emails, similar to the Slack and Gmail notification connectors. Docs for setting up - https://docs.sendgrid.com/for-developers/sending-email/api-getting-started ","[Label(name=""low priority""), Label(name=""new connector"")]","[NamedUser(login=""jddelaune"")]",3
590,Update Google Group reference to Slack Group reference in README,https://api.github.com/repos/move-coop/parsons/issues/601,Soren Spicknall,601,closed,2021-12-08 17:43:56+00:00,2021-12-08 22:03:03+00:00,2021-12-08 22:02:58+00:00,"We no longer actively use the Parsons Google Group, and our Slack group is where most of the day-to-day conversation about the tool happens. This updates the README to reflect that reality.",[],[],0
591,Trying to pass non Parsons Table objects to the Redshift copy method should throw an error,https://api.github.com/repos/move-coop/parsons/issues/600,brittany bennett,600,closed,2021-12-03 20:18:02+00:00,2022-03-17 17:13:23+00:00,2022-03-17 17:13:23+00:00,"I accidentally wrote come code to push a CSV to rs.copy() and I got a successful log message. However, the data was not copied bc it was in the wrong format. See this Slack message https://movement-coop.slack.com/archives/CPNCN48DA/p1638462326000400

Passing non Parsons Table objects to methods that require a Parsons Table should throw an error. 


Thank you! ✨","[Label(name=""bug""), Label(name=""parsons core""), Label(name=""connector update"")]",[],0
592,Zoom Pagination Bug,https://api.github.com/repos/move-coop/parsons/issues/599,,599,open,2021-11-30 22:30:28+00:00,2022-03-17 16:58:49+00:00,,"According to [this documentation](https://marketplace.zoom.us/docs/guides/stay-up-to-date/announcements#batch-list-api-changes), Zoom's API used to use `page_number` for pagination, but has switched to `next_page_token` for batch list endpoints.

The Zoom class's underlying `_get_request()` method seems to still be based on `page_number`.

I haven't done the research to determine if we'd need to completely change that method over to looking for `next_page_token`, or if it needs to have both options in case some methods are hitting endpoints that still use `page_number`.

In any case, the following is how I hacked it to work on a script dealing only with Webinar Registrations:
```
zoom.refresh_header_token()
params = {
    ""page_size"": 300
}
r = zoom.client.get_request(f'webinars/{webinar_id}/registrants', params=params)
zoom.client.data_key = 'registrants'
data = zoom.client.data_parse(r)

if r['next_page_token']:
    while r['next_page_token']:
        zoom.refresh_header_token()
        params['next_page_token'] = r['next_page_token']
        r = zoom.client.get_request(f'webinars/{webinar_id}/registrants', params=params)
        data.extend(zoom.client.data_parse(r))
        
registrants = Table(data)
```","[Label(name=""bug""), Label(name=""medium priority""), Label(name=""connector update"")]",[],1
593,Adds a Controlshift connector,https://api.github.com/repos/move-coop/parsons/issues/598,Chris Cuellar,598,closed,2021-11-19 17:34:47+00:00,2021-12-01 17:01:24+00:00,2021-12-01 17:01:19+00:00,"Just a reference note, OAuth2 implementation was based on this: https://requests-oauthlib.readthedocs.io/en/latest/oauth2_workflow.html#backend-application-flow 

There are [variations in how services can implement OAuth](https://requests-oauthlib.readthedocs.io/en/latest/examples/examples.html), so we may have to modify in the future for other use cases.

I only implemented the LIST `petitions` endpoint here, but added placeholders for more methods to be built as needed. 

(Updated flake8 and fixed a lot of new linting errors, but most pertinent files for this PR are the [class](https://github.com/move-coop/parsons/blob/0f202f5e5ec3d3b340a407569c3228724836734c/parsons/controlshift/controlshift.py), [tests](https://github.com/move-coop/parsons/blob/0f202f5e5ec3d3b340a407569c3228724836734c/test/test_controlshift/test_controlshift.py), [docs](https://github.com/move-coop/parsons/blob/0f202f5e5ec3d3b340a407569c3228724836734c/docs/controlshift.rst), and [oauth api connector](https://github.com/move-coop/parsons/blob/0f202f5e5ec3d3b340a407569c3228724836734c/parsons/utilities/oauth_api_connector.py))

cc @neverett for viz",[],[],1
594,Zoom Webinar Registrant Endpoint Fix,https://api.github.com/repos/move-coop/parsons/issues/597,,597,closed,2021-11-17 22:39:21+00:00,2021-11-23 13:18:16+00:00,2021-11-23 13:18:11+00:00,"For whatever reason, the path for the end point for retrieving webinar registrants _doesn't_ include `report`, even though the path for retrieving past webinar participants _does_, and [the documentation](https://marketplace.zoom.us/docs/api-reference/zoom-api/webinars/listwebinarparticipants) doesn't suggest any such difference between the two.","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""low priority"")]",[],0
595,F string in logging fix,https://api.github.com/repos/move-coop/parsons/issues/596,Melissa Woods,596,closed,2021-11-16 20:05:22+00:00,2021-11-17 22:01:35+00:00,2021-11-17 22:01:32+00:00,"A very small quick fix! I've been running create_signup a LOT lately and the logging just says ""signups INFO Signup {r} created"" because it's missing an f.",[],[],0
596,Make connector dependencies optional for more lightweight builds,https://api.github.com/repos/move-coop/parsons/issues/595,Chris Cuellar,595,closed,2021-11-05 17:14:59+00:00,2023-02-21 17:32:12+00:00,2023-02-21 17:32:12+00:00,Revisit https://github.com/move-coop/parsons/pull/186 for more background on previous work done to move to a more lightweight Parsons package that could run on AWS lambda.,"[Label(name=""enhancement""), Label(name=""parsons core"")]",[],1
597,Dependencies,https://api.github.com/repos/move-coop/parsons/issues/594,Chris Cuellar,594,closed,2021-11-05 15:46:52+00:00,2022-03-17 16:58:20+00:00,2022-03-17 16:58:20+00:00,,[],[],0
598,Redshift improvements and bug fixes epic,https://api.github.com/repos/move-coop/parsons/issues/593,Nikki Everett,593,closed,2021-11-03 20:06:57+00:00,2022-03-17 17:16:01+00:00,2022-03-17 17:16:01+00:00,"Gathering Redshift issues here so we can group / tackle them together when it makes sense to.

## Improvements

- [ ] [Merge Redshift and PostgresCore](https://github.com/move-coop/parsons/issues/190)
- [ ] [alter_varchar_column_widths() - compare 2 redshift tables](https://github.com/move-coop/parsons/issues/128)
- [ ] [Create Upsert Method that is Redshift to Redshift](https://github.com/move-coop/parsons/issues/253)
- [ ] [Redshift: Add DIST/SORT keys to upsert.](https://github.com/move-coop/parsons/issues/194)
- [ ] [Convenience Function to get Distkey & Sortkey](https://github.com/move-coop/parsons/issues/364)
- [ ] [Deduplicate records in staging on upsert](https://github.com/move-coop/parsons/issues/355)
- [ ] [Redshift get_table_definition method based on non-standard Redshift views](https://github.com/move-coop/parsons/issues/301)

## Bug fixes

- [ ] [Redshift populate_table_from_query silent error](https://github.com/move-coop/parsons/issues/343)
- [ ] [Redshift: Add tag to temp file used to copy into Redshift](https://github.com/move-coop/parsons/issues/371)
- [ ] [Redshift Connector not working in Python 3.6](https://github.com/move-coop/parsons/issues/518)
- [ ] [Fixing Specifycols in Redshift Copy S3](https://github.com/move-coop/parsons/issues/457)
- [ ] [rs.upsert alter_table not working](https://github.com/move-coop/parsons/issues/493)
- [ ] [RS Upsert Vacuum Error](https://github.com/move-coop/parsons/issues/401)
- [ ] [populate_table_from_query - doesn't cascade](https://github.com/move-coop/parsons/issues/311)
",[],[],1
599,reverts etl column mapper to run exact match,https://api.github.com/repos/move-coop/parsons/issues/592,Chris Cuellar,592,closed,2021-11-03 19:56:53+00:00,2021-11-03 19:59:46+00:00,2021-11-03 19:59:41+00:00,"Reverts ETL column mapper fuzzy matching default (introduced [here](https://github.com/move-coop/parsons/commit/245fb922feb5975e6987fcd2ca45ffae34afaccb)). This change introduced new bugs in other connectors using this method (for example, the hustle connector).",[],[],1
600,Validation epic,https://api.github.com/repos/move-coop/parsons/issues/591,Nikki Everett,591,closed,2021-11-03 18:55:34+00:00,2022-03-17 17:17:45+00:00,2022-03-17 17:17:45+00:00,"This is a parent issue to track validation-related issues:
- [ ] [Phone number validation](https://github.com/move-coop/parsons/issues/28)
- [ ] [Email validation](https://github.com/move-coop/parsons/issues/91)
- [ ] [Address validation](https://github.com/move-coop/parsons/issues/483)",[],[],1
601,Add polling to VAN Bulk Import method,https://api.github.com/repos/move-coop/parsons/issues/590,Chris Cuellar,590,open,2021-11-01 20:32:27+00:00,2023-02-21 17:38:18+00:00,,"To improve the user-friendliness of this method. For more context, see https://github.com/move-coop/parsons/pull/388#discussion_r488194754","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update"")]",[],1
602,Reverts changes to ddl methods,https://api.github.com/repos/move-coop/parsons/issues/589,Chris Cuellar,589,closed,2021-10-29 17:05:23+00:00,2022-01-25 20:49:37+00:00,2021-10-29 18:37:09+00:00,Reverts changes from previous merge of #336 ,[],[],0
603,Redshift: updated PR for #373 - Add dist/sort keys to upsert,https://api.github.com/repos/move-coop/parsons/issues/588,Chris Cuellar,588,closed,2021-10-29 00:55:41+00:00,2021-11-05 17:09:39+00:00,2021-11-05 17:09:36+00:00,This cherry-picks original changes from #373 on top of the latest so it's easier to read. Also implements the original requested changes for handling primary key and sort key lists.,[],[],0
604,Salesforce query method returns garble ,https://api.github.com/repos/move-coop/parsons/issues/587,Cormac Martinez del Rio,587,open,2021-10-26 18:55:48+00:00,2023-02-21 17:41:54+00:00,,"The Salesforce query method returns garble, possible because Salesforce changed the response object to be a ordered dictionary. The following lines should resolve the issue:
```table = Table([{k:v for k,v in i.items() if k != 'attributes'} for i in {{{response_object}}}['records']])```

Unit test should also be updated. ","[Label(name=""bug""), Label(name=""medium priority""), Label(name=""connector update""), Label(name=""testing""), Label(name=""requires access"")]","[NamedUser(login=""cmdelrio"")]",1
605,Quickbase connector - some initial endpoint implementations,https://api.github.com/repos/move-coop/parsons/issues/586,Soren Spicknall,586,closed,2021-10-20 15:57:56+00:00,2021-11-15 22:37:32+00:00,2021-11-15 22:29:49+00:00,Not ready to merge yet - a connector that implements some basic Quickbase methods. This will need changes to testing and probably expansion of some of its functions before merging. Opening a PR right now to get easy access in one place to current results from CI routines.,[],[],1
606,Create graphql method for Shopify connector,https://api.github.com/repos/move-coop/parsons/issues/585,Kathy Nguyen,585,closed,2021-10-18 19:58:26+00:00,2021-10-27 19:50:22+00:00,2021-10-27 19:50:22+00:00,This PR allows custom query calls to be made to Shopify's GraphQL Admin API: https://shopify.dev/api/admin-graphql,[],[],0
607,WealthEngine Connector,https://api.github.com/repos/move-coop/parsons/issues/584,Melissa Woods,584,open,2021-10-18 14:15:06+00:00,2023-02-21 17:44:07+00:00,,"Information on the tool itself [is here](https://www.wealthengine.com/).

Additional context: WealthEngine has an integration with EveryAction which can pull data like giving capacity, net worth, etc. into EA contact records for donors, but groups find they get a very low match rate because you can't specify which address or other information the integration should use in order to match EA donors to their WealthEngine records. Other limitations are lack of customization of how the data appears in EA and the fact that records pushed from EA to WE do not show up in the WE interface, so you can't interact with your EA list within WE.

[API documentation is here](https://apidocs.wealthengine.com/#section/Getting-Started) and [WealthEngine's Developers site is here](https://www.wealthengine.com/developer-api/) which contains FAQs, API examples, etc. Developers can access the sandbox and test code without having a WealthEngine contract.

The most helpful methods to have in Parsons would be a find_person method & find_people method using the [Screen Profiles endpoint](https://apidocs.wealthengine.com/#tag/Screen-Profiles). `find_person` and `find_person_json` in the [VAN class in Parsons](https://move-coop.github.io/parsons/html/ngpvan.html#people) could be helpful models here.

Ideally the response for both would return as much data as possible (note that in the Response section items like identity, demographics, etc are objects that contain many fields) - `get_person` in the [VAN class](https://move-coop.github.io/parsons/html/ngpvan.html#parsons.ngpvan.van.People.get_person) which offers the argument `expand_fields` could be a helpful model for how to handle the data.","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""new connector"")]",[],0
608,Reimplement New/Mode Connector Without SDK Import,https://api.github.com/repos/move-coop/parsons/issues/583,Soren Spicknall,583,open,2021-10-15 16:11:28+00:00,2023-02-21 17:44:48+00:00,,"Our New/Mode connector in Parsons is based on New/Mode's own published SDK, which was convenient to use at the time the connector was first published since it meant the connector required little new code to be written. However, relying directly on New/Mode's SDK means that the Parsons community can't directly change behaviors around API calls and responses, since it's limited by the functionality of the SDK. This issue seeks to reimplement the New/Mode connector directly on top of the New/Mode API, rather than passing through the vendor's published SDK.","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update""), Label(name=""requires access"")]",[],1
609,"Proposal: standard table patterns like EventTable, Donation w/ standard cols",https://api.github.com/repos/move-coop/parsons/issues/582,Schuyler Duveen,582,open,2021-10-15 15:30:40+00:00,2023-02-21 17:45:39+00:00,,"I think we should have some 'standard table objects' -- e.g. EventTable could be a subclass of Table and no matter what it's exported from it will have a specific list of (super-inclusive) columns.  Then methods for APIs that deal with events could have a method that imports/exports EventTable where the field-mapping from the standard is pre-known.  That way, when passing records between systems, people don't have to rename fields manually for the matching.  Same thing with ?people, ?donations, ?...

For a past project, this was the 'superset' of event columns that worked across many different systems
https://github.com/MoveOnOrg/eventroller/blob/main/event_store/models.py#L100-L201
When systems import they can ignore columns that they don't have fields for. Similarly, they can leave them blank on-export, but having a common set of columns will make interchange easier.","[Label(name=""enhancement""), Label(name=""parsons core""), Label(name=""medium priority""), Label(name=""needs discussion"")]",[],0
610,VAN Changed Entity Method,https://api.github.com/repos/move-coop/parsons/issues/581,,581,closed,2021-10-14 19:48:52+00:00,2021-10-18 15:01:25+00:00,2021-10-18 15:01:20+00:00,This adds the `VAN.get_changed_entities()` method that allows you to download contact history and survey question responses from the past 90 days. It is useful for organizations that may not have their own VAN replication service.,"[Label(name=""enhancement"")]",[],0
611,Add profile and region args to AWS cloud storage utility,https://api.github.com/repos/move-coop/parsons/issues/580,Jennifer McKaig,580,closed,2021-10-06 15:59:32+00:00,2021-10-06 16:14:58+00:00,2021-10-06 16:12:39+00:00,It would be helpful to have the option to further define the credentials passed to aws including profile name and region. ,[],[],1
612,Add cleanup_s3_file option to VAN methods,https://api.github.com/repos/move-coop/parsons/issues/579,Melissa Woods,579,open,2021-10-01 14:28:06+00:00,2023-02-21 17:47:19+00:00,,"A handful of VAN methods require cloud file storage to post files to, but these methods do not clean up the files after they have been loaded. This issue proposes incorporating that functionality similar to how we have `cleanup_s3_file` as an option in Redshift.copy.

The VAN methods that I think could use this parameter are:

`post_bulk_import`
`bulk_apply_activist_codes`
`upload_scores`
`upload_saved_list_rest`","[Label(name=""documentation""), Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],2
613,fixed four broken links,https://api.github.com/repos/move-coop/parsons/issues/578,Vignesh Ponraj,578,closed,2021-09-10 18:05:55+00:00,2021-09-21 14:02:24+00:00,2021-09-21 14:02:24+00:00,closes #549  ,[],[],0
614,Add Shopify and update ActionKit documentation,https://api.github.com/repos/move-coop/parsons/issues/577,Kathy Nguyen,577,closed,2021-09-09 21:51:15+00:00,2021-09-21 14:00:40+00:00,2021-09-21 14:00:40+00:00,Documentation for new Shopify connector and ActionKit functions that have been added in the past.,[],[],0
615,add a ControlShift connector,https://api.github.com/repos/move-coop/parsons/issues/576,Chris Cuellar,576,closed,2021-09-09 18:21:58+00:00,2022-01-14 23:18:51+00:00,2022-01-14 23:18:51+00:00,"For info on the platform, see: https://www.controlshiftlabs.com/

API documentation here: https://developers.controlshiftlabs.com/#json-api-endpoints","[Label(name=""new connector"")]","[NamedUser(login=""ChrisC"")]",3
616,Issues installing Parsons on Python v3.9.x,https://api.github.com/repos/move-coop/parsons/issues/575,Chris Cuellar,575,closed,2021-09-09 17:54:30+00:00,2023-02-21 17:49:43+00:00,2023-02-21 17:49:42+00:00,A lot of folks are reporting errors running Parsons upon first install with Python `v3.9.x` with errors related to loading to `psycopg`. Seems to be related to the Postgres `.DLL` files not being added to `$PATH`... some folks have found a workaround by installing `postgres` via homebrew. It would be great to improve the initial install step so that there isn't a need for extra steps if postgres is not installed locally.  ,"[Label(name=""bug""), Label(name=""documentation""), Label(name=""dependency update"")]",[],2
617,Ganymede post survey,https://api.github.com/repos/move-coop/parsons/issues/574,Aili Huber,574,closed,2021-09-08 18:20:08+00:00,2021-09-21 14:03:56+00:00,2021-09-21 14:03:56+00:00,Added some api requests related to mailers,[],[],0
618,Update ActionNetwork.add_person() to include mobile_phone parameter,https://api.github.com/repos/move-coop/parsons/issues/573,Cormac Martinez del Rio,573,closed,2021-08-30 23:35:53+00:00,2021-09-21 14:19:44+00:00,2021-09-21 14:19:44+00:00,"Action Network has updated their data structure and API to include a mobile phone field (previously you would have had to store this in a custom field) and also to allow a person to be created without an email address as long as they have a mobile phone ([documentation](https://move-coop.github.io/parsons/html/_modules/parsons/action_network/action_network.html#ActionNetwork.add_person)).

Thus PR updates the `ActionNetwork.add_person()` method so that users can add mobile phones to the mobile_phone field in Action Network with or without an email address. The mobile_phone parameter follows very similar logic to the email_address parameter in terms of the data types it accepts. The method raises an error if the user doesn't include an email address or phone. 

I tested this script with several different parameter combinations and everything worked as intended. It also passes the built in parsons tests and linter. ",[],[],0
619,Add a Strive class,https://api.github.com/repos/move-coop/parsons/issues/572,brittany bennett,572,open,2021-08-26 18:39:19+00:00,2023-02-22 17:08:15+00:00,,"Hello nerds,

Another vague and open ended issue, so my apologies, but it could be useful to have a wrapper for the Strive API in Parsons. ","[Label(name=""enhancement""), Label(name=""high priority""), Label(name=""new connector"")]","[NamedUser(login=""thebbennett"")]",3
620,Extend Alchemer connector to include more methods ,https://api.github.com/repos/move-coop/parsons/issues/571,brittany bennett,571,open,2021-08-26 18:18:26+00:00,2023-02-21 17:51:59+00:00,,"Hello Nerds!

I work with Alchemer data frequently at Sunrise and would be interested in someone building out more Alchemer methods to more efficiently working with raw Alchemer data. I'll probably end up building this out myself, but I'm imagining a `survey_pivot` method to take the long responses table and pivot it into a table with one row per response. ","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update""), Label(name=""needs more info"")]",[],1
621,add documentation for existing Alchemer methods ,https://api.github.com/repos/move-coop/parsons/issues/570,Chris Cuellar,570,open,2021-08-26 18:14:40+00:00,2023-02-21 17:51:36+00:00,,"The connector currently has two methods that need to be documented:
`get_surveys` and `get_survey_responses`","[Label(name=""documentation""), Label(name=""good first issue""), Label(name=""medium priority"")]",[],0
622,Update ActionNetwork add_person to reflect mobile phone field updates,https://api.github.com/repos/move-coop/parsons/issues/569,Cormac Martinez del Rio,569,closed,2021-08-25 22:57:55+00:00,2021-11-03 16:49:59+00:00,2021-11-03 16:49:59+00:00,"Currently, the ActionNetwork.add_person method requires email address and there is no argument for mobile phone. However, Action Network has updated their data structure and API to include a mobile phone field and also to allow a person to be created without an email address as long as they have a mobile phone ([documentation](https://move-coop.github.io/parsons/html/_modules/parsons/action_network/action_network.html#ActionNetwork.add_person)).",[],"[NamedUser(login=""cmdelrio"")]",2
623,Improved censoring of debug log regardless of supplied tokens,https://api.github.com/repos/move-coop/parsons/issues/568,,568,closed,2021-08-24 17:44:00+00:00,2021-09-01 20:30:06+00:00,2021-09-01 20:30:00+00:00,"Found that this logging was taking place even when the user doesn't supply override credentials. This way, credentials can never be logged.

@cmdelrio @elyse-weiss for visibility",[],"[NamedUser(login=""ydamit"")]",0
624,Fix tiny typo in README.md,https://api.github.com/repos/move-coop/parsons/issues/567,Laura James,567,closed,2021-08-24 16:41:11+00:00,2021-08-26 18:21:52+00:00,2021-08-26 18:21:52+00:00,Fixed tiny typo :) ,[],[],0
625,Typo fix,https://api.github.com/repos/move-coop/parsons/issues/566,Melissa Woods,566,closed,2021-08-23 17:25:09+00:00,2021-08-26 18:21:41+00:00,2021-08-26 18:21:35+00:00,The smallest of PRs - adding an F to a log message that was intended to be an f string.,"[Label(name=""low priority"")]",[],0
626,Update paramiko version and functions to ActionKit,https://api.github.com/repos/move-coop/parsons/issues/565,Kathy Nguyen,565,closed,2021-08-19 16:59:06+00:00,2021-08-26 18:21:23+00:00,2021-08-26 18:21:23+00:00,"1. Update paramiko version to fix this bug: https://github.com/paramiko/paramiko/issues/1738
2. Add update_order and update_transaction functions to ActionKit",[],[],0
627,van connector failing,https://api.github.com/repos/move-coop/parsons/issues/564,Jennifer McKaig,564,closed,2021-08-18 14:42:32+00:00,2022-03-17 21:06:01+00:00,2022-03-17 21:06:01+00:00,"The van connector is failing. It is giving me a Type Error- it is trying to concatenate a string to a tuple. Screenshot attached.
<img width=""1126"" alt=""Screen Shot 2021-08-18 at 10 11 15 AM"" src=""https://user-images.githubusercontent.com/67813032/129918732-5d45dfa9-9e01-4b56-a667-4372adf32618.png"">
","[Label(name=""bug"")]",[],1
628,Redshift: Add support for a compound table sortkey,https://api.github.com/repos/move-coop/parsons/issues/563,Jason,563,closed,2021-08-13 14:21:06+00:00,2021-10-27 20:48:59+00:00,2021-10-27 20:48:59+00:00,https://dev.classmethod.jp/articles/redshift-sortkey-usecase-en/,[],[],0
629,updating airtable-python-wrapper to v0.13.0,https://api.github.com/repos/move-coop/parsons/issues/562,Elliot L. Richardson,562,closed,2021-08-12 19:51:32+00:00,2021-09-01 20:29:29+00:00,2021-09-01 20:29:29+00:00,"This version of the airtable-python-wrapper includes a [patch](https://github.com/gtalarico/pyairtable/pull/61/commits/880844bd20acc2957e5e93855af3a9a8be3f2e32) for the `_batch_request` and `batch_insert` functions that allow the creation of new multi- and single-select values when inserting records via the Parsons connector.

I was able to run an Airtable sync [script](https://github.com/sunrisedatadept/engineering-general/blob/feature/airtable_sync/airtable/redshift_to_airtable_sync/civis_to_airtable.py) on Civis by including the following `sed` statements that accomplish the exact same thing as the above patch so I am hoping that means there aren't any conflicts with other parts of Parsons!

```
sed -i -e 's/def _batch_request(self, func, iterable):/def _batch_request(self, func, iterable, **kwargs):/' /usr/local/lib/python3.7/site-packages/airtable/airtable.py
sed -i -e 's/responses.append(func(item))/responses.append(func(item, **kwargs))/' /usr/local/lib/python3.7/site-packages/airtable/airtable.py
sed -i -e 's/return self._batch_request(self.insert, records)/return self._batch_request(self.insert, records, typecast=typecast)/' /usr/local/lib/python3.7/site-packages/airtable/airtable.py
```",[],[],0
630,Create a Bluelink connector,https://api.github.com/repos/move-coop/parsons/issues/561,Phil Dreizen,561,closed,2021-08-12 19:10:16+00:00,2021-09-01 20:29:47+00:00,2021-09-01 20:29:46+00:00,"This PR is for creating a Bluelink connector that works with Bluelink's Webhook API.

[Bluelink](https://bluelink.org/) is an online tool for connecting the various [digital software tools](https://https://bluelink.org/product/#integrations) used by campaigns and movement groups in the political and non-profit space to allow you to seamlessly and easily sync data between them.

This integration currently supports sending your structured person data and related tags to Bluelink via the
[Bluelink Webhook API](https://bluelinkdata.github.io/docs/BluelinkApiGuide#webhook), after which you can use our UI to send to any of our [supported tools](https://https://bluelink.org/product/#integrations).

The bluelink package provides a Person class that follows the Bluelink data model. A Person object can be populated with data via the constructor and then upserted via the connector.

The connector allows for bulk upserting of people data via a Parson's Table, using a passed in function that can transform a Table row into a Person object.

Examples are provided in the documentation, and also the unit tests.
",[],[],0
631,Upgrade Slack notifications to use latest Python SDK,https://api.github.com/repos/move-coop/parsons/issues/560,Chris Cuellar,560,closed,2021-08-12 15:45:14+00:00,2022-03-17 17:17:37+00:00,2022-03-17 17:17:37+00:00,"Parsons is using a legacy slack client: https://github.com/slackapi/python-slack-sdk#slackclient-is-in-maintenance-mode 

See Slack's docs for migration to latest SDK: https://slack.dev/python-slack-sdk/v3-migration/index.html","[Label(name=""enhancement"")]",[],1
632,update version to 0.17.2,https://api.github.com/repos/move-coop/parsons/issues/559,Chris Cuellar,559,closed,2021-08-11 17:05:52+00:00,2021-08-11 19:14:55+00:00,2021-08-11 19:14:55+00:00,,[],[],0
633,misc Hustle Connector fixes,https://api.github.com/repos/move-coop/parsons/issues/558,Chris Cuellar,558,closed,2021-08-05 19:15:05+00:00,2021-08-10 18:01:00+00:00,2021-08-10 18:01:00+00:00,"- only adds page limit parameters to GET requests
- fixes issues with adding Tag ID's when creating leads",[],[],0
634,Censoring hotfix,https://api.github.com/repos/move-coop/parsons/issues/557,,557,closed,2021-08-05 12:49:36+00:00,2021-08-05 13:02:16+00:00,2021-08-05 13:02:16+00:00,Only censor when items are not `None`,[],"[NamedUser(login=""ydamit"")]",0
635,More censoring,https://api.github.com/repos/move-coop/parsons/issues/556,,556,closed,2021-08-05 00:11:27+00:00,2021-08-05 00:25:10+00:00,2021-08-05 00:25:10+00:00,,[],"[NamedUser(login=""ydamit"")]",0
636,Censor sensitive data in debug log of copy statement,https://api.github.com/repos/move-coop/parsons/issues/555,,555,closed,2021-08-04 21:04:09+00:00,2021-08-05 00:07:55+00:00,2021-08-04 21:09:15+00:00,,[],"[NamedUser(login=""ydamit"")]",0
637,Add more helper methods and utilities to Parsons (and create standardized process for it),https://api.github.com/repos/move-coop/parsons/issues/554,Schuyler Duveen,554,closed,2021-07-30 19:43:13+00:00,2024-07-22 01:36:06+00:00,2024-07-22 01:36:05+00:00,"_Note: Sky's original title was ""Easy 'glue' connector script/format""._

There are a lot of ""copy from X over to Y"" uses in Parsons, and at MoveOn we found it useful to write a little 'generic' script that can take basically some JSON data and a link to credentials and shovel bits from one place to another place(s).

Here is our internal script:
https://gist.github.com/schuyler1d/6a5200214d43196e1d48cc9db136e720

It covers only the things that we, ourselves, needed as sources/destinations, but it would be great if we started building something like this in Parsons.  It could also pave the way for things like common From-A-to-B recipes, where A and B have specific schema mappings that we could save the same ""created_at"" => ""date_created"" kind of boring-work","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""parsons core""), Label(name=""medium priority""), Label(name=""sample script""), Label(name=""needs discussion"")]",[],2
638,Add get_survey_question and update_survey_question to ActionKit,https://api.github.com/repos/move-coop/parsons/issues/553,Kathy Nguyen,553,closed,2021-07-30 18:47:25+00:00,2021-08-05 19:25:44+00:00,2021-08-05 19:25:44+00:00,,[],[],0
639,Added updated NGPVAN API documentation URLs to repo docs,https://api.github.com/repos/move-coop/parsons/issues/552,Don Smith,552,closed,2021-07-30 16:45:33+00:00,2021-08-05 19:34:42+00:00,2021-08-05 19:34:42+00:00,"Updated API URLs in repo docs using https://docs.ngpvan.com/reference/overview. 

I'm a newbie, so please let me know if the URLs I've added are incorrect or you need additional changes. Committed to getting this right. Thanks for the opportunity!",[],[],0
640,"tests finish with ""Exception ignored"" message",https://api.github.com/repos/move-coop/parsons/issues/551,Schuyler Duveen,551,open,2021-07-29 15:40:25+00:00,2023-02-21 17:59:53+00:00,,"When running the tests with Python 3.7.3 and master (9bf7ac6c79194491fc628e09ddfac9c74c9351cc) branch, I get the following error when I run `pytest --rootdir ./  test/` from root:

```
Exception ignored in: <function TempDirectory.__del__ at 0x7f2f0268a048>
Traceback (most recent call last):
  File ""/home/sky/sb/moveon/parsons/parsons/utilities/files.py"", line 352, in __del__
  File ""/home/sky/sb/moveon/parsons/parsons/utilities/files.py"", line 369, in remove
  File ""/home/sky/sb/moveon/parsons/ve37/lib/python3.7/shutil.py"", line 506, in rmtree
AttributeError: 'NoneType' object has no attribute 'path'
Exception ignored in: <function TempDirectory.__del__ at 0x7f2f0268a048>
Traceback (most recent call last):
  File ""/home/sky/sb/moveon/parsons/parsons/utilities/files.py"", line 352, in __del__
  File ""/home/sky/sb/moveon/parsons/parsons/utilities/files.py"", line 369, in remove
  File ""/home/sky/sb/moveon/parsons/ve37/lib/python3.7/shutil.py"", line 506, in rmtree
AttributeError: 'NoneType' object has no attribute 'path'
```

This is non-fatal, but it seems like something that should be fixed.","[Label(name=""bug""), Label(name=""low priority""), Label(name=""testing"")]",[],1
641,URGENT: fix van connector because of ngpvan api change,https://api.github.com/repos/move-coop/parsons/issues/550,Bella Wang,550,closed,2021-07-28 12:57:32+00:00,2021-07-28 16:18:39+00:00,2021-07-28 16:18:39+00:00,"The NGPVAN API just had a [change to get saved lists pagination](https://docs.ngpvan.com/changelog) which causes the `get_saved_lists()` function to keep paginating forever. This is a quick fix that should correct the issue, although I will admit I had to solve it pretty quickly this morning and there might be a more elegant way to do it/I haven't checked whether any other paginations are weird.",[],"[NamedUser(login=""ydamit"")]",0
642,VAN: Fix links to NGPVAN API documentation,https://api.github.com/repos/move-coop/parsons/issues/549,Melissa Woods,549,closed,2021-07-27 17:06:58+00:00,2021-09-21 14:02:24+00:00,2021-09-21 14:02:24+00:00,"Links that navigate to NGP VAN's API documentation are broken and lead to a 404 Page Not Found. Looks like they made updates recently (instead of developers.ngpvan.com it's docs.ngpvan.com)

The new API docs are [here](https://docs.ngpvan.com/reference/overview).

I noticed this with links in the Overview section as well as the update_person_json, upsert_person_json, and apply_response methods, but haven't checked further than that!","[Label(name=""bug""), Label(name=""documentation""), Label(name=""good first issue""), Label(name=""housekeeping/meta"")]","[NamedUser(login=""vignesh-ponraj"")]",12
643,Switch default handling of py floats as RS floats,https://api.github.com/repos/move-coop/parsons/issues/548,,548,closed,2021-07-22 14:45:44+00:00,2021-08-03 13:36:26+00:00,2021-08-03 00:00:02+00:00,Definitely want a second pair of eyes to see if there are any terrible data implications to switching from `DECIMAL` as the default data type to `FLOAT`.,[],"[NamedUser(login=""ydamit"")]",1
644,Add GCS as option for Cloud Storage Utility.,https://api.github.com/repos/move-coop/parsons/issues/547,,547,closed,2021-07-21 21:36:37+00:00,2021-07-28 16:59:30+00:00,2021-07-28 16:59:30+00:00,"This PR adds Google Cloud Storage as an option for the Cloud Storage Utility. Previously, you were only able to use S3 at the cloud storage service. This utility is leveraged by the VAN methods that require an asynchronous load from a publicly accessible source on the internet. 

These methods include `VAN.upload_saved_list()`, `VAN.upload_scores` and `VAN.apply_activist_codes`. Additionally, the documentation was cleaned up to reflect this new option.",[],[],0
645,Redshift Create Table Float Bug,https://api.github.com/repos/move-coop/parsons/issues/546,,546,closed,2021-07-21 15:48:51+00:00,2021-08-03 00:00:02+00:00,2021-08-03 00:00:02+00:00,"*Steps to Reproduce:*
```
float_tbl = Table([
    {""col1"": 7.239829837, ""col2"": 7},
    {""col1"": 23.2391287, ""col2"": 23},
    {""col1"": 2309.47, ""col2"": None}
]).to_redshift('schema.table_name')

rs.query(""SELECT * FROM schema.table_name"")
```
This will show that the floats have been converted to ints, such that the first two rows appear to have identical values for `col1` and `col2`. Querying DDL, we find that `col1` is actually `DECIMAL(18, 0)`.

*Possible Source:*
I believe the problem might be that the constant passed to [the Redshift create_sql() method](https://github.com/move-coop/parsons/blob/914f41752c5e487cfe7d1574f912bf773794912b/parsons/databases/redshift/constants.py) for python `float` types is `decimal`. However, without specified precision or scale, the defaults of 18 and 0 are used.

*Workaround until Fixed:*
Supplying `columntypes` arg when loading to Redsihft","[Label(name=""bug""), Label(name=""medium priority"")]","[NamedUser(login=""ydamit"")]",2
646,DRAFT: Only add limits to GET requests in Hustle connector,https://api.github.com/repos/move-coop/parsons/issues/545,Soren Spicknall,545,closed,2021-07-20 15:56:02+00:00,2021-11-15 22:37:28+00:00,2021-07-28 16:31:57+00:00,"No need to review yet, just checking on testing, etc.",[],[],0
647,Insufficient permissions to create new single-select value (airtable-python-wrapper version),https://api.github.com/repos/move-coop/parsons/issues/544,Elliot L. Richardson,544,closed,2021-07-15 22:13:18+00:00,2021-11-04 14:16:48+00:00,2021-11-04 14:16:48+00:00,"When using the airtable connector to bring updates from Redshift into an existing airtable, I've gotten an error saying I have insufficient permissions to create a new value in the single-select or multi-select columns. The airtable-python-wrapper [required by parsons](https://github.com/move-coop/parsons/blob/master/requirements.txt) is an older version (0.11.3.post1) and there was a [patch](https://github.com/gtalarico/airtable-python-wrapper/pull/61/commits/880844bd20acc2957e5e93855af3a9a8be3f2e32) for this issue in later versions of the wrapper. it looks like versions 0.13.0 onwards might not throw the same errors so I thought I'd point it out in case it might be helpful!",[],[],5
648,Updating Parsons Code for Hustle Lead Load,https://api.github.com/repos/move-coop/parsons/issues/543,,543,closed,2021-07-15 19:18:43+00:00,2021-08-10 18:01:00+00:00,2021-08-10 18:01:00+00:00,"The Hustle Load Leads script is coming across errors in trying to run it.

The original script here: https://platform.civisanalytics.com/spa/#/templates/scripts/39378
Member script I'm trying to run: https://platform.civisanalytics.com/spa/#/scripts/custom/114549763

The error that we are facing is a http API error:
07/12/2021 12:35:12 PM
raise HTTPError(http_error_msg, response=self)
07/12/2021 12:35:12 PM
**requests.exceptions.HTTPError: 422 Client Error: Unprocessable Entity for url: https://api.hustle.com/v1/groups/4w8DZjBTYV/leads?limit=1000**

After meeting with Parsons WG, it was decided that this is thinking it's a pull rather than a post request. The script is trying to make a post request but the API URL is written as a pull. No need to limit a post to 1000.

Here is the original Parsons code for this script: https://move-coop.github.io/parsons/html/_modules/parsons/hustle/hustle.html#Hustle.create_leads

Canales: https://github.com/move-coop/canales/blob/bf56f8e6a314c840dcb4ff74f6affe21f8ec0bf4/vendors/hustle/push/new_leads.py

As well as some hustle documentation: https://api.hustle.com/docs/#operation/createLeadInGroup
",[],"[NamedUser(login=""elyse-weiss"")]",0
649,add live tests for Airtable connector,https://api.github.com/repos/move-coop/parsons/issues/542,Chris Cuellar,542,closed,2021-06-30 20:45:51+00:00,2022-03-17 19:27:12+00:00,2022-03-17 19:27:12+00:00,,"[Label(name=""enhancement""), Label(name=""testing"")]",[],1
650,Updates urllib3 security vulnerability,https://api.github.com/repos/move-coop/parsons/issues/541,Chris Cuellar,541,closed,2021-06-22 21:24:32+00:00,2021-06-23 15:43:11+00:00,2021-06-23 15:42:51+00:00,,[],[],0
651,add typecast param to insert_record,https://api.github.com/repos/move-coop/parsons/issues/540,brittany bennett,540,closed,2021-06-19 15:51:21+00:00,2021-07-28 16:30:56+00:00,2021-07-28 16:30:56+00:00,"Issue: [#539: Optionally Allow Modification of Allowable Multiple Choice Options in Insert Functions](https://github.com/move-coop/parsons/issues/539)

This fix adds a typecast param to the Airtable insert_records function",[],[],4
652,Optionally Allow Modification of Allowable Multiple Choice Options in Insert Functions,https://api.github.com/repos/move-coop/parsons/issues/539,Soren Spicknall,539,closed,2021-06-17 21:06:26+00:00,2021-07-29 18:59:23+00:00,2021-07-29 18:59:23+00:00,"The Airtable connector currently errors out if you try to insert values into a multiple select field that aren't allowed in the field definition. For some use cases, that would be desirable - you wouldn't want to accidentally insert a bunch of garbage into a tightly controlled field. But there should be an optional way to update the field with newly found values through the API, and that's not currently implemented in Parsons.

At first I thought that we'd have to fully implement new update functions, since the Python package we base the Parsons connector on doesn't implement anything from the Airtable metadata API, but I think there's a way to do this with a much smaller change. Looking at an example column, I found this documentation for changes to the allowable destination values:

>When creating or updating records, if  a  choice string does not exactly match an existing option, the request will fail with an INVALID_MULTIPLE_CHOICE_OPTIONS error unless the typecast parameter is enabled. If typecast is enabled, a new choice will be created if one does not exactly match.

Looking at the current code in Parsons, it seems like the typecast parameter just isn't implemented for the insert and batch_insert functions. This should be a pretty easy fix.",[],[],1
653,Hustle API Updates,https://api.github.com/repos/move-coop/parsons/issues/538,elyse-weiss,538,closed,2021-06-14 20:44:33+00:00,2024-07-22 01:36:30+00:00,2024-07-22 01:36:30+00:00,"Hustle is making a number of updates that require some small changes to Parsons code:

```
Dear Developers! 

Hustle’s API is scheduled to be publicly available Thursday July 1. 

To prepare for this release the following changes were made:

The current `createLead` endpoint was relabelled to `createLeadInGroup`. This endpoint will remain at the same path with the same parameters. 

A new endpoint, `createGroupMembership`, was introduced to add leads to hustle groups

A new endpoint labelled `createLead`, with a different path and set of parameters, was introduced to add leads to hustle organizations


On July 1, lower rate limits will be introduced for some endpoints including `createLead`, `createLeadInGroup`, and `createAgent`.  

When changes are introduced more detail will continue to be available at https://api.hustle.com/docs/

Let us know if you have any questions!
```","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""low priority""), Label(name=""connector update"")]","[NamedUser(login=""thebbennett"")]",1
654,table.fillna_column relocates column,https://api.github.com/repos/move-coop/parsons/issues/537,Cormac Martinez del Rio,537,closed,2021-06-11 14:54:59+00:00,2023-02-21 18:00:38+00:00,2023-02-21 18:00:37+00:00,"Hi!

I was conflicted about whether or not this issue was serious enough to report, but I noticed that the Table.fillna_column method not only fills empty values in a column, but it also reposition the column to the last position. For me, one line of code fixes the issue, but maybe it would cause issues in a more complicated script where column position is important. 
<img width=""1349"" alt=""Screen Shot 2021-06-10 at 5 27 52 PM"" src=""https://user-images.githubusercontent.com/66973815/121706142-f5a5f300-ca9a-11eb-9445-53328ab64a55.png"">

Thanks!
Cormac","[Label(name=""bug""), Label(name=""parsons core"")]",[],1
655,ActionKit: Modify action sources,https://api.github.com/repos/move-coop/parsons/issues/536,Melissa Woods,536,open,2021-06-10 20:42:01+00:00,2023-02-21 18:01:46+00:00,,"This issue is for creating methods to add, remove, or update action sources. From my understanding these can only be modified via API, so it would be very useful to do this with Parsons!

[Example from ActionKit here](https://roboticdogs.actionkit.com/docs/manual/api/rest/examples/actionsource.html)

Ideally we would have a method for updating a specific source when provided with a single action ID and a method that loops through a list of several IDs, which then requires a method to get this list of IDs based on desired parameters.

From my understanding that can be done in two ways: by [running a report](https://roboticdogs.actionkit.com/docs/manual/api/rest/reports.html#running-reports-synchronously) which is more flexible but limited to 1000 records and may have some lag (reports run against the client reporting database as opposed to the main database), or by filtering with the rest/v1/action endpoint which is more limited compared to the filtering capabilities of reports but does not have the 1000 record limit and does not have the danger of data lag.","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update"")]","[NamedUser(login=""mkwoods927"")]",0
656,Update Version Number for 0.17.1 Release,https://api.github.com/repos/move-coop/parsons/issues/535,Soren Spicknall,535,closed,2021-06-09 14:17:12+00:00,2021-11-15 22:37:32+00:00,2021-06-09 14:24:26+00:00,,[],[],0
657,Update Parsons Version Number for 0.17.1 Release,https://api.github.com/repos/move-coop/parsons/issues/534,Soren Spicknall,534,closed,2021-06-09 14:14:16+00:00,2021-11-15 22:37:29+00:00,2021-06-09 14:14:33+00:00,,[],[],1
658,verify_row_count in db_sync causing issues,https://api.github.com/repos/move-coop/parsons/issues/533,elyse-weiss,533,open,2021-06-09 12:52:26+00:00,2023-02-21 18:02:25+00:00,,"I am using Parson's DBSync function. In this case, I needed to create a new table in the destination database. The script failed on the `verify_row_count()` section. When I set `verify_row_count=False`, the script succeeded. It seems like perhaps the `verify_row_count()` bit is running before the table has been fully copied over? 

Error log below:
![image](https://user-images.githubusercontent.com/29580051/121357644-f48c8e80-c8ff-11eb-8a73-6f33ef838d21.png)
","[Label(name=""bug""), Label(name=""parsons core""), Label(name=""medium priority"")]",[],0
659,Add __init__ to generic database methods,https://api.github.com/repos/move-coop/parsons/issues/532,Soren Spicknall,532,closed,2021-06-08 19:41:56+00:00,2021-11-15 22:37:33+00:00,2021-06-09 14:12:13+00:00,"Our generic database imports, added a few months ago, are causing issues when interacting with the flow from the package-level __init__ file, through the Redshift connector, and to its dependencies. This resolves the issue by instituting an __init__ file for the generic database methods.",[],[],0
660,Add Pandas df vs Parsons table to documentation,https://api.github.com/repos/move-coop/parsons/issues/531,brittany bennett,531,closed,2021-06-08 16:12:40+00:00,2022-03-25 16:10:22+00:00,2022-03-25 16:01:21+00:00,"Stem from this Slack post: I may be missing this in the documentation, but would someone be willing to talk to me about the inspiration and reasoning behind the Parsons Table, why someone like me may want to move away from something like Pandas?

Soren's response: The main reason we use petl as the basis for the Parsons table instead of Pandas is because Pandas manages memory in a fundamentally different way, requiring more of a table to be loaded into memory in order to work on it. petl, on the other hand, streams data from a table whenever it's needed and only loads it in full during major transformations. That's the lazy loading Elyse mentioned above. This means that you can do a lot of rapid-fire ETL tasks without needing a huge amount of computing power, which helps save organizations a bunch of money on compute resources and do more work on large datasets using their own computers instead of having to spin up a bunch of EC2 instances on AWS or similar.


Elyse's response: @brittany can you put in a Parsons issue so we can add this to documentation? probably something we should add!
","[Label(name=""documentation""), Label(name=""enhancement""), Label(name=""good first issue"")]",[],4
661,Integration Request: Reach app,https://api.github.com/repos/move-coop/parsons/issues/530,Matt Stevans,530,open,2021-06-04 20:19:53+00:00,2024-10-03 13:47:35+00:00,,"Would be great to have integration with the app Reach: https://www.reach.vote/

Sadly there's no open API, but they offer webhooks for organizations at their Movement level.","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""new connector""), Label(name=""requires access"")]",[],2
662,Update Parsons version number to match release,https://api.github.com/repos/move-coop/parsons/issues/529,Soren Spicknall,529,closed,2021-06-04 15:45:36+00:00,2021-11-15 22:37:33+00:00,2021-06-04 16:02:58+00:00,"Just a small fix in `setup.py`, updating the version number to match yesterday's new Parsons release.",[],[],0
663,Updates targetsmart sftp config,https://api.github.com/repos/move-coop/parsons/issues/528,Chris Cuellar,528,closed,2021-05-27 19:14:18+00:00,2021-06-01 17:28:56+00:00,2021-06-01 17:28:56+00:00,"Targetsmart is phasing out their old SFTP site by June 18, 2021. This updates the config for the new SFTP host.",[],[],2
664,Update Targetsmart Automation STFP Host & Port,https://api.github.com/repos/move-coop/parsons/issues/527,Chris Cuellar,527,closed,2021-05-24 15:59:03+00:00,2021-06-09 17:09:22+00:00,2021-06-09 17:09:22+00:00,"[Settings](
https://github.com/move-coop/parsons/blob/master/parsons/targetsmart/targetsmart_automation.py#L12) should be updated to:
Host: sftp://transfer.targetsmart.com
Port: 22
",[],"[NamedUser(login=""ChrisC"")]",1
665,Facebook API v10.0 and other version support,https://api.github.com/repos/move-coop/parsons/issues/526,Jeff Rose,526,open,2021-05-19 16:14:58+00:00,2023-02-21 18:04:22+00:00,,"Parsons currently uses `facebook-business` 6.0, which sends requests via api version 6.0 by default.

New Facebook Business Apps are set to use v10.0 of the API and cannot be downgraded. Using Parsons to make a call to a new Facebook Business App (or any app using v10.0) yields the following:

facebook_business.exceptions.FacebookRequestError: 

```
  Message: Call was not successful
  Method:  POST
  Path:    https://graph.facebook.com/v6.0/<account_id>/customaudiences
  Params:  {'name': <name>, 'subtype': 'CUSTOM', 'description': None, 'customer_file_source': 'PARTNER_PROVIDED_ONLY'}

  Status:  400
  Response:
    {
      ""error"": {
        ""message"": ""(#2635) You are calling a deprecated version of the Ads API. Please update to the latest version: v10.0."",
        ""type"": ""OAuthException"",
        ""code"": 2635,
        ""fbtrace_id"": <id>
      }
    }
```

Expected behavior is for v10.0 to be supported. Ideally, [with multiple valid API versions currently valid and in use by Facebook](https://developers.facebook.com/docs/graph-api/changelog/versions), the user should be able to specify version of the API to use directly in Parsons. ","[Label(name=""bug""), Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],0
666,Update NGPVAN.apply_canvass_result to include Phone,https://api.github.com/repos/move-coop/parsons/issues/525,elyse-weiss,525,closed,2021-05-19 14:31:52+00:00,2023-12-19 22:05:06+00:00,2023-12-19 22:05:06+00:00,"Parsons link: https://move-coop.github.io/parsons/html/ngpvan.html?highlight=canvass#parsons.ngpvan.van.People.apply_canvass_result

NGPVAN documentation: https://docs.ngpvan.com/reference/people?emci=3c487bf6-03b7-eb11-a7ad-0050f271b5d8&emdi=f24ba893-acb8-eb11-a7ad-501ac57ba3ed&ceid=6426064#peoplevanidcanvassresponses

NGPVAN sent an email that they will start requiring phone for applying canvass results with that canvass type. ","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""high priority""), Label(name=""connector update"")]","[NamedUser(login=""coastlines"")]",5
667,Open States API v3,https://api.github.com/repos/move-coop/parsons/issues/524,Audrey Sonntag,524,open,2021-05-19 14:00:08+00:00,2023-02-21 18:06:17+00:00,,"Those of us working with Everytown are interested in an[ Open States](https://docs.openstates.org/en/latest/api/v3/) connector for their new v3 API. As this doesn't appear to be in the issue tracker and we need the functionality, we will be creating a connector class based on the documentation.","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""new connector"")]",[],1
668,feat(Table): Add use_petl method,https://api.github.com/repos/move-coop/parsons/issues/523,Daniel,523,closed,2021-05-19 06:24:51+00:00,2021-06-07 15:52:49+00:00,2021-06-07 15:41:17+00:00,Add a method that exposes the petl functions to `parsons.Table`.,[],[],4
669,Zoom pagination no longer working,https://api.github.com/repos/move-coop/parsons/issues/522,Jason,522,closed,2021-05-11 14:52:36+00:00,2022-03-17 17:24:25+00:00,2022-03-17 17:24:25+00:00,"Previously the Zoom API paginated request responses by returning `page_count` and `page_number` values in a query response. You could use this to query for the next page, and so on, until you have retrieved all of the data from that endpoint. This is [documented on their docs here](https://marketplace.zoom.us/docs/api-reference/pagination). 

On that page, they note that this method is being deprecated, and they are instead returning a `next_page_token` string with requests that can be used to query for the next page directly. The Parsons' Zoom connector has code in `Zoom._get_request` to automatically iterate through the old pagination scheme, but only requests the first page if a `next_page_token` is returned instead. The documentation above and for individual endpoints lists the old method as 'deprecated,' which implies it is still available, but we have noticed that in some endpoints this is no longer true.  At least for the following endpoints, we're finding that we're not getting back the `page_count`  and `page_number` values in the response, but there is more data returned if you increase the page size:

- /groups/{id}/members
- /meetings/{id}/registrants
- /report/meetings/{id}/participants
- /webinars/{id}/registrants
- /report/webinars/{id}/participants

For now, as a workaround, we are sending `page_size: 300` with all of the above requests, and it's working as all but one of our meetings have fewer than 300 registrants/attendees. Still, we should add additional pagination code to `Zoom._get_request` to detect `next_page_token` in the response and use it to request the next page.","[Label(name=""connector update"")]",[],1
670,Group ID Assignment,https://api.github.com/repos/move-coop/parsons/issues/521,elyse-weiss,521,closed,2021-05-10 16:04:42+00:00,2021-05-10 17:03:37+00:00,2021-05-10 17:03:32+00:00,Hustle tech team caught this!,[],[],0
671,Add Action Network events endpoints,https://api.github.com/repos/move-coop/parsons/issues/520,Elijah Snow-Rackley,520,closed,2021-05-05 18:37:29+00:00,2023-11-14 18:03:37+00:00,2023-11-14 18:03:37+00:00,"Currently Parsons doesn't include the events endpoints for Action Network. ~This prevents us from specifying page size and page number and makes pulling down events complex and difficult to parse.~

By adding these, we could use a function similar to `get_people` to pull down events specifying the page number and page size as args. Right now we can accomplish that by invoking `_get_page`, but not as its own function. ","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update"")]","[NamedUser(login=""schlich"")]",12
672,Page Iterator in utilities,https://api.github.com/repos/move-coop/parsons/issues/519,elyse-weiss,519,closed,2021-05-05 17:12:11+00:00,2023-02-21 18:40:59+00:00,2023-02-21 18:40:59+00:00,"Given the number of endpoints that include page number, it may be helpful to add a Page Iterator to utilities.

Here's a helpful Slack thread for context: https://tmc-parsons.slack.com/archives/C018N5ZCB99/p1620164384066600","[Label(name=""enhancement""), Label(name=""parsons core"")]",[],1
673,Redshift Connector not working in Python 3.6,https://api.github.com/repos/move-coop/parsons/issues/518,Kathy Nguyen,518,closed,2021-05-03 15:28:38+00:00,2022-08-24 13:31:13+00:00,2022-08-24 13:31:13+00:00,"When trying to use the Redshift connector in Python 3.6, this line:
`import parsons.databases.redshift.constants as consts`
in the `parsons/databases/redshift/re_create_table.py` file causes the following error:
`AttributeError: module 'parsons.databases' has no attribute 'redshift'` and the connector fails to work correctly.

The issue does not occur in Python 3.7 or Python 3.8.","[Label(name=""parsons core""), Label(name=""needs more info"")]",[],4
674,Improve parsons experience when limiting needed dependencies,https://api.github.com/repos/move-coop/parsons/issues/517,Schuyler Duveen,517,closed,2021-04-22 19:00:52+00:00,2023-02-21 18:43:44+00:00,2023-02-21 18:43:43+00:00,"Currently there is a somewhat elaborate way to use the `parsons` python library without installing all the dependencies.
https://move-coop.github.io/parsons/html/index.html#minimizing-resource-utilization

As more people use Parsons in deployment contexts where packaging all possible dependencies increases, this approach is more awkward for a larger part of the community.

This original workaround came about after one of my (first) PRs (https://github.com/move-coop/parsons/pull/147) was merged at the same time as an additional connector was added from a second PR and the second one was broken by the first's new requirement for a different pattern in the `parsons/__init__.py` file.

This issue is meant to discuss possible ways forward to improve the library ergonomics of installing fewer dependencies.

1. I think it is still possible to do the approach of #147 as a first step -- with more planning and testing this time.
2. If we did that, then there's a further step that might make it even easier to support this use-case -- instead of having an environment variable, etc, we would create a second package (`parsons-core` maybe) that used the `extras_require=` feature in pip's run of setup.py files for different components and their dependencies.

I'm interested in whether maintainers would support or allow this work to move forward and/or if there are other ideas or preferences pro/con from the community.","[Label(name=""enhancement"")]",[],7
675,S3 to GCS functionality,https://api.github.com/repos/move-coop/parsons/issues/516,elyse-weiss,516,open,2021-04-19 18:09:36+00:00,2023-02-21 18:47:01+00:00,,"I have a use case where I would like to move files directly between S3 and GCS. @tonywhittaker offered the following resources that could be helpful:

https://cloud.google.com/storage/docs/interoperability#using_the_gsutil_command_line
https://github.com/GoogleCloudPlatform/gsutil/

I would love it if someone in the community could take a stab at adding this to Parsons!","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update""), Label(name=""sample script"")]",[],2
676,Add ActionKit mailer methods,https://api.github.com/repos/move-coop/parsons/issues/515,Kathy Nguyen,515,closed,2021-04-15 14:41:02+00:00,2021-04-26 15:48:16+00:00,2021-04-26 15:48:16+00:00,Add some methods from ActionKit's Mailer API,[],[],0
677,Google Sheets: Add optional folder_id to create_spreadsheet,https://api.github.com/repos/move-coop/parsons/issues/514,Melissa Woods,514,closed,2021-04-08 18:59:03+00:00,2021-04-09 20:45:24+00:00,2021-04-09 20:45:15+00:00,"Added optional parameter of folder_id to the create_spreadsheet() method in the Google Sheets class.

Addresses issue #509 ",[],[],1
678,File URL returned by cloud_storage.post_file with s3 parameter not publicly accessible,https://api.github.com/repos/move-coop/parsons/issues/513,Will Raphaelson,513,closed,2021-04-08 18:34:09+00:00,2023-02-21 18:48:06+00:00,2023-02-21 18:48:05+00:00,"I'm finding that the url returned by cloud_storage.post_file with s3 parameter not publicly accessible. The url comes back (for me) in the form https://saved-list-load.s3.amazonaws.com/daae0432-9897-11eb-bf3e-acde48001122.csv.zip?AWSAccessKeyId=***&Signature=***%3D&Expires=1617909954'. 

The url preceding the query params is hittable from code and browser as expected. The url with query params gives ""The authorization mechanism you have provided is not supported. Please use AWS4-HMAC-SHA256"" in browser and from code. ","[Label(name=""bug""), Label(name=""connector update""), Label(name=""needs more info"")]",[],5
679,Fix constant references in Redshift table creation,https://api.github.com/repos/move-coop/parsons/issues/512,Soren Spicknall,512,closed,2021-03-31 19:02:21+00:00,2021-11-15 22:37:34+00:00,2021-03-31 19:06:02+00:00,"Within rs_create_table.py, two lines were previously referencing constants defined in constants.py improperly, causing errors. This change fixes those references.",[],"[NamedUser(login=""ChrisC"")]",0
680,feat(Redshift): Add option to alter table cascade when copying a table,https://api.github.com/repos/move-coop/parsons/issues/511,Daniel,511,closed,2021-03-28 22:44:16+00:00,2021-04-14 14:34:48+00:00,2021-04-14 14:34:48+00:00,This addresses #440.,[],[],0
681,feat(Box): Add method to download file from Box ,https://api.github.com/repos/move-coop/parsons/issues/510,Daniel,510,closed,2021-03-27 01:50:05+00:00,2021-04-13 05:06:31+00:00,2021-03-31 17:28:11+00:00,"Add a method to download a file in Box to a file locally, instead of downloading it to a parson table.",[],[],1
682,Update to Google Sheets create_spreadsheet(),https://api.github.com/repos/move-coop/parsons/issues/509,Melissa Woods,509,closed,2021-03-26 21:02:55+00:00,2021-06-10 20:31:31+00:00,2021-06-10 20:31:31+00:00,"We recently updated the requirements to a newer version of gspread which allows for passing a folder ID when creating a new Google Sheet. The function below updates the existing create_spreadsheet to incorporate this parameter. I've tested this code, it just needs unit tests to get over the finish line!

Note for documentation: if you create a spreadsheet in a shared folder it will be shared with people who are shared on the folder.

def create_spreadsheet(self, title, editor_email=None, folder_id=None):
  """"""
  Create a Google spreadsheet from a Parsons table. Optionally shares the new doc with
  the given email address. Optionally creates Google spreadsheet in specified folder.
  `Args:`
      title: str
          The human-readable title of the new spreadsheet
      editor_email: str (optional)
          Email address which should be given permissions on this spreadsheet
      folder_id: str (optional)
          ID of the folder where the Google Sheet should be created (tip: get this from the folder URL)
  `Returns:`
      str
          The spreadsheet ID
  """"""

  spreadsheet = self.gspread_client.create(title=title, folder_id=folder_id)

  if editor_email:
      self.gspread_client.insert_permission(
          spreadsheet.id,
          editor_email,
          perm_type='user',
          role='writer',
      )

  logger.info(f'Created spreadsheet {spreadsheet.id}')
  return spreadsheet.id","[Label(name=""enhancement""), Label(name=""low priority"")]",[],1
683,feat(Database): Add support for parsing booleans,https://api.github.com/repos/move-coop/parsons/issues/508,Daniel,508,closed,2021-03-26 00:47:39+00:00,2021-12-16 23:45:20+00:00,2021-12-16 23:45:20+00:00,"Add the ability to parse (sql) booleans. By default the option is turned off, to ensure current workflows don't unintentionally get altered. 

To turn on boolean parsing set
```python
DatabaseCreateStatement.DO_PARSE_BOOLS = True
```",[],[],1
684,Temporarily roll back JSON loading changes,https://api.github.com/repos/move-coop/parsons/issues/507,Soren Spicknall,507,closed,2021-03-23 16:15:13+00:00,2021-11-15 22:37:35+00:00,2021-03-23 16:39:15+00:00,"We merged #488 yesterday with the intent of reducing memory use when loading multi-line JSON files, but something isn't being handled as expected and is causing errors in certain TMC jobs. It seems we have an edge case not caught by testing, so this PR will roll back the changes of #488 in order to stabilize things while we investigate the issue.",[],[],0
685,Bump gspread version,https://api.github.com/repos/move-coop/parsons/issues/506,Soren Spicknall,506,closed,2021-03-22 15:25:38+00:00,2021-11-15 22:37:39+00:00,2021-03-22 15:44:19+00:00,"Newer versions of gspread include some new functionality, and are backwards compatible with everything currently written in Parsons utilizing the package. Bumping the version to enable use of these newer features.",[],[],0
686,Bump jinja2 from 2.10.1 to 2.11.3,https://api.github.com/repos/move-coop/parsons/issues/505,,505,closed,2021-03-20 01:37:56+00:00,2021-03-22 15:53:21+00:00,2021-03-22 15:53:13+00:00,"Bumps [jinja2](https://github.com/pallets/jinja) from 2.10.1 to 2.11.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/pallets/jinja/releases"">jinja2's releases</a>.</em></p>
<blockquote>
<h2>2.11.3</h2>
<p>This contains a fix for a speed issue with the <code>urlize</code> filter. <code>urlize</code> is likely to be called on untrusted user input. For certain inputs some of the regular expressions used to parse the text could take a very long time due to backtracking. As part of the fix, the email matching became slightly stricter. The various speedups apply to <code>urlize</code> in general, not just the specific input cases.</p>
<ul>
<li>PyPI: <a href=""https://pypi.org/project/Jinja2/2.11.3/"">https://pypi.org/project/Jinja2/2.11.3/</a></li>
<li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3</a></li>
</ul>
<h2>2.11.2</h2>
<ul>
<li>Changelog: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2</a></li>
</ul>
<h2>2.11.1</h2>
<p>This fixes an issue in async environment when indexing the result of an attribute lookup, like <code>{{ data.items[1:] }}</code>.</p>
<ul>
<li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1</a></li>
</ul>
<h2>2.11.0</h2>
<ul>
<li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0</a></li>
<li>Blog: <a href=""https://palletsprojects.com/blog/jinja-2-11-0-released/"">https://palletsprojects.com/blog/jinja-2-11-0-released/</a></li>
<li>Twitter: <a href=""https://twitter.com/PalletsTeam/status/1221883554537230336"">https://twitter.com/PalletsTeam/status/1221883554537230336</a></li>
</ul>
<p>This is the last version to support Python 2.7 and 3.5. The next version will be Jinja 3.0 and will support Python 3.6 and newer.</p>
<h2>2.10.3</h2>
<ul>
<li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3</a></li>
</ul>
<h2>2.10.2</h2>
<ul>
<li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>
<blockquote>
<h2>Version 2.11.3</h2>
<p>Released 2021-01-31</p>
<ul>
<li>Improve the speed of the <code>urlize</code> filter by reducing regex
backtracking. Email matching requires a word character at the start
of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>
</ul>
<h2>Version 2.11.2</h2>
<p>Released 2020-04-13</p>
<ul>
<li>Fix a bug that caused callable objects with <code>__getattr__</code>, like
:class:<code>~unittest.mock.Mock</code> to be treated as a
:func:<code>contextfunction</code>. :issue:<code>1145</code></li>
<li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods
by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>
<li>Fix a hang when displaying tracebacks on Python 32-bit.
:issue:<code>1162</code></li>
<li>Showing an undefined error for an object that raises
<code>AttributeError</code> on access doesn't cause a recursion error.
:issue:<code>1177</code></li>
<li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which
removed the dependency on setuptools and pkg_resources, and added
limited support for namespace packages. The changes caused issues
when using Pytest. Due to the difficulty in supporting Python 2 and
:pep:<code>451</code> simultaneously, the changes are reverted until 3.0.
:pr:<code>1182</code></li>
<li>Fix line numbers in error messages when newlines are stripped.
:pr:<code>1178</code></li>
<li>The special <code>namespace()</code> assignment object in templates works in
async environments. :issue:<code>1180</code></li>
<li>Fix whitespace being removed before tags in the middle of lines when
<code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>
<li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate
intermediate strings during rendering. This prevents early
evaluation which could change the value of an expression.
:issue:<code>1186</code></li>
</ul>
<h2>Version 2.11.1</h2>
<p>Released 2020-01-30</p>
<ul>
<li>Fix a bug that prevented looking up a key after an attribute
(<code>{{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>
<li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>
<li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>
<li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>
<li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>
<li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>
<li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>
<li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>
<li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>
<li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>
<li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
687,New Upload Saved List Method for VAN,https://api.github.com/repos/move-coop/parsons/issues/504,Will Raphaelson,504,closed,2021-03-16 22:38:31+00:00,2021-04-19 16:10:34+00:00,2021-04-19 16:10:34+00:00,"**Summary**
The SOAP endpoint used in [parsons.VAN.upload_saved_list()](https://github.com/move-coop/parsons/blob/a49dba54053b35bddf4f3d9f7881f5ece8f82cf4/parsons/ngpvan/saved_lists.py#L70) is using a deprecated endpoint. The new v4 endpoint has additional required arguments, only works with uploads based on VAN ID, and supports multiple column uploads. This PR Introduces:
- A new upload_saved_list_rest() method that supports VAN ID uploads with more than one column. 
- Adds a deprecation warning on the old method.

**Deprecation Plan**
- Create a Github issue for the future. When NGPVAN allows external ids, route all calls through the correct endpoint, flag upload_saved_list as deprecated with deprecation warning.
- After deprecation period - remove the function entirely or just continue to have it route to the correct endpoint, with validation on the new required arguments of the rest endpoint (callback url, for example)

**Other Potential Features Discovered During Development**
- Support the other listener types on all jobs that hit the /fileLoadingJobs endpoint? https://developers.ngpvan.com/van-api#file-loading-jobs
- Support other file upload methods than S3?
- Check if files are zipped prior to uploading to cloud storage If not, Zip em ourselves? If so, just go ahead and upload.
",[],[],1
688,dbsync: improve performance,https://api.github.com/repos/move-coop/parsons/issues/503,Eliot Stone,503,closed,2021-03-12 01:38:24+00:00,2021-03-17 19:10:46+00:00,2021-03-17 19:10:43+00:00,"This commit improves performance of the `DBSync` class by making
distinct checks and row counts optional. The commit is meant to
allow users to opt out of these features to workaround issues
seen with really big tables.

This commit also fixes a bug from a previous commit where an
invalid class attribute was being accessed. The value is now
pulled in via a constant in another module.",[],[],0
689,"DB sync ""primary key"" vs. ""order by""",https://api.github.com/repos/move-coop/parsons/issues/502,Eliot Stone,502,open,2021-03-03 20:08:26+00:00,2023-02-21 18:49:22+00:00,,"When syncing data incrementally, the `DBSync` class' `table_sync_incremental` method has a `primary_key` argument. The argument is used to figure out what is ""new"" in the source table. In order to determine what's ""new"", the source table is ordered by the ""primary key"" and any rows greater than a given value are ""newer"" than that value.

The problem is that for some tables, primary keys are not necessarily sequential (e.g. UUIDs are a common primary key type in Postgres, which are semi-random values). This is mostly an issue of documentation -- the `primary_key` argument is being used more as an `order_by`. We should make it more clear how we are using the `primary_key` argument and how users might identify it in their table. For example, sometimes this might be a `created_at` column or something. We might also want to rename the argument to make this more clear.","[Label(name=""documentation""), Label(name=""good first issue""), Label(name=""low priority"")]",[],1
690,Add more logging to DB sync,https://api.github.com/repos/move-coop/parsons/issues/501,Eliot Stone,501,closed,2021-03-03 20:01:07+00:00,2021-03-22 16:05:35+00:00,2021-03-22 16:05:35+00:00,Right now the DB sync tool has minimal logging; it would be helpful to add some `debug` level log messages to help when troubleshooting / profiling DB syncs.,"[Label(name=""enhancement""), Label(name=""medium priority"")]",[],1
691,Issue Passing Non utf-8 files Through Box.get_table_by_file_id Method,https://api.github.com/repos/move-coop/parsons/issues/500,,500,closed,2021-03-02 23:03:56+00:00,2022-03-17 21:06:52+00:00,2022-03-17 21:06:52+00:00,"While trying to load some files from Box into redshift using the `box.get_table_by_file_id` method, we got the following error: `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 5561: invalid continuation byte`. We realized that we were only getting this error on excel files that were not saved as CSV utf-8 by a third party. 

Using the chardet package, we attempted to figure out a way to detect the encoding of the excel saved CSVs. The package detected with a confidence of 1.0, that the files were ascii but when we tried to pass that encoding to parsons.from_csv and `petl.fromcsv` we still got a UnicodeDecodeError implying that ascii is also not correct. 

We would love to have a way to reliably detect, anticipate, or at least attempt typical excel encodings and be able to pass those through methods that use `.fromcsv `","[Label(name=""bug""), Label(name=""connector update"")]",[],2
692,TalkWalker Sync ,https://api.github.com/repos/move-coop/parsons/issues/499,,499,closed,2021-03-02 22:57:01+00:00,2022-03-17 23:33:47+00:00,2022-03-17 23:33:47+00:00,"From the member: 

We would like to be able to sync our data from TalkWalker into our S3 warehouse so that we have long-term access to it & we are also potentially interested in seeing if there are any points of reference to match with our other data. 

The Talkwalker API documentation is found here: 
https://apidocs.talkwalker.com/","[Label(name=""enhancement""), Label(name=""new connector"")]",[],0
693,VAN _unpack_signups() method errors on empty table,https://api.github.com/repos/move-coop/parsons/issues/498,,498,open,2021-03-02 17:17:07+00:00,2023-02-21 18:52:00+00:00,,"The `_unpack_signups()` internal method is frequently used in the `Signups()` VAN class, but it throws a hard-to-trace `ValueError` when the method using it returns an empty table, ultimately settling on the message `'person' is not in list`.

I propose that the easiest solution would be to add an `if` statement to the beginning of `_unpack_signups()` to check for rows > 0","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""low priority""), Label(name=""connector update"")]",[],3
694,dbsync: handle create table errors,https://api.github.com/repos/move-coop/parsons/issues/497,Eliot Stone,497,closed,2021-03-01 20:35:39+00:00,2021-03-01 21:26:18+00:00,2021-03-01 21:26:18+00:00,"This commit updates the DB Sync class to workaround errors when
creating tables using SQLAlchemy. With this commit, if the table
creation fails, then the DB Sync class will fallback to just using
`copy` to create the table.",[],[],0
695,Box authentication,https://api.github.com/repos/move-coop/parsons/issues/496,Melissa Woods,496,open,2021-02-26 22:41:09+00:00,2023-02-21 18:52:21+00:00,,"The current Box class setup has instructions for getting a one-time access token which is only valid for 60 minutes and does not come with a refresh token. We need this class to use the SDK to automatically refresh so that we don't need to provide a new one-time access token manually each time we run a script for example in a scheduled job.

[SDK Documentation](https://github.com/box/box-python-sdk)

The Zoom class does this already by requiring a client ID and secret and then generating the access token in the init.","[Label(name=""bug""), Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update"")]",[],0
696,Extended RockTheVote Connector,https://api.github.com/repos/move-coop/parsons/issues/495,Seth Russell,495,closed,2021-02-25 20:52:04+00:00,2021-03-03 19:59:25+00:00,2021-03-03 19:59:25+00:00,I added the method `get_state_requirements` to rtv.py.  This makes the state_requirements API call to the RockyAPI.,[],[],0
697,DBSync (Redshift) not recognizing views,https://api.github.com/repos/move-coop/parsons/issues/494,elyse-weiss,494,closed,2021-02-25 15:32:11+00:00,2021-03-22 17:56:56+00:00,2021-03-22 17:56:56+00:00,"For TMC folks - Civis link: https://platform.civisanalytics.com/spa/#/scripts/custom/89471826

In this case, I am syncing a view, not a table. The view definitely exists, but the error is telling me the table does not exist. 

From some really quick googling, it looks like there might be an additional step we need to take to include views in what SQLAlchemy sees. The final answer to this Stack Overflow post is the lead I'll follow if no one else fixes this by the time I can look more thoroughly: https://stackoverflow.com/questions/21310549/list-database-tables-with-sqlalchemy","[Label(name=""bug""), Label(name=""medium priority"")]",[],0
698,rs.upsert alter_table not working ,https://api.github.com/repos/move-coop/parsons/issues/493,elyse-weiss,493,closed,2021-02-25 15:28:01+00:00,2023-02-21 19:24:39+00:00,2023-02-21 19:24:39+00:00,"I have noticed this a few times over the last few weeks.

Code I am running:
```
rs.upsert(tbl, ""mvp_bloomerang.constituents_PrimaryAddress"", primary_key = 'id', 
          vacuum=False, distinct_check=True, cleanup_temp_table=False, alter_table=True)
```

Relevant error:
```
InternalError_: Load into table 'constituents_primaryaddress_stg_20210225_1023_8672' failed.  Check 'stl_load_errors' system table for details.
```

STL Load Error:
![image](https://user-images.githubusercontent.com/29580051/109175740-1a789380-7754-11eb-9b62-2ac4ae904277.png)

Seems like `alter_table=True` is not working as expected.","[Label(name=""bug""), Label(name=""connector update""), Label(name=""needs more info"")]",[],3
699,DBSync: Create Table from DDL,https://api.github.com/repos/move-coop/parsons/issues/492,,492,closed,2021-02-22 16:07:23+00:00,2021-02-24 19:25:53+00:00,2021-02-24 19:25:51+00:00,"This PR leverages SQL Alchemy in order to create an empty table. This functionality should reduce the likelihood of errors due to incorrect column types and width. The previous code relied on inferring from sample data.

Need to wait until Eliot's PR for DBSync in merged before pushing this.","[Label(name=""enhancement"")]",[],0
700,Removing MySQLTable class so as not to override BaseTable query methods,https://api.github.com/repos/move-coop/parsons/issues/491,,491,closed,2021-02-19 15:34:42+00:00,2021-02-19 16:58:11+00:00,2021-02-19 16:58:07+00:00,"I discovered that the actual cause of the strange powers of 2 problem when using DBSync to do a full refresh from a MySQL table with (significantly) more rows than the chunk size was that the LIMIT clause in the `MySQLTable()` class's override of the `get_rows()` method was configured incorrectly, such that OFFSET and LIMIT were switched. The default SQL provided by the `BaseTable()` class, however, would actually work properly in MySQL.

Considering that the only purpose of the `MySQLTable()` class appeared to be to override that method as well as `get_new_rows()`, and that issue #453 makes the same observation about _that_ method, it seemed simplest to remove the `MySQLTable()` class entirely, and default to the `BaseTable()` class, instead.",[],"[NamedUser(login=""ydamit"")]",0
701,dbsync: add retries and configurable chunking,https://api.github.com/repos/move-coop/parsons/issues/490,Eliot Stone,490,closed,2021-02-16 21:42:33+00:00,2021-02-24 18:58:23+00:00,2021-02-24 18:58:20+00:00,"This commit makes a few fixes to the Parsons DBSync class
to improve the performance and stability of the functionality.

 * Add a `retries` argument to the `DBSync` class to configure
 the number of times the sync will retry reading / writing data
 if there is a failure. This should make the sync process less
 brittle.
 * Add `read_chunk_size` and `write_chunk_size` arguments to
 the `DBSync` class to allow users to configure how much data is
 downloading from the source DB or written to the destination DB
 at a time.
 * Added a `strict_length` argument to the Redshift `copy_s3` and
 all DB `copy` functions that allows users to opt out or into of new
 functionality when creating tables to use a step function when
 determining what length to specify on a `varchar` column.

---
This is one of those ""went to do 1 or 2 things and ended up doing 5"" kind of Pull Requests. I'm happy to rework or break up if it seems useful. I tested the DB sync script against Postgres to Postgres, Postgres to Redshift, and Redshift to Postgres.",[],[],0
702,postgres should just use text when creating string columns,https://api.github.com/repos/move-coop/parsons/issues/489,Eliot Stone,489,open,2021-02-16 17:42:18+00:00,2023-02-21 18:55:35+00:00,,Postgres' `text` column type allows for variable length strings to be stored in a column without worrying about defining a max string length. Leveraging this would allow us to avoid the problems we have seen in Redshift around attempting to append data to a table that was not sized appropriately.,"[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update""), Label(name=""needs more info"")]",[],1
703,refactor(Table): Use petl to load json lines files,https://api.github.com/repos/move-coop/parsons/issues/488,Daniel,488,closed,2021-02-13 23:15:36+00:00,2021-03-29 19:10:33+00:00,2021-03-22 15:52:55+00:00,"As mentioned in #432, `petl` supports json lines in more recent versions. Nows that the `petl` version req has been bumped to `1.6.8` it's possible to have `petl` load the data directly.",[],[],0
704,fix(Table): Fix pass-through kwargs for to_civis,https://api.github.com/repos/move-coop/parsons/issues/487,Daniel,487,closed,2021-02-13 21:32:42+00:00,2021-03-29 19:10:09+00:00,2021-02-25 20:37:28+00:00,"The `Table.to_civis` method take multiple keyword args that get passed through to the underlying `CivisClient.table_import` method. However, many were passed as `None`.",[],[],0
705,feat(CivisClient): Modify methods to return futures,https://api.github.com/repos/move-coop/parsons/issues/486,Daniel,486,closed,2021-02-13 21:25:34+00:00,2021-03-29 19:10:05+00:00,2021-02-25 20:37:39+00:00,"The CivisClient methods have an option to 'wait' for an underlying civis methods to complete. However, if 'wait' if set to False then there is not access the results of the method call.",[],[],0
706,feat(ActionKit): Add methods to get events,https://api.github.com/repos/move-coop/parsons/issues/485,Daniel,485,closed,2021-02-13 20:36:20+00:00,2021-03-29 19:09:56+00:00,2021-03-22 15:59:17+00:00,This PR add the methods to get a single event or multiple events from Action Kit.,[],[],0
707,Bump httplib2 from 0.18.0 to 0.19.0,https://api.github.com/repos/move-coop/parsons/issues/484,,484,closed,2021-02-08 20:32:28+00:00,2021-03-22 15:59:45+00:00,2021-03-22 15:59:31+00:00,"Bumps [httplib2](https://github.com/httplib2/httplib2) from 0.18.0 to 0.19.0.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/httplib2/httplib2/blob/master/CHANGELOG"">httplib2's changelog</a>.</em></p>
<blockquote>
<p>0.19.0</p>
<p>auth: parse headers using pyparsing instead of regexp
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/pull/182"">httplib2/httplib2#182</a></p>
<p>auth: WSSE token needs to be string not bytes
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/pull/179"">httplib2/httplib2#179</a></p>
<p>0.18.1</p>
<p>explicit build-backend workaround for pip build isolation bug
&quot;AttributeError: 'module' object has no attribute '<strong>legacy</strong>'&quot; on pip install
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/issues/169"">httplib2/httplib2#169</a></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/httplib2/httplib2/commit/81e80d00c2b3cb51de6019cfd28b793184d7699a""><code>81e80d0</code></a> v0.19.0 release</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/c3aed1eea5ddcc90344ad5c483d3aee22a135473""><code>c3aed1e</code></a> fix release script, interactive part</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/bd9ee252c8f099608019709e22c0d705e98d26bc""><code>bd9ee25</code></a> parse auth headers using pyparsing instead of regexp</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/33090ab8f40ee6d4507d602df4641b5eaad84e1e""><code>33090ab</code></a> initial fuzz testing integration with OSS-Fuzz</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/595e248d0958c00e83cb28f136a2a54772772b50""><code>595e248</code></a> auth: WSSE token needs to be string not bytes</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/9bf300cdc372938f4237150d5b9b615879eb51a1""><code>9bf300c</code></a> v0.18.1 release</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/cb2940a5046c34b6c3568054e8679ae064da4f72""><code>cb2940a</code></a> explicit build-backend workaround pip build isolation bug 6264</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/94f48efe2ffb1caa3fbcba0598e7583df02b832a""><code>94f48ef</code></a> check-manifest build tool</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/828c26d8ca1e7e3c9c3e154885c9bf3a13426cbe""><code>828c26d</code></a> Security Policy</li>
<li>See full diff in <a href=""https://github.com/httplib2/httplib2/compare/v0.18.0...v0.19.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=httplib2&package-manager=pip&previous-version=0.18.0&new-version=0.19.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
708,Address Validation,https://api.github.com/repos/move-coop/parsons/issues/483,,483,closed,2021-02-02 15:08:16+00:00,2022-03-17 21:30:09+00:00,2022-03-17 21:30:09+00:00,Creating an issue to see if anyone has come across a free address validation package that could be incorporated into Parsons,"[Label(name=""enhancement""), Label(name=""new connector"")]",[],0
709,fix bare except linting error,https://api.github.com/repos/move-coop/parsons/issues/482,Eliot Stone,482,closed,2021-01-30 14:04:35+00:00,2021-02-04 18:06:55+00:00,2021-02-04 18:06:51+00:00,,[],[],0
710,DB Sync use source table schema to build schema for destination table,https://api.github.com/repos/move-coop/parsons/issues/481,Eliot Stone,481,closed,2021-01-29 20:25:17+00:00,2021-03-22 16:07:08+00:00,2021-03-22 16:07:08+00:00,"Right now, the DB sync classes leans on the `copy` function to create destination tables during a sync. So, the first chunk of data loaded into the destination is used to determine the table schema. However, if data from another chunk differs from the first chunk (e.g. longer/bigger values in columns), the db sync can fail.

Ideally the DB sync would reflect on the schema of the source table and use that to determine the schema for the destination table.",[],[],2
711,DB Sync Lint,https://api.github.com/repos/move-coop/parsons/issues/480,,480,closed,2021-01-26 22:38:34+00:00,2021-08-10 22:54:14+00:00,2021-01-27 15:36:34+00:00,Fixes some linting issues that came up after the last PR was merged.,[],[],0
712,Linting Required in DB Sync Class,https://api.github.com/repos/move-coop/parsons/issues/479,,479,closed,2021-01-26 22:26:40+00:00,2021-03-22 16:05:58+00:00,2021-03-22 16:05:58+00:00,"While submitting another PR, CircleCI complained of the following linting failures:
```
parsons/databases/db_sync.py:41:98: W291 trailing whitespace
parsons/databases/db_sync.py:43:96: W291 trailing whitespace
parsons/databases/db_sync.py:68:17: E722 do not use bare 'except'
```","[Label(name=""low priority"")]",[],1
713,Zoom JWT Encryption Fix,https://api.github.com/repos/move-coop/parsons/issues/478,,478,closed,2021-01-26 22:15:33+00:00,2021-08-10 22:54:12+00:00,2021-01-27 14:46:43+00:00,"Turns out it was a comedy of errors in which both the desired package, PyJWT and another similar one, python-jwt, are both imported with `import jwt`. The latter took precedence for some reason, until we added PyJWT explicitly into requirements.txt.

That worked locally, but I'd also like to try to test this live once before merging.",[],"[NamedUser(login=""ydamit"")]",0
714,feat(Bloomerang): Add ordering and filtering params,https://api.github.com/repos/move-coop/parsons/issues/477,Daniel,477,closed,2021-01-26 19:44:19+00:00,2021-03-29 19:09:50+00:00,2021-01-27 15:34:13+00:00,"Add ordering parameters `order_by` and `order_direction` to:
- `get_constituents`
- `get_transactions`
- `get_transaction_designations`

Add filtering parameter `last_modified` to:
- `get_constituents`",[],[],2
715,Add Params to Bloomerang Class,https://api.github.com/repos/move-coop/parsons/issues/476,elyse-weiss,476,closed,2021-01-26 18:29:51+00:00,2021-03-22 17:57:10+00:00,2021-03-22 17:57:10+00:00,"Hello! I have a fairly urgent need to use the Bloomerang class, and would LOVE if someone could help me out by adding some additional parameters.

For the functions that loop through pages - `get_constituents`, `get_transactions`, `get_transaction_designations`, I would love to add:
- [ ] orderBy
- [ ] orderDirection
- [ ] lastModified (only relevant for constituents)

I know @rgriff23 was the original author!","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""high priority"")]",[],4
716,dbsync drop if needed,https://api.github.com/repos/move-coop/parsons/issues/475,elyse-weiss,475,closed,2021-01-25 14:17:59+00:00,2021-01-26 15:32:29+00:00,2021-01-26 15:32:25+00:00,Closes #474 ,[],[],0
717,add drop_if_error to Parsons DBSync,https://api.github.com/repos/move-coop/parsons/issues/474,elyse-weiss,474,closed,2021-01-19 20:57:43+00:00,2021-01-26 15:32:25+00:00,2021-01-26 15:32:25+00:00,"I had a DBSync set to `truncate` and it failed because of a varchar length or something equally benign. In this case, I am also more than happy to drop when needed. Seems like we could easily add a `try/except` block where the parameter option is `drop_if_error`.

","[Label(name=""enhancement"")]",[],0
718,Creation of an Action Builder class,https://api.github.com/repos/move-coop/parsons/issues/473,,473,closed,2021-01-12 20:07:48+00:00,2024-07-22 01:38:09+00:00,2024-07-22 01:38:09+00:00,"Action Builder is currently redoing their API, so this would be pending that

Right now, the only way to upload new data to Action Builder is to manually do it via a CSV upload

I would like for that to be easier!

I (Rebecca @ DSA) am happy to intro folks to Martha Grant to make this easier!","[Label(name=""enhancement""), Label(name=""high priority""), Label(name=""new connector"")]",[],3
719,zoom.get_request() error in latest version,https://api.github.com/repos/move-coop/parsons/issues/472,Tracy Urquhart,472,closed,2021-01-12 14:35:56+00:00,2021-03-22 17:58:10+00:00,2021-03-22 17:58:10+00:00,"I am encountering an error for `zoom.get_meetings()` on the `latest` docker image tag. The script runs as expected with the `0.16.0` docker image tag.

```
Traceback (most recent call last):
2021-01-12 09:16:01 AM  File ""vendors/zoom/zoom_meeting_to_redshift_w_meeting_id.py"", line 96, in <module>
2021-01-12 09:16:01 AM    main()
2021-01-12 09:16:01 AM  File ""vendors/zoom/zoom_meeting_to_redshift_w_meeting_id.py"", line 41, in main
2021-01-12 09:16:01 AM    host_mtg = zoom.get_meetings(host)
2021-01-12 09:16:01 AM  File ""/src/parsons/zoom/zoom.py"", line 121, in get_meetings
2021-01-12 09:16:01 AM    tbl = self._get_request(f'users/{user_id}/meetings', 'meetings')
2021-01-12 09:16:01 AM  File ""/src/parsons/zoom/zoom.py"", line 44, in _get_request
2021-01-12 09:16:01 AM    self.refresh_header_token()
2021-01-12 09:16:01 AM  File ""/src/parsons/zoom/zoom.py"", line 37, in refresh_header_token
2021-01-12 09:16:01 AM    token = jwt.encode(payload, self.api_secret, algorithm='HS256').decode(""utf-8"")
2021-01-12 09:16:01 AMAttributeError: 'str' object has no attribute 'decode'
2021-01-12 09:16:03 AMFailed: The job container failed. Exit code 1
```","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""medium priority"")]","[NamedUser(login=""ydamit"")]",4
720,table: improve __bool__ performance,https://api.github.com/repos/move-coop/parsons/issues/471,Eliot Stone,471,closed,2021-01-08 13:40:27+00:00,2021-01-11 21:44:31+00:00,2021-01-11 21:44:28+00:00,"This commit updates the `__bool__` magic method implementation
in the Parsons `Table` to be more performant. Before this PR the
`__bool__` method used the `num_rows` to determine whether or not
there was any data in the `Table`. The problem is that `num_rows`
will scan the whole `Table`, but `__bool__` only needs to know if
there is a single row of data. The new implementation leverages
the `petl.head` method to pull out a single row from the `Table`.",[],[],0
721,Alchemer: Update Class Name,https://api.github.com/repos/move-coop/parsons/issues/470,,470,closed,2021-01-04 20:27:43+00:00,2021-08-10 22:54:09+00:00,2021-01-04 22:43:06+00:00,"Changes name of the class from SurveyGizmo to Alchemer.

- Has some functions for backwards compatibility. Not sure if it is the most elegant way to do it.
- Adds in the documentation, which was never created.

Addresses #469 ",[],[],0
722,SurveyGizmo now called Alchemer,https://api.github.com/repos/move-coop/parsons/issues/469,elyse-weiss,469,closed,2021-01-04 18:39:09+00:00,2021-01-04 22:45:00+00:00,2021-01-04 22:45:00+00:00,"We should probably update things!

https://www.alchemer.com/","[Label(name=""documentation""), Label(name=""housekeeping/meta"")]",[],0
723,add tabulate to Parsons docker image,https://api.github.com/repos/move-coop/parsons/issues/468,elyse-weiss,468,closed,2020-12-21 16:55:55+00:00,2022-03-17 17:35:53+00:00,2022-03-17 17:35:52+00:00,not urgent! but we use tabulate in notification_helpers.py in canalespy,"[Label(name=""enhancement""), Label(name=""housekeeping/meta"")]",[],1
724,tests: fix more tests,https://api.github.com/repos/move-coop/parsons/issues/467,Eliot Stone,467,closed,2020-12-15 00:03:20+00:00,2020-12-15 17:38:45+00:00,2020-12-15 17:38:42+00:00,"This commit fixes a few more tests that started failing when
adding a row count check as part of `assert_matching_tables`.",[],[],0
725,Track number of rows in Table object,https://api.github.com/repos/move-coop/parsons/issues/466,Eliot Stone,466,open,2020-12-14 14:07:01+00:00,2023-02-21 18:58:21+00:00,,"Currently, calculating the number of rows for a table involves scanning through the entire table to get the count of rows. This operation can be expensive, and it is done every time `num_rows` is called. Ideally, the Parsons `Table` would remember the last value and only re-calculate the number of rows if that underlying `petl` table changes.","[Label(name=""enhancement""), Label(name=""parsons core""), Label(name=""medium priority"")]",[],1
726,Add TargetSmart SmartMatch API to the TargetSmart connector,https://api.github.com/repos/move-coop/parsons/issues/465,Eliot Stone,465,closed,2020-12-09 21:08:44+00:00,2023-02-21 19:01:31+00:00,2023-02-21 19:01:31+00:00,TargetSmart has a new(er) API for matching that would be good to add to the TargetSmart connector: https://docs.targetsmart.com/developers/tsapis/v2/service/smartmatch.html,"[Label(name=""enhancement""), Label(name=""connector update"")]",[],2
727,added zoom apis,https://api.github.com/repos/move-coop/parsons/issues/464,Binny Zupnick,464,closed,2020-12-08 15:37:43+00:00,2020-12-21 13:51:09+00:00,2020-12-08 16:18:04+00:00,"for issue #459 

This is a replica of PR https://github.com/move-coop/parsons/pull/461

I had to make the replica as I didn't create a feature branch from my end but rather pushed directly to master. ",[],[],0
728,copper: fix copper tests,https://api.github.com/repos/move-coop/parsons/issues/463,Eliot Stone,463,closed,2020-12-07 18:43:56+00:00,2020-12-09 13:32:30+00:00,2020-12-09 13:32:26+00:00,"This commit fixes a few bugs in the copper tests. Namely, it
fixes the pagination being done by the test code when simulating
the server, and it aligns the the expected results for one test
with what is being given to the `Copper` connector in the test.

---

cc @crayolakat

This fixes an issue where tests seem to be passing, but adding an explicit check on the number of lines when comparing tables causes things to fail.",[],[],0
729,enabled fixes for Airtables missing columns that are sparsely populated,https://api.github.com/repos/move-coop/parsons/issues/462,Binny Zupnick,462,closed,2020-12-06 18:10:52+00:00,2020-12-08 16:18:41+00:00,2020-12-08 16:18:41+00:00,"PR for issue #454 

The notes in the issue linked above go through the investigation. In their I laid out 2 different solutions:

1. give the `header` manually.
2. Make the `sample` larger.

This PR enables the developer to do both of those. 

The default `sample_size` remains 1000 but the developer can make that larger (or smaller) by sending that parameter to `get_records`.

Additionally, the `fields` parameter will now be sent to `petl` and it will no longer need to use sampling at all but rather will take these headers as they are.",[],[],2
730,Added functionality to Zoom class,https://api.github.com/repos/move-coop/parsons/issues/461,Binny Zupnick,461,closed,2020-12-05 22:29:35+00:00,2020-12-08 15:39:54+00:00,2020-12-08 15:39:35+00:00,"This is a PR for [this issue](https://github.com/move-coop/parsons/issues/459).

This is my first PR here so let me know if I missed something!",[],[],2
731,Bump petl from 1.2.0 to 1.6.8,https://api.github.com/repos/move-coop/parsons/issues/460,,460,closed,2020-12-02 18:29:30+00:00,2021-01-04 20:28:36+00:00,2021-01-04 20:28:28+00:00,"Bumps [petl](https://github.com/petl-developers/petl) from 1.2.0 to 1.6.8.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/petl-developers/petl/releases"">petl's releases</a>.</em></p>
<blockquote>
<h2>v1.6.8</h2>
<h3>Channges</h3>
<ul>
<li>Allow using a custom/restricted xml parser in fromxml() on <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/527"">#527</a>.</li>
<li>Fixed the <a href=""https://github.com/nvn1729/advisories/blob/master/cve-2020-29128.md"">CVE-2020-29128</a>.</li>
</ul>
<h2>v1.6.7</h2>
<h2>Changes</h2>
<ul>
<li>Reduced memory footprint for JSONL files, huge improvement on <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/523"">#523</a>.</li>
</ul>
<h2>v1.6.6</h2>
<h2>Changes</h2>
<ul>
<li>Fixed <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/487"">#487</a>: compatibility with python3.8 in <code>petl.timings.clock()</code>.</li>
<li>Added <a href=""https://jsonlines.org"">json lines</a> support in <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/521"">#521</a></li>
<li>Added testing <code>petl</code> with python3.8 in CI. <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/517"">#517</a> also prepares for testing with python3.9.</li>
</ul>
<h2>v1.6.5</h2>
<h2>Changes</h2>
<ul>
<li>Fixed <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/514"">#514</a>: <code>fromxlsx()</code> with <code>read_only=True</code> crashes.</li>
</ul>
<h2>v1.6.4</h2>
<h2>Changes</h2>
<ul>
<li>Fixed exception when writing to S3 with <code>fsspec</code> <code>auto_mkdir=True</code> on <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/512"">#512</a>.</li>
</ul>
<h2>v1.6.3</h2>
<h3>Changes</h3>
<ul>
<li>Allowed reading and writing Excel files in remote sources.  On <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/506"">#506</a>.</li>
<li>Allow <code>toxlsx()</code> to add or replace a worksheet.  On <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/502"">#502</a>.</li>
<li>Improved avro: improve message on schema or data mismatch. On <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/507"">#507</a>.</li>
<li>Fixed build for failed test case on <a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/508"">#508</a></li>
</ul>
<h2>v1.6.2</h2>
<h3>Changes</h3>
<ul>
<li>Fixed boolean type detection in <code>toavro()</code>. (:issue:<code>504</code>).</li>
<li>Fixed another unavoidable warning if <a href=""https://filesystem-spec.readthedocs.io/en/latest/"">fsspec</a> is installed but some optional package is not installed. (:issue:<code>503</code>).</li>
</ul>
<h2>v1.6.1</h2>
<h3>Changes</h3>
<ul>
<li>Fixed: unavoidable <code>warning</code> downgraded to <code>debug</code>. (<a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/500"">#500</a>)</li>
<li>Feature: added <code>extra_requires</code>  and documentation on <a href=""https://petl.readthedocs.io/en/stable/intro.html#dependencies-and-extensions"">Installation dependencies</a>. (<a href=""https://github-redirect.dependabot.com/petl-developers/petl/issues/501"">#501</a>)</li>
</ul>
<h2>v1.6.0</h2>
<ul>
<li>Feature: <a href=""https://petl.readthedocs.io/en/latest/io.html#remote-i-o-helper-classes"">read and write</a> from files in <a href=""https://filesystem-spec.readthedocs.io/en/latest/api.html#implementations"">remote</a> <a href=""https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations"">servers/clouds</a> using <a href=""https://github.com/intake/filesystem_spec"">fsspec</a> package.</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/petl-developers/petl/blob/master/docs/changes.rst"">petl's changelog</a>.</em></p>
<blockquote>
<h2>Version 1.6.8</h2>
<ul>
<li>Allow using a custom/restricted xml parser in <code>fromxml()</code>.
By :user:<code>juarezr</code>, :issue:<code>527</code>.</li>
</ul>
<h2>Version 1.6.7</h2>
<ul>
<li>Reduced memory footprint for JSONL files, huge improvement.
By :user:<code>fahadsiddiqui</code>, :issue:<code>522</code>.</li>
</ul>
<h2>Version 1.6.6</h2>
<ul>
<li>
<p>Added python version 3.8 and 3.9 to tox.ini for using in newer distros.
By :user:<code>juarezr</code>, :issue:<code>517</code>.</p>
</li>
<li>
<p>fix compatibility with python3.8 in <code>petl.timings.clock()</code>.
By :user:<code>juarezr</code>, :issue:<code>484</code>.</p>
</li>
<li>
<p>add json lines support. By :user:<code>fahadsiddiqui</code>.</p>
</li>
</ul>
<h2>Version 1.6.5</h2>
<ul>
<li>Fixed fromxlsx with read_only crashes.
By :user:<code>juarezr</code>, :issue:<code>514</code>.</li>
</ul>
<h2>Version 1.6.4</h2>
<ul>
<li>Fixed exception when writing to S3 with <code>fsspec</code> <code>auto_mkdir=True</code>.
By :user:<code>juarezr</code>, :issue:<code>512</code>.</li>
</ul>
<h2>Version 1.6.3</h2>
<ul>
<li>
<p>Allowed reading and writing Excel files in remote sources.
By :user:<code>juarezr</code>, :issue:<code>506</code>.</p>
</li>
<li>
<p>Allow <code>toxlsx()</code> to add or replace a worksheet.
By :user:<code>churlrich</code>, :issue:<code>502</code>.</p>
</li>
<li>
<p>Improved avro: improve message on schema or data mismatch.
By :user:<code>juarezr</code>, :issue:<code>507</code>.</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/petl-developers/petl/commit/52447c0a2ff87f2c7bdd0017a5f3b6874ad262c6""><code>52447c0</code></a> more fixes for xml test failure in ci</li>
<li><a href=""https://github.com/petl-developers/petl/commit/c3223fe9710cdb55ce5b41d2b5e5c95b71b4df9a""><code>c3223fe</code></a> improve custom xml parser docs</li>
<li><a href=""https://github.com/petl-developers/petl/commit/238219ae808588e69bbe1b599ff9f3b38cd25929""><code>238219a</code></a> fix test failure for fromxml() without lxml</li>
<li><a href=""https://github.com/petl-developers/petl/commit/ab7ec4fa5ae34410b121590c6fb16328e3eaf21b""><code>ab7ec4f</code></a> add changes for v1.6.8</li>
<li><a href=""https://github.com/petl-developers/petl/commit/d45e25045002ab8bfa7ce6f8015ef80828728208""><code>d45e250</code></a> test custom xml parser for fromxml()</li>
<li><a href=""https://github.com/petl-developers/petl/commit/07420ef8463cc387aea84e2d6241cf556574e2a5""><code>07420ef</code></a> allow using a custom/restricted xml parser</li>
<li><a href=""https://github.com/petl-developers/petl/commit/defbc0527fe6e84c5b88a11893095adda4b8856a""><code>defbc05</code></a> fix test test_fromxml_url</li>
<li><a href=""https://github.com/petl-developers/petl/commit/bed3bfbfdfb24608911b3e648ed0c24aeed2d685""><code>bed3bfb</code></a> Changes Written in docs/*.rst</li>
<li><a href=""https://github.com/petl-developers/petl/commit/7ec0488ba10ea5ad0a79763e270c739bf1186c30""><code>7ec0488</code></a> Memory Footprint Resolved</li>
<li><a href=""https://github.com/petl-developers/petl/commit/704cbe4cf1e28011835c59e7af27337bbfdfeb97""><code>704cbe4</code></a> relax more deps for py38</li>
<li>Additional commits viewable in <a href=""https://github.com/petl-developers/petl/compare/v1.2.0...v1.6.8"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=petl&package-manager=pip&previous-version=1.2.0&new-version=1.6.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
732,Additions to Zoom class,https://api.github.com/repos/move-coop/parsons/issues/459,Melissa Woods,459,closed,2020-12-02 14:31:26+00:00,2020-12-08 17:59:00+00:00,2020-12-08 16:18:55+00:00,"It would be helpful to add the following to the Zoom class:

- [get meeting registrants] (https://marketplace.zoom.us/docs/api-reference/zoom-api/meetings/meetingregistrants)
- [get webinars] (https://marketplace.zoom.us/docs/api-reference/zoom-api/webinars/webinars)
- [get webinar participants] (https://marketplace.zoom.us/docs/api-reference/zoom-api/reports/reportwebinarparticipants)
- [get webinar registrants] (https://marketplace.zoom.us/docs/api-reference/zoom-api/webinars/webinarregistrants)","[Label(name=""enhancement"")]",[],2
733,Create s3 Poller to Ping for New File,https://api.github.com/repos/move-coop/parsons/issues/458,elyse-weiss,458,closed,2020-11-30 17:19:24+00:00,2020-12-10 14:09:50+00:00,2020-12-10 14:09:50+00:00,"Idea being to build a helper function to identify if the file you are waiting for in s3 has arrived!

```
keys = ts_s3.list_keys(bucket = bucket, prefix = prefix, suffix = file_name)
        for key in keys:
            logger.info(key)
            last_modified = keys[key]['LastModified']
            if type(last_modified)==str:
                last_modified = parser.parse(last_modified)

            if last_modified.date()==today:
                msg = ""TS has dropped EV data. Kicking off workflow!""
```

Below is some sample code I've used where compare `last_modified` to `today`, but looking to generalize so someone would put a comparison timestamp in as a variable.","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update"")]",[],7
734,Fixing Specifycols in Redshift Copy S3,https://api.github.com/repos/move-coop/parsons/issues/457,,457,open,2020-11-24 23:19:09+00:00,2023-02-21 19:02:16+00:00,,"Right now, the `specifycols` argument in `copy_s3()` is getting passed straight to `copy_statement()` which won't work, and might even cause an error. Its intended functionality (as it stands in `copy()`) is to generate a list of columns to pass to `copy_statement()`.

Since `specifycols` is unnecessary if the table is new, I propose taking the s3 object out of the `if` block, and then incorporating code along the following lines if `specifycols` is set to `True`:
```
s3_dict = s3.client.get_object(
    Bucket=bucket,
    Key=key,
    Range='bytes=0-1000000'
)
body_str = s3_dict['Body'].read().decode(""utf-8"")
first_line = body_str.split(""\n"")[0]
cols = first_line.split(',')
```","[Label(name=""bug""), Label(name=""low priority""), Label(name=""connector update"")]",[],0
735,Add pandas to requirements.txt,https://api.github.com/repos/move-coop/parsons/issues/456,,456,closed,2020-11-10 16:34:20+00:00,2020-11-11 15:54:40+00:00,2020-11-11 15:54:40+00:00,"When using the `movementcooperative/parsons` docker image and running `CivisClient.table_import()`, I get a `ModuleNotFoundError: No module named 'pandas'` error. 

Extended stack trace:
```
File ""/src/parsons/civis/civisclient.py"", line 111, in table_import
    fut = civis.io.dataframe_to_civis(table_obj.to_dataframe(), database=self.db,
File ""/src/parsons/etl/tofrom.py"", line 33, in to_dataframe
    columns=columns, coerce_float=coerce_float)
File ""/usr/local/lib/python3.7/site-packages/petl/io/pandas.py"", line 30, in todataframe
    import pandas as pd
ModuleNotFoundError: No module named 'pandas'
```",[],[],2
736,Box integration not sending csv files,https://api.github.com/repos/move-coop/parsons/issues/455,,455,closed,2020-11-10 01:42:06+00:00,2022-03-18 01:04:24+00:00,2022-03-18 01:04:24+00:00,Files are being sent as .txt and not .csv.,"[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""connector update"")]",[],1
737,Airtable get_records column issue,https://api.github.com/repos/move-coop/parsons/issues/454,Melissa Woods,454,closed,2020-11-05 15:58:20+00:00,2020-12-08 17:54:26+00:00,2020-12-08 16:19:35+00:00,"On larger Airtable tables columns that are sparsely populated may get skipped in get_records unless sort is provided and the column is sorted descending.

The specific example came up in a table with 1,299 records where the column only had data in one record. When we tried to run get_records without any arguments the column doesn't come up in the returned Parsons Table. It also didn't come up when we provided a parameter for fields. Sorting by the column in descending order did work, and using a formula setting the column equal to the exact value worked as well.

We speculate that creating a Parsons Table from a list of dictionaries must take some sort of sample of the first x number of dictionaries to determine the columns and if only one dictionary has a column and it's buried deep in the list it does not get sampled.

Two likely solutions would 1. be able to specify in Table how many rows to sample or 2. have some lines in this specific Airtable method that figures out what all the fields are and fills out all the dictionaries so they all have the same number of items.","[Label(name=""bug"")]",[],4
738,get_new_rows() in MySQL broken,https://api.github.com/repos/move-coop/parsons/issues/453,,453,closed,2020-11-03 13:59:23+00:00,2021-02-20 18:16:11+00:00,2021-02-20 18:16:11+00:00,"The fact that `get_new_rows()` is set up with the expectation that `cutoff_value` is an integer may block the full implementation of another issue I submitted for allow true incremental updates via the DBSync class, using a date field.

A bit of testing suggests that using the native `get_new_rows()` from the `BaseTable` class works just fine, so my proposal is that we simply get rid of the override in `MySQLTable` (though it may be wise to test this some more before committing).","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""medium priority"")]","[NamedUser(login=""ydamit"")]",0
739,Add Timeout Arg to MySQL Connector,https://api.github.com/repos/move-coop/parsons/issues/452,,452,open,2020-11-03 12:15:13+00:00,2023-02-21 19:02:32+00:00,,"I think it's as simple as modifying init and connection along the following lines:
```
def __init__(self, host=None, username=None, password=None, db=None, port=3306, timeout=1200):

    self.username = check_env.check('MYSQL_USERNAME', username)
    self.password = check_env.check('MYSQL_PASSWORD', password)
    self.host = check_env.check('MYSQL_HOST', host)
    self.db = check_env.check('MYSQL_DB', db)
    self.port = port or os.environ.get('MYSQL_PORT')
    self.timeout = os.environ.get('MYSQL_TIMEOUT') or timeout

@contextmanager
def connection(self):

    # Create a mysql connection and cursor
    connection = mysql.connect(host=self.host,
                               user=self.username,
                               passwd=self.password,
                               database=self.db,
                               port=self.port,
                               connect_timeout=self.timeout
                              )

    try:
        yield connection
    except mysql.Error:
        connection.rollback()
        raise
    else:
        connection.commit()
    finally:
        connection.close()
```","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""low priority""), Label(name=""connector update"")]",[],0
740,redshift: upsert(from_s3:bool) and s3_copy(template_table:str),https://api.github.com/repos/move-coop/parsons/issues/451,Schuyler Duveen,451,closed,2020-10-28 18:33:34+00:00,2021-04-27 21:49:38+00:00,2021-04-27 21:49:38+00:00,"Instead of using a local table, this allows using the upsert command to upload an existing S3 file.

cc: @crayolakat ",[],"[NamedUser(login=""ydamit"")]",3
741,Implement Shopify driver,https://api.github.com/repos/move-coop/parsons/issues/450,Kathy Nguyen,450,closed,2020-10-21 16:20:46+00:00,2020-12-21 13:45:12+00:00,2020-12-21 13:45:12+00:00,"Driver for Shopify API

Also updated utils.py to ensure that the tables are the same length. (Before, if the tables had different lengths, the assertion would pass as long as the content of shorter table with length n matched the content of the first n rows of the longer table.)",[],[],10
742,table: materialize_to_file return file,https://api.github.com/repos/move-coop/parsons/issues/449,Eliot Stone,449,closed,2020-10-19 22:08:19+00:00,2020-10-20 16:24:33+00:00,2020-10-20 16:24:30+00:00,"This commit updates the `Table.materialize_to_file` method to
return the path to the temp file that contains the data of the
Parsons `Table`. This allows users to track and clean up the
temp files as needed.",[],[],0
743,Table.from_s3_csv: Allow for zipped archives.,https://api.github.com/repos/move-coop/parsons/issues/448,,448,closed,2020-10-19 14:57:30+00:00,2021-08-10 22:54:05+00:00,2020-10-20 14:20:34+00:00,Fixes a bug that was not unzipping s3 archives.,"[Label(name=""bug"")]",[],0
744,rs.get_table_definition() errors,https://api.github.com/repos/move-coop/parsons/issues/447,elyse-weiss,447,closed,2020-10-19 14:35:35+00:00,2021-11-03 21:50:56+00:00,2021-11-03 21:50:56+00:00,"Getting `InternalError_: cache lookup failed for attribute 1 of relation 23802817`

I know others have experienced errors using this method as well. ","[Label(name=""bug"")]",[],3
745,Action network create_event method,https://api.github.com/repos/move-coop/parsons/issues/446,Ty Schlichenmeyer,446,closed,2020-10-19 08:35:45+00:00,2021-03-31 17:09:53+00:00,2021-03-31 17:09:52+00:00,"First pull request, be gentle 😅 

Took the code shared from #412 and wrote a unit test based off the previous unit tests written for the AN module.  Please double check my testing logic as i'm still fairly new to writing unit tests especially with mock API requests.  Let me know if anything could be improved!",[],[],0
746,Adding a note to the NGP VAN apply_activist_code function,https://api.github.com/repos/move-coop/parsons/issues/445,Viswa Challa,445,open,2020-10-17 18:00:24+00:00,2023-02-21 19:04:35+00:00,,"Can we add a note in the Parsons docs on the NGP VAN appy_activist_code function that using it will also apply a canvass status to the person? A member raised that this caused an issue in their canvassing counts. It wasn’t clear why the previously loaded activist codes were impacting canvass results without looking at the underlying VAN API docs.

Source docs: https://developers.ngpvan.com/van-api#activist-codes","[Label(name=""documentation""), Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority"")]",[],3
747,Google Cloud Storage: Create Presigned URL,https://api.github.com/repos/move-coop/parsons/issues/444,,444,closed,2020-10-16 14:46:31+00:00,2021-08-10 22:54:04+00:00,2020-10-16 19:17:26+00:00,"This add two methods that mirror existing methods in the S3 class.

`GoogleCloudStorage.get_url()` - Generates a presigned expiring url for the file.

`Table.to_gcs_csv()` - Convenience method to pass a table to GCS.","[Label(name=""enhancement"")]",[],0
748,Databases Docs: Small Typo,https://api.github.com/repos/move-coop/parsons/issues/443,,443,closed,2020-10-16 02:14:50+00:00,2021-08-10 22:54:07+00:00,2021-03-25 18:12:07+00:00,Just found a minor typo in the Quickstart.,[],[],0
749,bump version to v0.16.0,https://api.github.com/repos/move-coop/parsons/issues/442,Eliot Stone,442,closed,2020-10-15 21:15:21+00:00,2021-08-10 22:53:54+00:00,2020-10-16 18:55:18+00:00,Getting ready to release tomorrow.,[],[],0
750,update parsons table docs,https://api.github.com/repos/move-coop/parsons/issues/441,,441,closed,2020-10-15 18:30:37+00:00,2020-11-09 20:13:26+00:00,2020-11-09 20:13:25+00:00,Edited table docs with valid python issue #430 ,[],[],1
751,Deal with dependent views and alter_table(),https://api.github.com/repos/move-coop/parsons/issues/440,elyse-weiss,440,closed,2020-10-12 15:41:24+00:00,2022-01-24 15:07:50+00:00,2022-01-24 15:07:50+00:00,You cannot run an alter_table if there are dependent views.,"[Label(name=""bug""), Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],2
752,Turn off No Disktey/Sortkey Alert on append and truncate,https://api.github.com/repos/move-coop/parsons/issues/439,elyse-weiss,439,closed,2020-10-09 17:47:51+00:00,2020-10-16 20:08:41+00:00,2020-10-16 20:08:41+00:00,"Right now, if you use `to_redshift(..., if_exists='append')` you get an alert related to distkey and sortkey on COPY, which is irrelevant. We should turn it off if `if_exists` is append or truncate. ","[Label(name=""low priority"")]",[],2
753,Fixing test_gmail.py,https://api.github.com/repos/move-coop/parsons/issues/438,Seth Russell,438,closed,2020-10-08 19:59:28+00:00,2020-10-11 00:36:45+00:00,2020-10-11 00:36:45+00:00,"This is my fix for Issue #282 Unit test failures on Windows
TestGmail.test_create_message_attachments() uses the file loremipsum_b64_txt.txt to compare against the base64 encoding of the loremipsum.txt file.  The test and file assume you are working on a non-windows machine, where reading the file would show newlines as '\n' rather than '\r\n'.  This difference in the line separator results in a different base64 encoding of the file on windows machines.
So I created loremipsum_b64_win_txt.txt and adjusted the test to use this file if the user's operating system uses '\r\n' for line separators.",[],[],0
754,add instructions to modify parsons/_init_.py to build_a_connector.rst,https://api.github.com/repos/move-coop/parsons/issues/437,,437,closed,2020-10-08 19:55:42+00:00,2021-10-28 22:28:48+00:00,2021-10-28 22:28:48+00:00,Documentation updates for #416 ,[],[],1
755,Add SurveyGizmo connector to the top-level parson's imports,https://api.github.com/repos/move-coop/parsons/issues/436,Corey Haines,436,closed,2020-10-08 18:36:41+00:00,2020-10-08 19:22:32+00:00,2020-10-08 19:22:32+00:00,"When building the connector originally, the last step to add it to the parson's top-level import wasn't done.

Based on [this comment](https://github.com/move-coop/parsons/issues/217#issuecomment-698567117)",[],[],0
756,add instructions for building docs,https://api.github.com/repos/move-coop/parsons/issues/435,,435,closed,2020-10-08 17:54:17+00:00,2020-10-08 18:49:58+00:00,2020-10-08 18:49:58+00:00,Addresses issue #425 ,[],[],1
757,Twilio support for loading all of an account's phone numbers (possibly with any other metadata),https://api.github.com/repos/move-coop/parsons/issues/434,Schuyler Duveen,434,open,2020-10-08 15:25:38+00:00,2023-02-21 19:05:34+00:00,,There's a use-case for getting the full list of phone numbers (no matter how big) from an account (or subaccount) so they can be shared within TMC -- possibly include other details -- ideally returned in a Table type,"[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update"")]",[],1
758,etl: refactor match columns,https://api.github.com/repos/move-coop/parsons/issues/433,Eliot Stone,433,closed,2020-10-07 02:55:26+00:00,2020-10-17 13:15:34+00:00,2020-10-17 13:15:31+00:00,"This commit refactors the implementation of the
`Table.match_columns` method to make it more efficient.
Specifically, the changes replace a number of `rename_column`,
'move_column', and `remove_column` calls with a call to `cut`
and a call to `setheader`.",[],[],0
759,Table.from_json use petl when line delimited,https://api.github.com/repos/move-coop/parsons/issues/432,Eliot Stone,432,closed,2020-10-06 23:36:40+00:00,2022-03-17 21:08:26+00:00,2022-03-17 21:08:26+00:00,"The `Table.from_json` method has a `lines_delimited` argument which indicates that the JSON file you are loading is a series of JSON objects that are newline separated, as so:

```json
{""one"": 1, ""two"": 2}
{""one"": 3, ""two"": 4}
```

As opposed to a JSON file that is a single JSON array, like:

```json
[{""one"": 1, ""two"": 2}, {""one"": 3, ""two"": 4}]
```

Right now, if you specify `lines_delimited=True`, the `from_json` method attempts to load the whole file and parse the JSON data all at once, which takes up a lot of memory.

petl's `fromjson` method has a `lines` argument which can be used to load a file with new-line separated JSON objects (or new line separated lists) in compliance with the JSON lines specification. When petl loads this data, it does it incrementally, so the whole file does not need to be loaded into memory at once.

We should swap the implementation of `Table.from_json` to use petl's `fromjson` function with the `lines=True` argument.

As a workaround currently, I am doing the following:

```
import petl

local_file = '/tmp/file.json'
tbl = Table(petl.fromjson(local_file, lines=True))
```","[Label(name=""enhancement""), Label(name=""parsons core"")]",[],2
760,Add zoom_to_van.py sample script,https://api.github.com/repos/move-coop/parsons/issues/431,Shauna Gordon-McKeon,431,closed,2020-10-06 20:50:23+00:00,2020-10-08 20:52:02+00:00,2020-10-08 20:52:01+00:00,,[],[],0
761,Make sure examples in docs are valid Python,https://api.github.com/repos/move-coop/parsons/issues/430,Shauna Gordon-McKeon,430,closed,2020-10-06 19:46:02+00:00,2022-03-17 17:41:50+00:00,2022-03-17 17:41:49+00:00,"I noticed that some of the scripts in [this docs page](https://move-coop.github.io/parsons/html/table.html#parsons.etl.etl.ETL.coalesce_columns) are not valid Python - in particular, they're using non-strings as dictionary keys, for example:

![Screenshot from 2020-10-06 15-45-21](https://user-images.githubusercontent.com/1179362/95252436-fb9c5380-07ea-11eb-8869-a374b23184bf.png)


While this would be valid in Javascript, it's not in Python, and we don't want to mess anyone who is copying and pasting from the docs.  We should fix this and maybe also do a sweep of the docs to find and fix other examples.","[Label(name=""documentation""), Label(name=""good first issue"")]","[NamedUser(login=""ndelrossi7"")]",2
762,Make match_columns more efficient,https://api.github.com/repos/move-coop/parsons/issues/429,Eliot Stone,429,closed,2020-10-06 17:05:04+00:00,2020-10-23 19:32:39+00:00,2020-10-23 19:32:39+00:00,"We have been using `match_columns` more and more to help us standardize incoming data as we load it into the database, but the implementation is a little inefficient. I think we can refactor it to be more efficient using some of built in `petl` functions.",[],"[NamedUser(login=""eliotst"")]",1
763,Add Box connector,https://api.github.com/repos/move-coop/parsons/issues/428,David Pablo Cohn,428,closed,2020-10-04 01:00:56+00:00,2020-10-13 16:58:08+00:00,2020-10-13 16:58:08+00:00,First pass implementation of Box connector.,[],[],11
764,Add VAN script for random sample of a saved list,https://api.github.com/repos/move-coop/parsons/issues/427,Pat Sier,427,closed,2020-10-02 02:49:28+00:00,2020-10-15 20:29:22+00:00,2020-10-15 20:29:22+00:00,"Creates a template script based on some of the conversations in Slack that demonstrates creating a new saved list in VAN based on a random sample of an existing saved list.

This is my first sample script, so I wanted to try something relatively simple to make sure I've got all the right pieces together and that I'm using the VAN API correctly. Let me know what I can change, thanks!",[],[],1
765,"Fix ""note"" display in VAN connector docs",https://api.github.com/repos/move-coop/parsons/issues/426,Shauna Gordon-McKeon,426,closed,2020-10-01 20:49:10+00:00,2022-04-01 17:58:50+00:00,2022-04-01 17:58:50+00:00,"The formatting of some of the note displays in the VAN connector are off.

This is what it looks like:

![Screenshot from 2020-10-01 16-47-43](https://user-images.githubusercontent.com/1179362/94861658-dfbd3a00-0405-11eb-992b-7a6d5ee7366f.png)

This is what it should look like:

![Screenshot from 2020-10-01 16-47-29](https://user-images.githubusercontent.com/1179362/94861682-e946a200-0405-11eb-8037-bb345fc323aa.png)

If whoever fixes this skims through the other connectors looking for similar problems, that'd be great, but just fixing the VAN connector may be enough.

**Edited to add**: it looks like the notes also reference an `email_address` variable but the docstrings only mention an `email` parameter, we should check which is valid.

Another issue to fix with the VAN docs (""support"" misspelled, plus sentence without ending):

![Screenshot from 2020-10-01 19-10-02](https://user-images.githubusercontent.com/1179362/94872129-cb833800-0419-11eb-94c8-c5ef7f8413a3.png)
","[Label(name=""bug""), Label(name=""documentation""), Label(name=""good first issue"")]","[NamedUser(login=""Tomiiwa"")]",7
766,Add instructions for building docs to contributing guide,https://api.github.com/repos/move-coop/parsons/issues/425,Shauna Gordon-McKeon,425,closed,2020-10-01 13:55:31+00:00,2020-10-08 19:15:27+00:00,2020-10-08 19:15:27+00:00,"Currently, the contributing guide just says, about building docs:

> Make your changes and re-build the docs (for now, please follow this guide and/or ask for help in Slack)

The docs should instead include the instructions for how to actually build the docs.  Whoever solves this issue should prove that it works (at least on their machine) by building the docs from scratch.

Instructions are [here](https://github.com/move-coop/parsons/pull/313#issuecomment-660397875) but instead of listing out the packages to install we should just tell people to install whatever's in `/docs/requirements.txt`.","[Label(name=""documentation""), Label(name=""good first issue""), Label(name=""medium priority"")]","[NamedUser(login=""shepardjma"")]",1
767,Clean up docs args,https://api.github.com/repos/move-coop/parsons/issues/424,Randi Griffin,424,closed,2020-09-30 19:55:16+00:00,2020-10-06 11:41:33+00:00,2020-10-06 11:41:32+00:00,"Minor fixes for #269:

- Bloomerang: rm some extraneous backticks from args in docstrings
- Twilio: escape hyperlink formatting using backslash in the `from_` arg",[],[],3
768,New connector: Sisense / Periscope ,https://api.github.com/repos/move-coop/parsons/issues/423,Randi Griffin,423,closed,2020-09-30 19:31:32+00:00,2020-10-08 13:03:49+00:00,2020-10-08 13:03:48+00:00,"This PR adds an MVP Sisense/Periscope connector and should close #270. 

Summary of class methods:

- `get_dashboards` fetches a specific dashboard by ID or set of dashboards, depending on whether the argument `dashboard_id` is passed
- `get_dashboard_shares` fetches shares for a specific dashboard by ID or set of dashboards, depending on whether the argument `share_id` is passed
- `publish_dashboard` publishes a dashboard 

I don't have credentials so I have not tested the methods against the API. ",[],[],2
769,docs: unit test doc,https://api.github.com/repos/move-coop/parsons/issues/422,Eliot Stone,422,closed,2020-09-27 20:08:27+00:00,2021-03-09 17:32:23+00:00,2021-03-09 17:32:21+00:00,"This commit adds a HOW TO doc for writing unit tests. The doc
is meant to give an overview of writing unit tests to
contributors.",[],[],0
770,Twilio argument bug ,https://api.github.com/repos/move-coop/parsons/issues/421,Randi Griffin,421,closed,2020-09-26 14:48:07+00:00,2020-10-01 14:13:22+00:00,2020-09-26 14:52:11+00:00,"Fix Twilio bug #419, leaving other documentation changes in place",[],[],3
771,"Revert ""Docs general cleanup (#415)""",https://api.github.com/repos/move-coop/parsons/issues/420,Shauna Gordon-McKeon,420,closed,2020-09-26 14:35:45+00:00,2021-08-10 22:53:59+00:00,2020-09-26 14:44:55+00:00,This reverts commit 049ab3333650748dd7f19a4dca221f084724bd8c for reasons discussed here: https://github.com/move-coop/parsons/issues/419,[],[],4
772,Twilio get_messages not working,https://api.github.com/repos/move-coop/parsons/issues/419,,419,closed,2020-09-26 14:23:24+00:00,2020-10-01 20:31:34+00:00,2020-10-01 20:31:34+00:00,"A script that ran successfully one day returned an unfamiliar error the next, saying
```
TypeError: list() got an unexpected keyword argument 'sent_from'
```
I had a really hard time finding any clear instructions in Twilio's REST API documentation, but it seems like that's not an argument the endpoint is expecting, so I'm not sure what motivated that recent change. Since my script didn't have to pass that particular information, it was easy for me to refer directly to the underlying client, and I'm not aware of too many other uses of the method (or even the class), so I don't think this is urgent to fix.","[Label(name=""bug"")]",[],5
773,surveygizmo: fix test lint errors,https://api.github.com/repos/move-coop/parsons/issues/418,Eliot Stone,418,closed,2020-09-26 13:33:39+00:00,2021-08-10 22:53:56+00:00,2020-09-26 14:43:05+00:00,"This commit fixes a few minor lint errors in the tests for the
`SurveyGizmo` Connector.

---

When these changes were put up for PR, the branch didn't have the new linting rules so the branch passed for the pull request. After being merged into master, the new rules were run against the changes on master, and failed.",[],[],0
774,Issue 269,https://api.github.com/repos/move-coop/parsons/issues/417,David Pablo Cohn,417,closed,2020-09-25 05:19:02+00:00,2020-09-29 19:18:44+00:00,2020-09-29 19:18:43+00:00,Filled out the google.rst doc with quickstarts and enhanced overviews.,[],[],6
775,Add missing import information to How to Build a Connector doc,https://api.github.com/repos/move-coop/parsons/issues/416,Shauna Gordon-McKeon,416,closed,2020-09-24 20:20:26+00:00,2022-03-17 17:43:18+00:00,2022-03-17 17:43:18+00:00,The [How to Build a Connector](https://move-coop.github.io/parsons/html/build_a_connector.html) doc currently doesn't remind the user to hook up the connector to Parson's quirky import system.  See [this comment](https://github.com/move-coop/parsons/issues/217#issuecomment-698567117) (step 1) for the missing information.,"[Label(name=""documentation""), Label(name=""enhancement""), Label(name=""good first issue"")]","[NamedUser(login=""shepardjma"")]",4
776,Docs general cleanup,https://api.github.com/repos/move-coop/parsons/issues/415,Randi Griffin,415,closed,2020-09-24 17:38:56+00:00,2020-09-26 11:40:45+00:00,2020-09-25 16:27:15+00:00,"This PR is a final(ish) cleanup of the docs for #269 , not including the AWS and Google connectors which are still under review or in progress. I passed over all the connectors at `https://move-coop.github.io/parsons/html/index.html` and identified errors, many of which slipped through my recent PRs because I am not set up to test the docs locally. 

Some common mistakes and omissions corrected in this PR include:

- Many Quickstart code blocks were not appearing at all! This happened wherever I failed to include a blank line after `.. code-block:: python`.
- Many Quickstart code blocks had inconsistent indendantion (some white space issues were propogated to many connectors as i was copy-and-pasting without realizing the issue)
- Many docs were missing section headers (e.g., `Overview`, `API`), making it impossible to jump to the desired section from the sidebar menu.
- Some docstrings were missing type hints or had extra args
- Miscellaneous typos and misformatted text, code, and hyperlinks.

Some isolated bugs:

- NewMode's API 'autoclass' section was not rendering due to a formatting bug.
- Twilio had an argument ending in an underscore, which was appearing in the rendered docs as a hyperlink
- Twilio and Turbovote were out of order in the otherwise alphabetized index ",[],[],0
777,True Incremental DB Sync Using Upsert,https://api.github.com/repos/move-coop/parsons/issues/414,,414,open,2020-09-23 19:00:39+00:00,2023-02-21 19:25:06+00:00,,"The current `table_sync_incremental()` method only appends new rows from the source DB. It doesn't replace rows that have been altered. I extended the DBSync class with this method recently in a successful effort to sync tables in two DBs where the source rows can change. Requires there to be some sort of `updated_at` column.
```
def table_sync_incremental_upsert(self, source_table, destination_table, primary_key,
                               updated_col, distinct_check=True, **kwargs):
    """"""
    Incremental sync of table from a source database to a destination database
    using an an updated_at column.

    `Args:`
        source_table: str
            Full table path (e.g. ``my_schema.my_table``)
        destination_table: str
            Full table path (e.g. ``my_schema.my_table``)
        if_exists: str
            If destination table exists either ``drop`` or ``truncate``. Truncate is
            useful when there are dependent views associated with the table.
        primary_key: str
            The name of the primary key. This must be the same for the source and
            destination table.
        updated_col: str
            The name of the column storing when the record in each row was last
            updated. This must be the same for the source and destination table.
        distinct_check: bool
            Check that the source table primary key is distinct prior to running the
            sync. If it is not, an error will be raised.
        **kwargs: args
            Optional copy arguments for destination database.
    `Returns:`
        ``None``
    """"""

    # Create the table objects
    source_tbl = self.source_db.table(source_table)
    destination_tbl = self.dest_db.table(destination_table)

    # Check that the destination table exists. If it does not, then run a
    # full sync instead.
    if not destination_tbl.exists:
        self.table_sync_full(source_table, destination_table)

    # If the source table contains 0 rows, do not attempt to copy the table.
    if source_tbl.num_rows == 0:
        logger.info('Source table contains 0 rows')
        return None

    # Check that the source table primary key is distinct
    if distinct_check and not source_tbl.distinct_primary_key(primary_key):
        raise ValueError(f'{primary_key} is not distinct in source table.')

    # Get the max source table and destination table primary key
    source_max_updated = source_tbl.max_primary_key(updated_col)
    dest_max_updated = destination_tbl.max_primary_key(updated_col)

    # Check for a mismatch in row counts; if dest_max_pk is None, or destination is empty
    # and we don't have to worry about this check.
    if dest_max_updated is not None and dest_max_updated > source_max_updated:
        raise ValueError('Destination DB table updated later than source DB table.')

    # Do not copied if row counts are equal.
    elif dest_max_updated == source_max_updated:
        logger.info('Tables are in sync.')
        return None

    else:
        # Get count of rows to be copied.
        if dest_max_updated is not None:
            new_row_count = source_tbl.get_new_rows_count(updated_col, str(dest_max_updated))
        else:
            new_row_count = source_tbl.num_rows

        logger.info(f'Found {new_row_count} updated rows in source table since {str(dest_max_updated)}')

        copied_rows = 0
        # Copy rows in chunks.
        while copied_rows < new_row_count:
            # Get a chunk
            logger.info(f""OFFSET: {copied_rows}"")
            rows = get_new_rows(source_tbl.table, self.source_db, primary_key=updated_col,
                                           cutoff_value=str(dest_max_updated),
                                           offset=copied_rows,
                                           chunk_size=self.chunk_size)

            row_count = rows.num_rows if rows else 0
            if row_count == 0:
                break

            # Copy the chunk
            self.dest_db.upsert(rows, destination_table, primary_key, **kwargs)

            # Update the counter
            copied_rows += row_count

    self._row_count_verify(source_tbl, destination_tbl)

    logger.info(f'{source_table} synced to {destination_table}.')
```","[Label(name=""enhancement""), Label(name=""parsons core""), Label(name=""medium priority""), Label(name=""needs finishing"")]","[NamedUser(login=""ydamit"")]",0
778,Posting Attendances in Action Network,https://api.github.com/repos/move-coop/parsons/issues/413,,413,open,2020-09-23 18:11:35+00:00,2023-02-21 19:06:50+00:00,,"I successfully extended the ActionNetwork class with the following method in some code, so I thought I'd share it here in case someone else has more capacity to improve it and write the unit tests:
```
def create_attendance(self, event_id, email, given_name=None, family_name=None, postal_address=None,
                      custom_fields=None, source=None):
    """"""
    Sign a person up for an existing event, creating a new person record if email
    is not already in the system
    
    `Args:`
        event_id: str
            The Action Network Event's unique ID. To retrieve easily immediately
            after creating an event, see example:
            .. code-block:: python
            
                event_dict = an.create_event(""My Event Title"")
                event_id = event_dict[""event_id""]
        given_name: str
            OPTIONAL: The first name of event attendee
        family_name: str
            OPTIONAL: The last name of event attendee
        postal_address: dict
            OPTIONAL: A dict of address details. Can include any combination of the types of
            values in the following example:
            .. code-block:: python
            
                my_location = {
                    ""address_lines"": [
                        ""1600 Pennsylvania Ave""
                    ],
                    ""locality"": ""Washington"",
                    ""region"": ""DC"",
                    ""postal_code"": ""20009"",
                    ""country"": ""US""
                }
        custom_fields: dict
            OPTIONAL: A dict of custom field names and values
        source: str
            OPTIONAL: The desired source code for this specific attendance record
    `Returns:`
        Dict of Action Network Attendance data.
    """"""
    
    rsvp = {
        ""person"": {
            ""email_addresses"" : [{""address"" : email}]
        }
    }
    
    if given_name:
        rsvp[""person""][""given_name""] = given_name
    
    if family_name:
        rsvp[""person""][""family_name""] = family_name
    
    if isinstance(postal_address, dict):
        postal_addresses = [postal_address]
    elif isinstance(postal_address, list):
        postal_addresses = postal_address
    else:
        postal_addresses = None
    
    if postal_addresses:
        rsvp[""postal_addresses""] = postal_addresses
        
    if isinstance(custom_fields, dict):
        rsvp[""custom_fields""] = custom_fields
    
    if source:
        rsvp[""action_network:referrer_data""] = {
            ""source"": source
        }
    
    attendance_dict = self.api.post_request(
        url=f""{self.api_url}/events/{event_id}/attendances"", data=json.dumps(rsvp)
    )
    
    an_attendance_id = attendance_dict[""_links""][""self""][""href""].split('/')[-1]
    attendance_dict[""attendance_id""] = an_attendance_id
    
    return attendance_dict
```","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update"")]",[],0
779,Posting Events in ActionNetwork,https://api.github.com/repos/move-coop/parsons/issues/412,,412,closed,2020-09-23 16:26:12+00:00,2022-03-17 21:30:56+00:00,2022-03-17 21:30:56+00:00,"I successfully extended the ActionNetwork class with the following method in some code, so I thought I'd share it here in case someone else has more capacity to improve it and write the unit tests:
```
def create_event(self, title, start_date=None, location=None):
    """"""
    Create an event in Action Network
    
    `Args:`
        title: str
            The public title of the event
        start_date: str OR datetime
            OPTIONAL: The starting date & time. If a string, use format ""YYYY-MM-DD HH:MM:SS""
            (hint: the default format you get when you use `str()` on a datetime)
        location: dict
            OPTIONAL: A dict of location details. Can include any combination of the types of
            values in the following example:
            .. code-block:: python
            
                my_location = {
                    ""venue"": ""White House"",
                    ""address_lines"": [
                        ""1600 Pennsylvania Ave""
                    ],
                    ""locality"": ""Washington"",
                    ""region"": ""DC"",
                    ""postal_code"": ""20009"",
                    ""country"": ""US""
                }
    
    `Returns:`
        Dict of Action Network Event data. 
    """"""
    
    data = {
        ""title"": title
    }
    
    if start_date:
        start_date = str(start_date)
        data[""start_date""] = start_date
        
    if isinstance(location, dict):
        data[""location""] = location
    
    event_dict = self.api.post_request(url=f""{self.api_url}/events"", data=json.dumps(data))

    an_event_id = event_dict[""_links""][""self""][""href""].split('/')[-1]
    event_dict[""event_id""] = an_event_id
    
    return event_dict
```","[Label(name=""low priority""), Label(name=""connector update"")]",[],3
780,Docs: Update AWS,https://api.github.com/repos/move-coop/parsons/issues/411,Randi Griffin,411,closed,2020-09-23 00:21:12+00:00,2020-10-16 18:20:30+00:00,2020-10-16 18:20:30+00:00,"This PR addresses all of the AWS documentation and should check the AWS boxes for #269.

AWS: ALL

- Add global overview with links to different classes and methods at the top of the RST file 

Lambda
- Move description and example code from method docstrings to Overview and Quickstart
- No Authentication Guide because this isn't a typical Parsons class

S3
- Improve Overview and Quickstart

Redshift

- Add `psychopg2` link to Overview
- Consolidate Quickstart

Additional changes in this PR:

- In the `Redshift` section of `databases.rst`, I replaced the link to AWS docs with a more specific reference to the Redshift section of the AWS docs.",[],[],11
781,ThruTalk Connector,https://api.github.com/repos/move-coop/parsons/issues/410,Shauna Gordon-McKeon,410,closed,2020-09-22 19:39:49+00:00,2023-02-21 19:09:17+00:00,2023-02-21 19:09:17+00:00,"This is a bit of a placeholder issue; there's been interest in a ThruTalk connector, but we can't create one until ThruTalk makes an API available.  Folks who'd like this connector in Parsons should [contact TruTalk/GetThru](https://www.thrutalk.io/) about making an API available. Please feel free to leave a comment with what you'd use the connector for as well, so we know what to prioritize once we're unblocked.","[Label(name=""enhancement""), Label(name=""blocked""), Label(name=""new connector"")]",[],1
782,Impactive Connector,https://api.github.com/repos/move-coop/parsons/issues/409,Shauna Gordon-McKeon,409,closed,2020-09-22 19:37:43+00:00,2023-02-21 19:10:18+00:00,2023-02-21 19:10:17+00:00,"This is a bit of a placeholder issue; there's been repeated interest in an OutVote connector, but we can't create one until OutVote makes an API available.  Folks who'd like this connector in Parsons should [contact OutVote](https://www.outvote.io/) about making an API available. Please feel free to leave a comment with what you'd use the connector for as well, so we know what to prioritize once we're unblocked.","[Label(name=""enhancement""), Label(name=""blocked""), Label(name=""new connector"")]",[],1
783,Update hustle.py,https://api.github.com/repos/move-coop/parsons/issues/408,elyse-weiss,408,closed,2020-09-22 14:42:39+00:00,2020-09-22 17:55:51+00:00,2020-09-22 17:55:47+00:00,,[],[],0
784,Docs: Update Civis,https://api.github.com/repos/move-coop/parsons/issues/407,Randi Griffin,407,closed,2020-09-20 01:29:53+00:00,2020-09-25 19:20:38+00:00,2020-09-23 12:41:18+00:00,"This should check the boxes for Civis in #269:

- Add Authentication Guide, Overview, and Quickstart

Additional changes:

- Remove some unused function arguments
- Align order of args in the function definition, docstring, and usage in the init ",[],[],0
785,Docs: Update CrowdTangle,https://api.github.com/repos/move-coop/parsons/issues/406,Randi Griffin,406,closed,2020-09-19 23:49:55+00:00,2020-09-22 17:05:55+00:00,2020-09-21 14:44:35+00:00,"This should check the boxes for CrowdTangle in #269:

- Add Authentication Guide, Overview, and Quickstart

Additional changes to the class:

- Clean up docstrings, add links to API documentation
- Add info about rate limits in Overview and docstrings
- Use underscore convention to name private methods",[],[],0
786,Docs: Update Freshdesk,https://api.github.com/repos/move-coop/parsons/issues/405,Randi Griffin,405,closed,2020-09-19 18:42:13+00:00,2020-09-22 17:05:55+00:00,2020-09-21 14:41:00+00:00,"This should check the boxes for Freshdesk in #269:

- Add Authentication Guide, Overview, and Quickstart

Additional changes to the class:

- Clean up docstrings, adding links to API documentation
- Use underscore convention to name private methods",[],[],0
787,Docs: Update Redash,https://api.github.com/repos/move-coop/parsons/issues/404,Randi Griffin,404,closed,2020-09-19 12:30:55+00:00,2020-09-22 17:05:55+00:00,2020-09-21 14:42:41+00:00,"- Add Authentication Guide, Overview, and Quickstart
- Moved some text from docstrings to Quickstart
- Should check the boxes for Redash in #269",[],[],0
788,redshift: alter table on upsert optional,https://api.github.com/repos/move-coop/parsons/issues/403,Eliot Stone,403,closed,2020-09-17 23:35:34+00:00,2020-09-21 12:26:39+00:00,2020-09-21 12:26:36+00:00,"This commit updates the Redshift `upsert` function to make
altering the target table optional. Before this change, it was
not possible to leverage the `truncatecolumns` copy argument to
handle oversided varchar text instead of altering the table.",[],[],0
789,redshift: move distkey/sortkey warning,https://api.github.com/repos/move-coop/parsons/issues/402,Eliot Stone,402,closed,2020-09-17 20:58:34+00:00,2020-09-17 23:35:16+00:00,2020-09-17 23:35:13+00:00,"This commit moves the warning for missing distkey / sortkeys when
copying data into Redshift to only fire when a new table is being
created, instead of whenever `copy` (or `upsert`) is called.

cc @tiburona ",[],[],0
790,RS Upsert Vacuum Error,https://api.github.com/repos/move-coop/parsons/issues/401,,401,closed,2020-09-17 14:31:21+00:00,2022-03-17 21:29:49+00:00,2022-03-17 21:29:49+00:00,"On a recent Civis script run hit an error of `psycopg2.errors.FeatureNotSupported: VACUUM is running`. 

The code behind this script uses `rs.upsert()` at semi-regular intervals, but I guess there is a limit to how many vacuums there can be going on at a time? If that's the case, maybe there's a way to queue the vacuum command, or at least a way to catch this exception and try again after a delay (which I might do in my specific script in a future version as well)?","[Label(name=""enhancement""), Label(name=""connector update"")]",[],1
791,TargetSmart HotFix,https://api.github.com/repos/move-coop/parsons/issues/400,,400,closed,2020-09-17 13:08:23+00:00,2020-09-17 17:27:55+00:00,2020-09-17 13:09:52+00:00,Makes the arguments in automation kwargs.,[],[],0
792,Docs: Update Salesforce,https://api.github.com/repos/move-coop/parsons/issues/399,Randi Griffin,399,closed,2020-09-17 01:46:23+00:00,2020-09-22 17:05:54+00:00,2020-09-17 14:59:10+00:00,"- add Authentication Guide, improve Overview and Quickstart, fix typos
- Should check the boxes for Salesforce in #269",[],[],0
793,Docs: Update SFTP,https://api.github.com/repos/move-coop/parsons/issues/398,Randi Griffin,398,closed,2020-09-15 22:25:39+00:00,2020-10-06 11:41:17+00:00,2020-10-05 21:00:50+00:00,"- Add Authentication Guide and Quickstart, improve Overview
- Move example code from the `create_connection` docstring to the Quickstart
- Should check the boxes for SFTP in #269",[],[],0
794,Database: Add tests for other data types for the detect_data_type method,https://api.github.com/repos/move-coop/parsons/issues/397,Daniel,397,open,2020-09-15 16:42:17+00:00,2023-02-21 19:11:38+00:00,,"The `Database.detect_data_type` (also `Redshift.data_type`, `Postgres.data_type`, `MySQL.data_type`) method checks for the best sql type for specific data. Currently, it only returns types for ints, floats, and varchar. It would be great to test what would result from passing in other data types, e.g. py `list`, 'dict`, etc.

Additionally, we should determine what data types currently make it down to [here](https://github.com/dannyboy15/parsons/blob/database-refactor/parsons/databases/database/database.py#L179-L180)","[Label(name=""enhancement""), Label(name=""parsons core""), Label(name=""low priority""), Label(name=""testing"")]",[],0
795,MySQL: check reserved words for use column names.,https://api.github.com/repos/move-coop/parsons/issues/396,Daniel,396,closed,2020-09-15 16:34:48+00:00,2022-03-17 21:10:54+00:00,2022-03-17 21:10:54+00:00,"In the process of creating a table, several checks are made to ensure the column names conform to sql's constraints. In the `Redshift` and `Postgres` classes one test checks the column name isn't in a list of reserved words. Currently the `MySQL` method is missing that check.","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""connector update"")]",[],0
796,Docs: Update TargetSmart,https://api.github.com/repos/move-coop/parsons/issues/395,Randi Griffin,395,closed,2020-09-15 12:54:09+00:00,2020-09-16 23:04:51+00:00,2020-09-16 19:40:53+00:00,"- Add Authentication Guide and Quickstart and improve the Overview for the `TargetSmartConnector` and `TargetSmartAutomation` classes
- Should check the boxes for TargetSmart in #269

Additional refactors:

- Use `check_env` to simplify handling of authentication parameters in both classes
- Replace `ts` with `targetsmart` in the names of test and documentation files/folders in order to align them with the connector folder name (`parsons/targetsmart`) and to be consistent with the naming pattern for other connectors.
- Remove some unused function arguments",[],[],0
797,Docs: Update Mobilize America,https://api.github.com/repos/move-coop/parsons/issues/394,Randi Griffin,394,closed,2020-09-15 07:23:42+00:00,2020-09-15 23:26:02+00:00,2020-09-15 12:03:31+00:00,"- Add Quickstart, improve Overview and Authentication Guide 
- Should check the boxes for Mobilize America in #269

Also, I made some minor edits to the class:
- Add underscore prefix to the private class methods `request` and `request_paginate`, so they are now `_request` and `_request_paginate`)
- Remove some unused function arguments
",[],[],0
798,Docs: Update Facebook Ads,https://api.github.com/repos/move-coop/parsons/issues/393,Randi Griffin,393,closed,2020-09-15 00:44:39+00:00,2020-09-15 23:26:02+00:00,2020-09-15 12:00:38+00:00,"- Add Quick Start, add Authentication Guide, improve Overview
- Move some material from the class docstring to the RST Overview
- Should check the boxes for Facebook Ads in #269",[],[],0
799,Docs: Update Azure Blob Storage,https://api.github.com/repos/move-coop/parsons/issues/392,Randi Griffin,392,closed,2020-09-13 11:22:03+00:00,2020-09-15 23:26:01+00:00,2020-09-14 14:30:04+00:00,"- add Authentication Guide, improve Overview, and improve Quick Start 
- should check the boxes for Azure: Blob Storage in #269

**Question about the class:** 

@eliotst 

I believe the `account_domain` argument / env variable is unnecessary in `AzureBlogStorage` since it will always be `blob.core.windows.net` (this is why I don't mention it in the Authentication Guide or Quickstart). The argument currently defaults to  `blob.core.windows.net` and it doesn't cause any issues AFAIK; it just looks like clutter in the `__init__`. 

I haven't changed the class yet since I'm not 100% sure there isn't a good reason for keeping the argument, but if someone can confirm that `account_domain` should be a constant in the class, I'm happy to make the change in this PR. ",[],[],1
800,refactor(Database): Create a class that database classes can inherit from.,https://api.github.com/repos/move-coop/parsons/issues/391,Daniel,391,closed,2020-09-13 08:03:32+00:00,2021-03-29 19:09:31+00:00,2021-03-04 15:45:16+00:00,"The goal here is to make the code easier to maintain and update. (For example, multiple classes had to be changed in the same way to make this change #332). This is done by creating a class that database classes can inherit from. This PR only addresses the create table methods, but can be implemented for other common methods.

This PR creates a class, `DatabaseCreateStatement`. The class is very flexible and extensible through the use of constants and override-able methods. In addition, the methods have been renamed with more descriptive names and have been reimplemented for greater efficiency and robustness. Doc-strings were added to all public methods for better introspection. 

Addresses issues #341 ",[],[],6
801,Docs: Update Twilio,https://api.github.com/repos/move-coop/parsons/issues/390,Randi Griffin,390,closed,2020-09-12 18:33:33+00:00,2020-09-15 23:26:01+00:00,2020-09-12 19:28:17+00:00,"- add Authentication Guide, improve Overview, and improve Quick Start 
- should check the boxes for Twilio in #269

I also updated the method `table_convert` to `_table_convert`, following the convention for private methods.",[],[],0
802,Docs: Update Braintree,https://api.github.com/repos/move-coop/parsons/issues/389,Randi Griffin,389,closed,2020-09-12 17:54:35+00:00,2020-09-15 23:26:01+00:00,2020-09-12 19:27:20+00:00,"- add Authentication Guide and improve Quick Start 
- should check the boxes for Braintree in #269",[],[],0
803,VAN: Bulk Upsert Contact,https://api.github.com/repos/move-coop/parsons/issues/388,,388,closed,2020-09-12 16:48:21+00:00,2021-11-01 20:33:38+00:00,2021-11-01 20:33:31+00:00,"There are a few more follow up PRs that I'd like to do, but I think that this is enough for now.

- Adds some fuzziness to the `map_columns()` utility that ignores spaces, underscores and capitalization.
- Add `VAN.get_bulk_import_results()` method.
- Add `VAN.get_bulk_import_mapping_type_fields()` method.
- Add `VAN.bulk_upsert_concacts()` method.
- Adds an unused at the moment method to automatically discover mapping types. This is not being used at the moment, but I hope to in a future PR.",[],[],0
804,Docs: Update Hustle,https://api.github.com/repos/move-coop/parsons/issues/387,Randi Griffin,387,closed,2020-09-12 13:06:56+00:00,2020-09-15 23:26:01+00:00,2020-09-12 13:38:14+00:00,"- add Authentication Guide, minor formatting edits
- should check the boxes for Hustle in #269 ",[],[],0
805,Redshift: Unload Bug Second Attempt,https://api.github.com/repos/move-coop/parsons/issues/386,,386,closed,2020-09-11 20:06:55+00:00,2020-09-12 19:28:46+00:00,2020-09-12 19:28:41+00:00,,[],[],0
806,Redshift: Fix Unload Bug,https://api.github.com/repos/move-coop/parsons/issues/385,,385,closed,2020-09-11 18:24:32+00:00,2020-09-11 18:38:58+00:00,2020-09-11 18:38:58+00:00,Adding a line break to the SQL statement.,[],[],0
807,Docs: Table Update,https://api.github.com/repos/move-coop/parsons/issues/384,,384,closed,2020-09-11 16:46:44+00:00,2020-09-17 17:27:52+00:00,2020-09-11 20:12:58+00:00,"- Added links to methods from the tables at the top of the documentation.
- Added in tables to quickly scan key transformations.
- Cleaned up a few doc strings in the methods.
- Cleaned up a little bit of documentation.

#382 ",[],[],1
808,Docs: Update Mailchimp,https://api.github.com/repos/move-coop/parsons/issues/383,Randi Griffin,383,closed,2020-09-11 13:39:42+00:00,2020-09-15 23:26:00+00:00,2020-09-11 15:39:34+00:00,"- add Authentication Guide, improve Quickstart 
- should check the boxes for Mailchimp in #269",[],[],0
809,Better documentation for Parsons Table,https://api.github.com/repos/move-coop/parsons/issues/382,Corey Haines,382,closed,2020-09-10 18:18:45+00:00,2020-09-15 00:07:04+00:00,2020-09-13 17:56:39+00:00,"While working on some scripts using the `SurveyGizmo` connector, we found that it was difficult to find what functionality was available on `Table`.

Converting to/from a `Table` is nicely discoverable

![image](https://user-images.githubusercontent.com/3962/92779436-ac880d80-f367-11ea-9888-f35ce050bc9b.png)

But if we are looking for functions on how to manipulate them, then there isn't an equivalent list like this, so you have to scan through a list, and it is hard to scan. For example, using the ETL functionality 

![image](https://user-images.githubusercontent.com/3962/92779911-16a0b280-f368-11ea-9ef4-74ce8a282a80.png)

Would be awesome to have a scannable list for the different objects, similar to the summary lists for to_/from_ functions.",[],[],3
810,Adding line to capitalize Parsons to coding conventions #286,https://api.github.com/repos/move-coop/parsons/issues/381,Ehren,381,closed,2020-09-09 18:14:37+00:00,2020-09-09 23:18:20+00:00,2020-09-09 23:18:20+00:00,Deleted a previous fork and PR due to inability to squash commits. Created a clean fork for this single commit. cc @shaunagm ,[],[],1
811,Docs: Update Copper,https://api.github.com/repos/move-coop/parsons/issues/380,,380,closed,2020-09-09 18:08:41+00:00,2020-09-14 02:34:54+00:00,2020-09-14 02:34:50+00:00,Issue #269 ,[],[],0
812,Docs: Update Bill.com,https://api.github.com/repos/move-coop/parsons/issues/379,Randi Griffin,379,closed,2020-09-09 06:53:46+00:00,2020-09-10 20:02:14+00:00,2020-09-09 14:48:21+00:00,"- add Authentication Guide, improve Overview, improve Quickstart
- should check the boxes for Bill.com in #269",[],[],1
813,Docs: Update GitHub,https://api.github.com/repos/move-coop/parsons/issues/378,Randi Griffin,378,closed,2020-09-09 06:22:01+00:00,2020-09-10 16:25:23+00:00,2020-09-10 14:53:07+00:00,"- Improve Overview and Quickstart
- should check the boxes for GitHub in #269",[],[],0
814,Docs: Update PDI,https://api.github.com/repos/move-coop/parsons/issues/377,Randi Griffin,377,closed,2020-09-09 05:29:20+00:00,2020-09-09 19:03:35+00:00,2020-09-09 13:58:09+00:00,"- add Authentication Guide, improve Overview, improve Quickstart
- should check the boxes for PDI in #269",[],[],0
815,Docs: Update TurboVote,https://api.github.com/repos/move-coop/parsons/issues/376,,376,closed,2020-09-09 03:03:42+00:00,2020-09-14 02:35:17+00:00,2020-09-14 02:35:13+00:00,"Added authentication and quickstart. Also did some minor cleaning of the logging to adhere to more up to date norms.

#269 ",[],[],0
816,Docs: Update Newmode,https://api.github.com/repos/move-coop/parsons/issues/375,Randi Griffin,375,closed,2020-09-09 02:18:11+00:00,2020-09-09 19:03:34+00:00,2020-09-09 03:40:21+00:00,"This PR adds an Authentication Guide, improves the Overview, and improves the Quick Start for the `Newmode` docs. This should check the boxes for `Newmode` in #269",[],[],0
817,Docs: Update Zoom,https://api.github.com/repos/move-coop/parsons/issues/374,Randi Griffin,374,closed,2020-09-09 01:20:19+00:00,2020-09-09 19:03:34+00:00,2020-09-09 03:39:58+00:00,"This PR adds an Authentication Guide and improves the Overview and Quick Start for the Zoom connector, checking the boxes for Zoom in #269. 

In addition, I made the following changes after reviewing the docstrings and functions in the Zoom class:

- Corrected the return value description in the docstring for `get_past_meeting` from `dict` to `Table`
- Prepended an underscore to the private `get_request` method, following the convention used in most other connectors ",[],[],0
818,Redshift: Add dist/sort keys to upsert.,https://api.github.com/repos/move-coop/parsons/issues/373,,373,closed,2020-09-09 00:54:01+00:00,2021-10-29 20:43:41+00:00,2021-10-29 20:43:41+00:00,"This PR does the following:

- Adds arguments that allow the user to set the dist/sort keys for an upsert. If the table does not previously exist, this will set the dist/sort keys. If the table exists, it will set the dist/sort keys for the staging table to the provided arguments.

- If the dist/sort key areguments are `None`, then it will set the primary_key as the dist/sort keys for the staging table.

The benefit of all of this should be that upserts should run faster whether the argument is passed or not, since the staging table will have a dist/sort key which will most likely be the same as the destination table.",[],[],1
819,Redshift:  update statupdate and compupdate copy options,https://api.github.com/repos/move-coop/parsons/issues/372,Daniel,372,closed,2020-09-08 23:45:04+00:00,2021-06-30 02:01:25+00:00,2021-06-28 15:09:21+00:00,This would change the default behavior of copying tables into Redshift. It would align more with the defaults that Redshift uses.,[],[],5
820,Redshift: Add tag to temp file used to copy into Redshift,https://api.github.com/repos/move-coop/parsons/issues/371,Daniel,371,open,2020-09-08 19:32:48+00:00,2023-02-21 19:11:54+00:00,,"When `Reshift.copy` is called, the method creates and uploads a temporary file to S3. The name of that temp file is a [hash of the current time](https://github.com/move-coop/parsons/blob/master/parsons/databases/redshift/rs_copy_table.py#L125). The filename shows up in s3 while it's being copied, and in Redshift copy logs.

It would be nice to be able to add a 'tag' to the file name to help with debugging. For example, the file name would show up as, `s3://bucket/Parsons_RedshiftCopyTable/94703686879009732_<tag>.csv.gz`.","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""low priority""), Label(name=""connector update"")]",[],0
821,Fix lint errors and update CircleCi to test entire project,https://api.github.com/repos/move-coop/parsons/issues/370,Daniel,370,closed,2020-09-08 04:04:07+00:00,2021-03-29 19:09:20+00:00,2020-09-08 16:19:42+00:00,Addresses issue #340 ,[],[],0
822,fix actionnetwork empty list default value bug,https://api.github.com/repos/move-coop/parsons/issues/369,Randi Griffin,369,closed,2020-09-07 16:18:39+00:00,2020-09-21 17:29:13+00:00,2020-09-20 17:42:39+00:00,"This is an attempt to close issue #281. I followed the suggestion to filter for supplied arguments when creating the data dictionary. Not sure if there is a cleaner way to do this. 

Working on this PR raised several questions for me:

1. Is there an acceptable default value that can be used in place of the empty lists, thereby avoiding all the ugly if statements? 

2. The Action Network API docs say that only `email_address` is required to POST a new person, so why does the `ActionNetwork` class have arguments for these particular non-required fields rather than having all the optional fields passed in as `**kwargs`? https://actionnetwork.org/docs/v2/post-people/

3. Unrelated to the specific bug in this PR, I noticed that the ActionNetwork Quick Start example in the RST fails (line 29 in action_network.rst) because it does not specify an 'api_url' argument. My question is, why does the class require a user-provided 'api_url'? Won't this argument always be 'https://actionnetwork.org/api/v2'? Something needs to be changed, but I'm not sure whether it is the Quick Start example or the class itself. ",[],"[NamedUser(login=""ydamit"")]",6
823,Issue 248 regex sftp helper,https://api.github.com/repos/move-coop/parsons/issues/368,Katie Surrence,368,closed,2020-09-07 14:41:02+00:00,2020-12-21 13:45:40+00:00,2020-12-21 13:45:40+00:00,"I am opening this PR as a draft to solicit comments before I fuss about writing tests.  
I have written created four new public methods on SFTP:

- `list_files`
- `list_subdirectories`
- `get_files`
- `walk_tree`

All of these methods take a regex pattern to match file name or directory path as appropriate, and `walk_tree` takes both.  These regex patterns can always be None, in which case everything is retrieved.  

`walk_tree` returns a tuple, a list of directories traversed and a list of files touched.  `walk_tree` also takes a max_depth parameter which defaults to 2, i.e., search the provided directory and its subdirectories.  

`get_files` can take a list of files as well as a remote path or a list of remote paths, so the user could use some of these other methods to generate a file list and then feed it to get_files.  Note: I haven't really tested `get_files `yet; it's probably broken.

All the public methods have doc strings that spell this out in a bit more detail.

I also wrote a @connect decorator that appears to work and hides this kind of logic under the covers.
```
if connection:
     return connection.do_something()
else:
     with self.create_connection() as connection:
         return connection.do_something()

```
It was getting kind of cumbersome given that I had methods that called other methods and they all used connection. If people don't like it I can get rid of it, but if people do like it I could apply it to other methods in the module.  

Anyway, I just wanted to check in about the new functionality of the module before I got really detail oriented about testing.  If people wanted these methods to do different things, or return things in different formats, or wanted something eliminated or added, it would be better to make those changes before I polish.  ",[],[],7
824,add api token info to notification docs,https://api.github.com/repos/move-coop/parsons/issues/367,Randi Griffin,367,closed,2020-09-07 12:59:21+00:00,2020-09-09 14:23:11+00:00,2020-09-07 16:25:11+00:00,"This update to `notifications.rst` should close #284 

After following the steps to obtain an API Token, I successfully tested the `channels`, `users`, `message_channel`, and `upload_file` methods in the `Slack` class.





 ",[],[],0
825,RockTheVote: add support for extended report,https://api.github.com/repos/move-coop/parsons/issues/366,Daniel,366,closed,2020-09-04 23:53:33+00:00,2020-10-02 15:12:25+00:00,2020-09-08 13:12:59+00:00,"This adds the option to request the extended report which add a few extra fields. In addition, it adds support for the testing url.

h/t @natashamathur",[],[],1
826,New script merges SurveyGizmo columns,https://api.github.com/repos/move-coop/parsons/issues/365,Shauna Gordon-McKeon,365,closed,2020-09-04 20:29:59+00:00,2022-03-17 17:47:04+00:00,2022-03-17 17:47:04+00:00,"Here's the code for the sample script we worked on today.  It's not quite ready to add to the template library, because we haven't added the connectors (and are waiting on the SurveyGizmo connector to be merged to master), but once it is, we can reformat it to match the sample script template.  For now, this works just fine importing and exporting to CSV.

```
from parsons import Table as Table

# Configuration
grouped_column_list = [""gender"", ""hub_teams""]
delete_merged_columns = True
from_csv = 'data.csv'
to_csv = 'transformed_data.csv'

# Step 1: get data from SurveyGizmo (for now, load from CSV)
responses = Table.from_csv(from_csv)
columns = responses.columns

# Step 2: transform data
new_responses = []

for row in responses:

    for grouped_column in grouped_column_list:
        
        columns_to_merge = [column for column in columns if column.endswith("":"" + grouped_column)]

        new_column_value = []

        for column in columns_to_merge:

            if row[column]:
                new_column_value.append(row[column])

            if delete_merged_columns:
                del(row[column])

        new_column_value = "", "".join(new_column_value)

        row[grouped_column] = new_column_value

    new_responses.append(row)

# Step 3: send data to Google Sheets (for now, save to CSV)
new_responses = Table(new_responses)
new_responses.to_csv(to_csv)
```

","[Label(name=""medium priority"")]",[],2
827,Convenience Function to get Distkey & Sortkey,https://api.github.com/repos/move-coop/parsons/issues/364,elyse-weiss,364,open,2020-09-04 14:28:34+00:00,2023-02-21 19:54:49+00:00,,"Given the new warnings in the Redshift class, I would find it super useful to have a convenience function that gets you distkey and sortkey. ","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update""), Label(name=""requires access"")]",[],4
828,Build Empower Class,https://api.github.com/repos/move-coop/parsons/issues/363,Viswa Challa,363,open,2020-09-02 20:53:28+00:00,2023-04-27 20:55:34+00:00,,"Empower data has an API for their data. 

They are only implementing a single get request that will send back all of their data (not sure why they didn't break this up) in a big JSON.

API documentation: https://github.com/getempower/api-documentation","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""new connector"")]",[],3
829,DBSync - need to be schema owner,https://api.github.com/repos/move-coop/parsons/issues/362,elyse-weiss,362,closed,2020-09-02 16:34:31+00:00,2022-03-17 17:47:54+00:00,2022-03-17 17:47:54+00:00,"Add documentation about user needing to be a superuser or schema owner.

Also, it appears you can sync views, so we should probably name that.","[Label(name=""documentation"")]","[NamedUser(login=""elyse-weiss"")]",0
830,Documentation on cross region data in copy/unload/copy_s3/etc and dbsync,https://api.github.com/repos/move-coop/parsons/issues/361,elyse-weiss,361,closed,2020-09-02 16:30:14+00:00,2023-02-21 19:14:21+00:00,2023-02-21 19:14:20+00:00,"Ran into issues and @eliotst has a PR open, but we'll need to add more documentation.

CC: @tonywhittaker ","[Label(name=""documentation""), Label(name=""enhancement""), Label(name=""needs more info"")]",[],1
831,redshift: add region parameter to copy from S3,https://api.github.com/repos/move-coop/parsons/issues/360,Eliot Stone,360,closed,2020-09-01 17:17:59+00:00,2020-09-09 18:26:19+00:00,2020-09-09 18:26:16+00:00,"This commit adds the `aws_region` parameter to the `copy_s3`
method in the `Redshift` connector class to support loading keys
from an S3 bucket outside the current Redshift cluster's region.
The `aws_region` populates the `region` parameter in the `copy`
statement used to load the S3 data.

cc @elyse-weiss 

Fixes #356 

---
I don't have a good set up to test this. @elyse-weiss would you be able to test this locally for your use case?",[],[],1
832,Redshift: get_columns_list(),https://api.github.com/repos/move-coop/parsons/issues/359,elyse-weiss,359,closed,2020-09-01 13:15:56+00:00,2020-09-01 17:21:36+00:00,2020-09-01 17:21:33+00:00,,[],[],0
833,Speed up rs.get_columns(),https://api.github.com/repos/move-coop/parsons/issues/358,elyse-weiss,358,closed,2020-09-01 13:11:14+00:00,2020-09-01 17:21:33+00:00,2020-09-01 17:21:33+00:00,a `select * from {table} limit 1` is much faster than what we are currently doing - so leave available both options.,"[Label(name=""enhancement"")]","[NamedUser(login=""elyse-weiss"")]",0
834,Issue 195 dist/sort key nudge,https://api.github.com/repos/move-coop/parsons/issues/357,Katie Surrence,357,closed,2020-09-01 00:32:06+00:00,2020-09-02 01:52:51+00:00,2020-09-02 01:51:24+00:00,,[],[],2
835,Specify S3 Region in DBSync,https://api.github.com/repos/move-coop/parsons/issues/356,elyse-weiss,356,closed,2020-08-31 20:21:55+00:00,2020-09-09 18:26:16+00:00,2020-09-09 18:26:16+00:00,"We need a way to specify the s3 region of the destination bucket in DBSync class.

Received this error: 
![image](https://user-images.githubusercontent.com/29580051/91764821-f02f8a00-eba5-11ea-8e4d-ed84ebef0d88.png)

Per conversation in Slack:
In the parsons docs it looks like `parsons.Redshift.unload` supports aws_region in the arguments, but I don’t see a corresponding argument in `parsons.Redshift.copy` 
","[Label(name=""bug""), Label(name=""high priority"")]","[NamedUser(login=""jburchard""), NamedUser(login=""eliotst"")]",0
836,Deduplicate records in staging on upsert,https://api.github.com/repos/move-coop/parsons/issues/355,Eliot Stone,355,open,2020-08-28 16:18:36+00:00,2023-02-21 19:14:51+00:00,,"Right now, the `upsert` method on the `Redshift` connector will deduplicate between records in your Parsons table and the records already in the target table in Redshift. What it does not do is deduplicate the data in the Parsons table being upserted. So, if there are multiple records with the same value for the primary key(s) being used to upsert, it is still possible to end up with duplicates in the final Redshift table. The example below illustrates:

```
from parsons import Redshift, Table
rs = Redshift()
tbl = Table([{'key': 1, 'value': 'one'}, {'key': 2, 'value': 'two'}])
rs.upsert(tbl, 'test_upsert', primary_key='key')
# We have two records with a ""key"" of 1 in this table
tbl2 = Table([{'key': 1, 'value': 'one'}, {'key': 2, 'value': 'two'}, {'key': 1, 'value': 'three'}])
rs.upsert(tbl2, 'test_upsert', primary_key='key')
result = rs.query('select key from test_table')
print(result['key'])
```

The `print` statement above will print `[1, 2, 1]` because the second `upsert` call didn't deduplicate within `tbl2` before inserting the records into Redshift.

What I would want to have happen is that the second `upsert` deduplicates within the table, and the `print` statement prints `[1, 2]`.","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update"")]",[],0
837,SurveyGizmo connector,https://api.github.com/repos/move-coop/parsons/issues/354,Corey Haines,354,closed,2020-08-27 19:42:13+00:00,2020-09-26 02:39:36+00:00,2020-09-25 23:20:48+00:00,"Initial creation of SurveyGizmo connector based on the code in the issue https://github.com/move-coop/parsons/issues/297

[gist](https://gist.github.com/elyse-weiss/101d251eeed2e4f2fb25e8754ca493bd)


Have done manual testing
```python
>>> sg = SurveyGizmo(api_token=api_token, api_token_secret=api_token_secret)
>>> sg.get_surveys()
surveygizmo INFO Found 1 surveys.
{'id': '5820711', 'team': '1018675', 'type': 'Standard Survey', 'status': 'Launched', 'created_on': '2020-09-02 16:45:03', 'modified_on': '2020-09-02 16:46:46', 'title': 'Test Gizmo Connector'}
```

I would like to punt the automated microtests to a follow-up PR, in order to get this into availability for folks.



@elyse-weiss 
There are 3 methods in the provided gist that are not directly API wrappers, instead appearing to be helpers that were written for a specific case. Not sure whether we want to move these over. Or perhaps into a `SurveyGizmo.Helpers` or something.

- _strip_html
- parse_responses
- parse_respondents",[],[],13
838,Docs: Airtable,https://api.github.com/repos/move-coop/parsons/issues/353,,353,closed,2020-08-25 17:54:35+00:00,2020-09-02 19:39:53+00:00,2020-08-26 14:57:30+00:00,"Add a quickstart and update the name of the client to be `client`.

Issue #269.",[],[],1
839,[WIP] VAN Bulk Create / Update Contact,https://api.github.com/repos/move-coop/parsons/issues/352,,352,closed,2020-08-25 15:43:19+00:00,2020-09-07 17:22:27+00:00,2020-09-07 17:22:27+00:00,Initial PR to extend functionality to create and update contacts.,[],"[NamedUser(login=""jburchard"")]",0
840,phone2action: add update_advocate / create_advocate,https://api.github.com/repos/move-coop/parsons/issues/351,Eliot Stone,351,closed,2020-08-18 15:54:21+00:00,2020-08-24 18:38:44+00:00,2020-08-24 18:38:37+00:00,"This commit adds the `update_advocate` and `create_advocate`
methods to the `Phone2Action` Connector class.

This commit also updates the `APIConnector` class to allow users
to only have to pass in the relative path to an endpoint on
request calls to simplify the API. It also updates any classes
that were passing in the full URL to only pass in the relative
path. *Note:* It is still possible to pass in a full URL; the
full URL will override the base URL in the connector.

Finally, this commit tweaks the existing date utilities by
a) adding a new `parse_date` function for parsing semi-arbitrary
date values, and b) to rework the `iso_to_unix` function to the
`date_to_timestamp` function that can take any date-like value
and convert it into a UNIX timestamp.

---
Fixes #327 

I need to do some integration testing before merging, but wanted to get the PR up for review.

cc @abarrett9 ",[],[],0
841,Documentation: Cloud Storage Utility,https://api.github.com/repos/move-coop/parsons/issues/350,,350,closed,2020-08-15 16:21:09+00:00,2020-08-16 21:47:08+00:00,2020-08-16 14:10:46+00:00,Addresses #347.,"[Label(name=""documentation"")]",[],0
842,VAN: Bulk Upload Initial Methods,https://api.github.com/repos/move-coop/parsons/issues/349,,349,closed,2020-08-15 15:08:23+00:00,2020-09-02 19:39:54+00:00,2020-08-25 14:33:43+00:00,"This PR contains the initial commits for the Bulk Upload functionality. The Bulk Import endpoint is pretty powerful and allow you to update contacts and contributions. For this initial functionality, I focused on a more basic type, activist codes.

**Public Methods**
- `get_bulk_import_job()`
- `get_bulk_import_mapping_types()`
- `get_bulk_import_mapping_type()`
- `bulk_apply_activist_codes()`

For future PRs:

- `bulk_create_update_person` - The ability to update contact data on a person level.
- Better validation prior to sending the request (checking that required columns are in the table.
- Convenience method to download the results of a bulk import job as a Parsons Table.

Additional Needs:
- Better documentation and additional options for cloud storage. Outlined in Issue #347.
","[Label(name=""enhancement"")]",[],0
843,VAN: Bulk Upload Endpoints MVP,https://api.github.com/repos/move-coop/parsons/issues/348,,348,closed,2020-08-14 17:05:51+00:00,2020-08-25 21:24:22+00:00,2020-08-25 21:24:22+00:00,"Build out methods against the NGPVAN bulk import [endpoints](https://developers.ngpvan.com/van-api#bulk-import).

**Initial Methods**
- [x] `get_bulk_import_job()`
- [x] `get_bulk_import_resources()`
- [x] `get_bulk_import_mapping_types()`
- [x] `get_bulk_import_mapping_type()`
- [x] `get_bulk_import_field_values()`
- [x] `bulk_apply_activist_codes()`","[Label(name=""enhancement""), Label(name=""high priority"")]","[NamedUser(login=""jburchard"")]",0
844,Add Cloud Storage Documentation,https://api.github.com/repos/move-coop/parsons/issues/347,,347,closed,2020-08-14 14:45:22+00:00,2020-08-27 19:43:34+00:00,2020-08-27 19:43:34+00:00,"We've got a hidden utility called [cloud storage](https://github.com/move-coop/parsons/blob/master/parsons/utilities/cloud_storage.py) that tries to solve for the problem of POST methods that require access to a file on the internet.

This is currently being utilized for [VAN.upload_scores](https://move-coop.github.io/parsons/html/ngpvan.html#parsons.ngpvan.van.Scores.upload_scores) but will also be needed for VAN Bulk Upload endpoints. 

If this is used more extensively, it could benefit from some documentation.","[Label(name=""documentation""), Label(name=""medium priority"")]","[NamedUser(login=""jburchard"")]",0
845,correct sftp create_connection doc and is_csv_path fix,https://api.github.com/repos/move-coop/parsons/issues/346,Schuyler Duveen,346,closed,2020-08-13 18:17:50+00:00,2020-08-13 18:40:00+00:00,2020-08-13 18:40:00+00:00,,[],[],0
846,bump version to v0.15.0,https://api.github.com/repos/move-coop/parsons/issues/345,Eliot Stone,345,closed,2020-08-13 16:04:41+00:00,2020-08-13 20:25:45+00:00,2020-08-13 20:25:41+00:00,,[],[],0
847,docs: update quickstart,https://api.github.com/repos/move-coop/parsons/issues/344,Eliot Stone,344,closed,2020-08-11 16:02:04+00:00,2020-08-12 17:02:03+00:00,2020-08-12 17:01:59+00:00,"This commit updates the Quick Start code in the README.md file
for Parsons so that the quick start can be executed without
credentials. The goal is to make it so that anyone could run
the Quick Start code to begin understanding how to use Parsons.

This commit also adds a `download_table` method to the `GitHub`
Connector that allows downloading CSV data from GitHub as a
Parsons `Table`.

Finally, this commit tweaks the `download_file` method on the
`GitHub` connector to support token-based authentication when
downloading files / tables.",[],[],0
848,Redshift populate_table_from_query silent error,https://api.github.com/repos/move-coop/parsons/issues/343,elyse-weiss,343,closed,2020-08-10 23:41:17+00:00,2022-03-17 21:13:05+00:00,2022-03-17 21:13:05+00:00,"I am using rs.populate_table_from_query() with an append. We kept not seeing data get appended, and after some investigation in underlying Redshift tables, it looks like it hit this error:

psycopg2.errors.InternalError_: could not complete because of conflict with concurrent transaction

Ideally, the call would error and the error surfaced.

CC: @ydamit @eliotst @jburchard","[Label(name=""bug""), Label(name=""high priority"")]",[],11
849,Docs: Update ActionNetwork.,https://api.github.com/repos/move-coop/parsons/issues/342,,342,closed,2020-08-10 19:34:34+00:00,2020-08-16 21:47:09+00:00,2020-08-11 17:50:48+00:00,"Improves overview and adds authentication.

#269 ",[],[],0
850,Combine common database methods,https://api.github.com/repos/move-coop/parsons/issues/341,Daniel,341,closed,2020-08-10 07:08:54+00:00,2022-03-17 21:09:29+00:00,2022-03-17 21:09:29+00:00,"Parsons supports various SQL dialects. That is handled by creating a new class for each one (`Redshift` class, `MySQL` class, etc). Currently each class has its own copy of common methods, (e.g. `generate_data_types). It would be ideal to have a shared method that each class could inherit.

Assigning myself @dannyboy15 to this.","[Label(name=""low priority"")]",[],0
851,Fix lint errors and linting command,https://api.github.com/repos/move-coop/parsons/issues/340,Daniel,340,closed,2020-08-10 07:00:22+00:00,2020-09-09 00:59:00+00:00,2020-09-09 00:59:00+00:00,"The package now contains a flake8 config file so [this line](https://github.com/move-coop/parsons/blob/master/.circleci/config.yml#L55) may be updated accordingly. More importantly though, the command should be updated to check the entire package including `tests/` and `useful_resources/`.

Currently running `flake8 .` (from the root of the package) return ~2800 lint errors. (see attached file)
[lint_errors.txt](https://github.com/move-coop/parsons/files/5049393/lint_errors.txt)
","[Label(name=""low priority""), Label(name=""testing"")]",[],0
852,Databases Docs Update,https://api.github.com/repos/move-coop/parsons/issues/339,,339,closed,2020-08-09 22:45:48+00:00,2020-08-16 21:47:21+00:00,2020-08-10 18:37:10+00:00,"Updates documentation for:

- MySQL
- Postgres
- DBSync
- Database Global Overview

Addresses Issue #269 ",[],[],2
853,S3 Docs Update,https://api.github.com/repos/move-coop/parsons/issues/338,,338,closed,2020-08-09 18:54:43+00:00,2020-08-16 21:47:09+00:00,2020-08-10 22:42:58+00:00,"- Updates the S3 docs to add authentication and quickstart, following the current structure.
- Fixes a typo in the ActionKit quickstart.

Plugging away at #269.",[],[],0
854,"Bloomerang class, docs, and tests",https://api.github.com/repos/move-coop/parsons/issues/337,Randi Griffin,337,closed,2020-08-08 16:15:15+00:00,2020-09-17 20:01:33+00:00,2020-09-16 19:32:04+00:00,"This is a WIP addressing Issue #266. It's my first attempt at a connector, so I'm pausing for feedback. 

Summary of work:

- Added Bloomerang class, including methods for authorization and get/post/put/delete requests for constituents. I used the Hustle connector as my main example since it similarly uses headers for Bearer authorization. 
- Added Bloomerang docs, modeled after ActionKit
- Added a few tests, modeled after ActionKit

It would be very helpful if someone could point me to good examples for writing tests, since I don't have much experience with this, but I want to learn. I tried to set up some simple tests by following ActionKit as an example, but I'm not confident in my work. Currently my 'test_get_constituent' function is failing but I don't understand why, since I tried to set it up exactly the same as the ActionKit 'test_get_user' function. 

I'd also appreciate clarity on which methods should be added. For example, the issue description says ""Pull constituents and their emails, addresses, and phone numbers"", but it isn't clear to me if this is all covered by 'get_constituent', or if we want separate methods for emails, addresses, and phone numbers, since those are API endpoints as well. I'm also not sure if I should be adding methods for working with lists in addition to individual items (i.e., a 'get_constituents' method in addition to 'get_consituent'). ",[],[],4
855,Redshift Table Utilities: fix get definition methods,https://api.github.com/repos/move-coop/parsons/issues/336,Daniel,336,closed,2020-08-07 20:41:05+00:00,2021-10-28 23:42:19+00:00,2021-10-28 23:42:19+00:00,"The sql for the underlying queries is now included instead of relying on views to exist in the database.

Addresses #301 ",[],[],3
856,Table: Add support for manifests files to from_s3_csv,https://api.github.com/repos/move-coop/parsons/issues/335,Daniel,335,closed,2020-08-07 20:04:17+00:00,2021-03-29 19:08:29+00:00,2020-09-04 11:34:31+00:00,"Addresses #319, @eliotst was this what you had in mind?",[],[],2
857,Redshift: fix unload method,https://api.github.com/repos/move-coop/parsons/issues/334,Daniel,334,closed,2020-08-07 18:26:08+00:00,2021-03-29 19:07:51+00:00,2020-08-08 18:05:06+00:00,Addresses the first 2 issues in #322 ,[],[],0
858,Lint: add flake8 config,https://api.github.com/repos/move-coop/parsons/issues/333,Daniel,333,closed,2020-08-07 17:47:33+00:00,2021-03-29 19:07:37+00:00,2020-08-09 18:12:21+00:00,"Currently a [line length of 100 is allowed](https://github.com/move-coop/parsons/blob/master/CONTRIBUTING.md#linting). Therefore, the command to lint the code, as mentioned in the README, is `flake8 --max-line-length=100 parsons`. This change would allow for an easier way to run the command, `flake8 parsons` and allow for additional linting options to easily be added in the future.

This is also helpful for text editors that rely on flake8 to provide linting hints.",[],[],0
859,Postgres: Fix generate_data_types,https://api.github.com/repos/move-coop/parsons/issues/332,Daniel,332,closed,2020-08-07 17:36:26+00:00,2021-03-29 19:07:23+00:00,2020-08-14 14:59:16+00:00,"Currently the `generate_data_types` method doesn't correctly handle `float` for ""NA"" values. For example:
This table
| c1   | c2   |   c3 | c4   | c5   | c6   | c7   |
|------|------|------|------|------|------|------|
| a    |      |  1   | NA   | 1.4  | 1    | 2    |
| b    |      |  2   | NA   | 1.4  | 1    | 2    |
| c    |      |  3.4 | NA   |      |      | a    |
| d    |      |  5   | NA   | 1.4  | 1    | 2    |
| e    |      |  6   | NA   | 1.4  | 1    | 2    |
| f    |      |  7.8 | NA   | 1.4  | 1    | 2    |
| g    |      |  9   | NA   | 1.4  | 1    | 2    |

gets interpreted with types
`['varchar', 'varchar', 'smallint', '', 'varchar', 'varchar', 'varchar']` 
instead of
`['varchar', 'varchar', 'decimal', 'varchar', 'decimal', 'smallint', 'varchar']`
",[],[],3
860,Create infrastructure for sample code,https://api.github.com/repos/move-coop/parsons/issues/331,Shauna Gordon-McKeon,331,closed,2020-08-06 15:44:31+00:00,2020-08-07 14:20:20+00:00,2020-08-07 14:20:20+00:00,"This is my first stab at creating a new infrastructure for handling sample code.  I expect it to need significant edits before merging.

This PR:

- adds a README describing our sample script process, and including a table listing existing scripts
- creates a template_script.py file that outlines how we'd like the scripts to be structure
- edits the existing four scripts to be in line with the template

Some particular feedback I'd like:

- do the label names I've chosen work?  once I get the thumbs up, I'll create the labels and update the PR so the label links in the README link to the labels.
- does the structure of the template_script look good?
- does how we're now handling config variables look good?  I want to isolate config variables in their own section at the top so they're easy to use.  Currently these scripts may overwrite environmental variables, which may not be ideal.
- have I correctly described what each of the four sample scripts does?

Also if anyone can actually _test_ the refactored sample scripts that'd be swell.",[],[],0
861,Update ngpvan.rst,https://api.github.com/repos/move-coop/parsons/issues/330,elyse-weiss,330,closed,2020-08-06 13:35:47+00:00,2020-08-16 21:47:22+00:00,2020-08-09 18:11:40+00:00,,[],[],0
862,actionkit: use `with` when uploading,https://api.github.com/repos/move-coop/parsons/issues/329,Eliot Stone,329,closed,2020-08-06 12:38:21+00:00,2020-08-16 21:47:24+00:00,2020-08-14 14:59:39+00:00,"This commit updates the ActionKit Connector to use the `with`
statement when using `requests` to upload CSV data. This commit
is meant to handle an error seen by a user where connections
weren't properly closed after upload. The `with` statement should
ensure that `requests` properly closes connections between
calls to the ActionKit API.",[],[],0
863,Add Support Note to VAN Connector,https://api.github.com/repos/move-coop/parsons/issues/328,elyse-weiss,328,closed,2020-08-03 13:37:13+00:00,2020-08-09 18:11:40+00:00,2020-08-09 18:11:40+00:00,"We also need to say that TMC is responsible for maintenance, and VAN Support cannot handle any support issues.","[Label(name=""documentation"")]","[NamedUser(login=""jburchard""), NamedUser(login=""ydamit"")]",0
864,Phone2Action Update Advocate,https://api.github.com/repos/move-coop/parsons/issues/327,ABarrett,327,closed,2020-07-30 19:49:18+00:00,2020-08-24 18:38:36+00:00,2020-08-24 18:38:36+00:00,"Need to create endpoints for ```update_advocate()```  for both opt-in/opt-out and update phone number.
We need to be able to both push and pull data from the contacts ```phone``` , ```smsOptin```, ```advocateid ```, ```smsOptout```, and ```campaigns``` in order to update the contact's phone and or opt-out  or opt-in status.

[API Docs}(http://docs.phone2action.com/#overview)

","[Label(name=""high priority"")]","[NamedUser(login=""eliotst"")]",0
865,Add GitHub connector,https://api.github.com/repos/move-coop/parsons/issues/326,Pat Sier,326,closed,2020-07-29 13:47:13+00:00,2020-07-30 14:30:54+00:00,2020-07-30 14:30:54+00:00,"Should close #229. Adds an initial GitHub connector that's a thin wrapper around [PyGithub](https://github.com/PyGithub/PyGithub).

There's a ton of functionality, so this is an initial stab at a setup that we could build on more. A few things I wanted to flag:

* Right now it loads pull requests in any call to `list_repo_issues`, and I'm not seeing a clear way of filtering that out through the API. If it's a priority we might be able to do it manually
* A lot of the JSON responses are nested, and I'm not sure how the `Table` will handle that

Let me know what updates I can make, thanks!",[],[],1
866,Bump Python Version,https://api.github.com/repos/move-coop/parsons/issues/325,,325,closed,2020-07-28 17:59:22+00:00,2020-08-16 21:47:25+00:00,2020-07-29 16:15:37+00:00,"Update the setup to reflect that we support Python 3.8

Follows PR #303.",[],[],1
867,Pin Chrome + Chrome Driver version,https://api.github.com/repos/move-coop/parsons/issues/324,Eliot Stone,324,open,2020-07-27 14:19:47+00:00,2022-03-01 21:21:50+00:00,,"Currently, on the `latest` version of the Parsons Docker image, the version of Chrome installed by `sudo apt-get install google-chrome-stable` is 84. However, the version of ChromeDriver installed by `wget`ing the ""LATEST"" version of ChromeDriver is 85. When a user tries to use selenium with the Parsons Docker image, they get an error: `Message: session not created: This version of ChromeDriver only supports Chrome version 85`

To prevent the Docker image from getting in this situation, we should specify the version of Chrome and ChromeDriver that we install so we can ensure that they are compatible.

","[Label(name=""bug""), Label(name=""good first issue""), Label(name=""dependency update""), Label(name=""medium priority"")]",[],5
868,Redshift/Postgres Fix Duplicate Columns Bug,https://api.github.com/repos/move-coop/parsons/issues/323,,323,closed,2020-07-25 15:10:38+00:00,2020-08-16 21:47:26+00:00,2020-07-28 14:21:52+00:00,"When a table is copied and has duplicate columns, the list index of the duplicate column(s) have their list index appended, avoiding Postgres / Redshift errors.

Addresses Issue #204 ",[],[],0
869,Unload Issues,https://api.github.com/repos/move-coop/parsons/issues/322,elyse-weiss,322,closed,2020-07-23 19:38:50+00:00,2020-08-08 18:05:21+00:00,2020-08-08 18:05:21+00:00,"A few issues with the unload command:

- [ ] I was trying to run a query that ended with `where state='AK'` -- after some investigation by @eliotst, we discovered that I would need to write it as `where state=''AK''`, which is definitely not intuitive
- [ ] the data comes in pipe-delimited. This feels not ideal - slash would be great to be able to parameterize?
- [ ] the size limit meant I was producing A LOT of files - is there any way to up that?","[Label(name=""bug"")]",[],2
870,Adding 'Capitalize Parsons' to best practices issue #286,https://api.github.com/repos/move-coop/parsons/issues/321,Ehren,321,closed,2020-07-23 19:18:10+00:00,2020-09-09 18:05:05+00:00,2020-09-09 18:05:05+00:00,"Small change, apologies for all the unnecessary commits in my fork",[],[],9
871,Change the venv instructions to Python3,https://api.github.com/repos/move-coop/parsons/issues/320,Ilona Brand,320,closed,2020-07-23 16:44:18+00:00,2020-08-25 17:03:13+00:00,2020-07-23 22:11:30+00:00,Make sure that python3 is being used for the venv instructions because if the system defaults to Python 2.7 this syntax will fail.,[],[],0
872,Load CSV data from an S3 manifest,https://api.github.com/repos/move-coop/parsons/issues/319,Eliot Stone,319,closed,2020-07-21 18:50:09+00:00,2020-09-05 00:13:43+00:00,2020-09-05 00:13:43+00:00,"Redshift uses [manifest files](https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html) when loading data from S3 (using `COPY`) and when exporting data into S3 (using `UNLOAD`). It could be useful to have a method on the `S3` class that would be able to load multiple CSV files from S3 into a single `Table`.

Under the hood, the method would just need to:
  1) download the manifest file from S3
  2) use the manifest to figure out which files to grab from S3
  3) load each of the files into a `Table`
  4) concatenate all of the files into a single `Table`","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""low priority"")]",[],3
873,Issue #286 sphinx for building docs,https://api.github.com/repos/move-coop/parsons/issues/318,Ehren,318,closed,2020-07-21 05:29:53+00:00,2020-08-25 17:03:13+00:00,2020-07-21 17:15:53+00:00,"Added and lightly copyedited this comment to the docs section: https://github.com/move-coop/parsons/pull/313#issuecomment-660397875

Note, I wasn't able to `make html`, even after installing the proper versions of Sphinx & the theme, I believe due to https://github.com/sphinx-doc/sphinx/issues/7420. 

Should we assume readers will know the syntax `pip install -Iv Sphinx==1.8.3` or add to requirements.txt? ",[],[],2
874,VAN Documentation Fixes,https://api.github.com/repos/move-coop/parsons/issues/317,,317,closed,2020-07-18 01:13:53+00:00,2020-08-16 21:47:28+00:00,2020-07-19 16:09:59+00:00,"This PR does the following:

- Addresses Issue #291 
- Adds Targets methods to documentation. ",[],[],0
875,VAN.get_person() - Fix MyVoters Error,https://api.github.com/repos/move-coop/parsons/issues/316,,316,closed,2020-07-18 00:43:00+00:00,2020-07-28 15:46:50+00:00,2020-07-28 15:46:46+00:00,"The default fields returned from `VAN.get_person()` are only returned with EveryAction, not MyVoters. This PR removes them when the database is set to MyVoters.

Addresses issue #310.",[],[],2
876,action_kit: some upload features,https://api.github.com/repos/move-coop/parsons/issues/315,Schuyler Duveen,315,closed,2020-07-17 21:43:10+00:00,2020-08-19 20:18:30+00:00,2020-08-19 20:18:30+00:00,"1. bulk_upload_table set_only_columns argument 
2. collect upload_errors 
3. warn on missing email or user_id column uploads

",[],[],2
877,Add slack classmethod to optionally send a message through a webhook rather than the full-access api_key,https://api.github.com/repos/move-coop/parsons/issues/314,Schuyler Duveen,314,closed,2020-07-17 21:40:38+00:00,2020-07-28 16:26:16+00:00,2020-07-28 16:26:16+00:00,,[],[],0
878,Issue #269 update actionkit connector docs,https://api.github.com/repos/move-coop/parsons/issues/313,Randi Griffin,313,closed,2020-07-16 21:49:58+00:00,2020-10-07 14:14:17+00:00,2020-07-21 18:40:27+00:00,"This PR updates connector documentation for the ActionKit class, per issue #269.

Change summary:

- Updated the overview and quickstart in the `*.rst`, aiming to following the examples provided in the issue description as closely as possible
- Fixed a couple typos in the main docstring for the class
- Cleaned up `**kwargs` documentation, making it consistent and correct across functions

All tests passed ",[],[],5
879,Add connector account creation information to documentation,https://api.github.com/repos/move-coop/parsons/issues/312,Shauna Gordon-McKeon,312,closed,2020-07-16 19:22:18+00:00,2022-03-16 18:22:49+00:00,2022-03-16 18:22:36+00:00,"We want to add information for each connector about whether they offer free accounts that you can use to authenticate the connector, as well as whether they offer a dummy/public API (where you wouldn't even need to authenticate).  There's also a column which contains an evaluation of the effort involved in creating a Parsons-specific dummy account with shared/open credentials.  

The table below can be updated as we research the different connectors, and then once we're ready we'll move this to the sphinx docs.  In addition to a ""yes/no"", a link to where you found that information would be much appreciated.

| Connector | Has Free Accounts | Has Public API | Can Create Parsons Dummy Account| 
| -------------- | :--------- | ----------: | ----------: | 
| Action Kit
| Action Network
| Airtable | [yes](https://airtable.com/pricing) | no | yes |
| AWS: Lambda
| AWS: S3
| AWS: Redshift | No | No | Ragtag has an account, please ask for access |
| Azure: Blob Storage
| Bill.com
| Braintree
| Civis
| Copper
| CrowdTangle
| Databases: MySQL
| Databases: Postgres
| Databases: Redshift
| Databases: Database Sync
| Facebook Ads
| FreshDesk
| Google: Big Query
| Google: CloudStorage
| Google: Civic
| Google: Google Sheets
| Hustle
| MailChimp
| Mobilize America
| New/Mode
| Ngpvan | No | No | Ragtag has an account, please ask for access |
| PDI
| Phone2Action
| Redash
| Rock the Vote
| Salesforce | [yes](https://developer.salesforce.com/signup) | [yes](https://developer.salesforce.com/docs/api-explorer) | [yes](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_scratch_orgs.htm)
| SFTP
| TargetSmart
| Twilio
| TurboVote
| Zoom

","[Label(name=""documentation""), Label(name=""good first issue"")]",[],3
880,populate_table_from_query - doesn't cascade,https://api.github.com/repos/move-coop/parsons/issues/311,elyse-weiss,311,open,2020-07-16 18:57:11+00:00,2023-02-21 19:15:40+00:00,,"This is likely true of a bunch of commands that allow for a `drop`, but they error if there are dependent views. One approach would be to create an additional option of `drop_cascade`. 

Thoughts? @ydamit @eliotst ","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update""), Label(name=""needs more info"")]",[],1
881,VAN: EA Expand Fields Cause Error on MyVoters,https://api.github.com/repos/move-coop/parsons/issues/310,,310,closed,2020-07-16 13:32:02+00:00,2020-08-08 17:40:07+00:00,2020-08-08 17:40:07+00:00,"The `get_person()` method includes a default list for the `expand_fields` arg, but one of those: `codes` returns a specific 400 error about that not being available, and two others: `contribution_history` and `organization_roles` return generic 500 errors.

Should be easy to solve by removing those list elements if `self.db_code == 0`","[Label(name=""bug"")]",[],0
882,Reorganize contributing.md,https://api.github.com/repos/move-coop/parsons/issues/309,Shauna Gordon-McKeon,309,closed,2020-07-14 20:14:35+00:00,2020-07-15 18:37:57+00:00,2020-07-15 18:37:56+00:00,Address some of the items in #286 ,[],[],3
883,Issue #286 partial,https://api.github.com/repos/move-coop/parsons/issues/308,Ehren,308,closed,2020-07-14 16:48:40+00:00,2020-07-14 18:26:47+00:00,2020-07-14 18:26:47+00:00,"Item 2: Change folder to `useful_resources`
(partial) Item 3: Details about virtual environments 

I did not run the test suite on this, as changes are confined to CONTRIBUTOR.md. Viewed on github, formatting looks good.",[],[],1
884,add actionkit update events and signups,https://api.github.com/repos/move-coop/parsons/issues/307,Randi Griffin,307,closed,2020-07-14 13:48:57+00:00,2020-09-09 14:20:59+00:00,2020-07-15 18:54:38+00:00,"This PR adds two new methods:

`ActionKit.update_event()`
`ActionKit.update_event_signup()`

I also updated `.gitignore` to prevent tracking of PyCharm project files.

Issue addressed: #233 

All pytest tests passed.",[],[],1
885,Azure: Return full blob info rather than just blob name from list_blobs,https://api.github.com/repos/move-coop/parsons/issues/306,Soren Spicknall,306,closed,2020-07-13 22:12:08+00:00,2020-08-16 21:47:31+00:00,2020-07-13 22:15:11+00:00,"This augments the functionality of list_blobs to do what Azure's built in blob list function does, providing more than just the name of each blob.",[],[],0
886,Column Map + Coalesce,https://api.github.com/repos/move-coop/parsons/issues/305,elyse-weiss,305,closed,2020-07-13 19:00:40+00:00,2020-08-16 21:47:29+00:00,2020-07-25 13:43:47+00:00,,[],[],0
887,New Azure class to handle account authentication methods,https://api.github.com/repos/move-coop/parsons/issues/304,Soren Spicknall,304,closed,2020-07-13 15:10:10+00:00,2020-08-16 21:47:32+00:00,2020-07-14 21:07:05+00:00,"Added a new class, AzureAccountAccess, to eventually handle various methods of authentication for resource access. Initially, added one authentication method, using [TokenCredentials](https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.common.tokencredential.tokencredential?view=azure-python).

The intent here is to get one authentication method up and running immediately, then build out this class in more detail down the road with other valid Azure authentication methods.",[],[],1
888,update civis and joblib for python 3.8,https://api.github.com/repos/move-coop/parsons/issues/303,Eliot Stone,303,closed,2020-07-11 11:36:07+00:00,2020-07-12 22:12:22+00:00,2020-07-12 22:12:16+00:00,"This commit updates the versions of the civis and joblib
dependencies to ensure full support of Python 3.8.",[],[],0
889,upgrade psycopg2 version,https://api.github.com/repos/move-coop/parsons/issues/302,Eliot Stone,302,closed,2020-07-10 21:03:09+00:00,2020-07-11 00:56:35+00:00,2020-07-11 00:56:28+00:00,"This commit updates the version of the `psycopg2-binary`
dependency for Parsons. THe update is necessary due to a
limitation in the old version (2.7.6) that caused it to not play
well with Python 3.8.

This fixes issues with installing Parsons on Python 3.8.",[],[],0
890,Redshift get_table_definition method based on non-standard Redshift views,https://api.github.com/repos/move-coop/parsons/issues/301,Eliot Stone,301,open,2020-07-10 20:56:08+00:00,2023-02-21 19:18:19+00:00,,"The `get_table_definition` + `get_table_definitions` methods in `Redshift` are based on views that aren't standard in Redshift, so these functions will not work for most people. We should either rework the SQL in the methods to use only standard Redshift functionality, or else remove them.","[Label(name=""bug""), Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update"")]","[NamedUser(login=""dannyboy15"")]",2
891,Do not error if column already exists in map_columns,https://api.github.com/repos/move-coop/parsons/issues/300,elyse-weiss,300,closed,2020-07-10 19:29:15+00:00,2020-07-29 17:30:20+00:00,2020-07-29 17:30:20+00:00,"If the column name you are mapping to already exists, the `map_columns()` fails. Prefer for it to coalesce.","[Label(name=""enhancement"")]","[NamedUser(login=""elyse-weiss"")]",0
892,Redshift + S3: Move Some Logging To DEBUG,https://api.github.com/repos/move-coop/parsons/issues/299,,299,closed,2020-07-09 15:20:47+00:00,2020-08-16 21:47:34+00:00,2020-07-12 22:16:59+00:00,"A first pass a moving a couple of table utility methods from `INFO` to `DEBUG`. This should reduce the size of our logs significantly.

@SorenSpicknall @eliotst - Anything else that we want to add to this PR? Just thread some ideas.",[],[],2
893,Redshift: Correctly pass S3 Credentials,https://api.github.com/repos/move-coop/parsons/issues/298,,298,closed,2020-07-09 14:32:49+00:00,2020-07-09 14:54:05+00:00,2020-07-09 14:54:02+00:00,"Addresses #285 in which S3 credentials that were being passed via the construction of the Redshift class as kwargs were not actually being read.

Now Working:

```
rs = Redshift(..., aws_access_key_id='MyFakeID', aws_secret_access_key='MyFakeSecret')
rs.copy(tbl, 'schema.table')
```","[Label(name=""bug"")]",[],0
894,Build SurveyGizmo Class,https://api.github.com/repos/move-coop/parsons/issues/297,elyse-weiss,297,closed,2020-07-09 13:03:59+00:00,2020-10-05 21:12:18+00:00,2020-10-05 21:12:18+00:00,"TMC wrote some code that we know works: https://gist.github.com/elyse-weiss/101d251eeed2e4f2fb25e8754ca493bd

Eager to finalize as Parsons connector.","[Label(name=""low priority""), Label(name=""new connector"")]","[NamedUser(login=""coreyhaines"")]",2
895,Fix typo,https://api.github.com/repos/move-coop/parsons/issues/296,Marjorie Roswell,296,closed,2020-07-08 03:43:43+00:00,2020-07-08 04:11:30+00:00,2020-07-08 04:11:30+00:00,,[],[],0
896,Sdutta/targets fix,https://api.github.com/repos/move-coop/parsons/issues/295,Shilpa Dutta,295,closed,2020-07-06 21:51:26+00:00,2020-07-25 13:50:45+00:00,2020-07-25 13:50:45+00:00,,[],[],2
897,Add Azure Blob Storage connector,https://api.github.com/repos/move-coop/parsons/issues/294,Pat Sier,294,closed,2020-07-05 20:42:52+00:00,2020-07-09 01:29:59+00:00,2020-07-08 12:22:44+00:00,"Progress on #73 (might close if it was only intended for Azure storage). Adds a connector for Azure Blob Storage primarily based on the Google Cloud Storage connector, but also pulling a bit from the S3 connector.

Here are a few things in particular I'm interested in feedback on:

- I split out `AZURE_ACCOUNT_NAME` and `AZURE_ACCOUNT_DOMAIN` from `AZURE_ACCOUNT_URL` because the account domain is almost always the same thing, and I haven't seen as strong of conventions for environment variables for Azure storage as I've seen around S3
- The example code uses an access token for authentication, and I broadened that here to support any credential accepted by the `BlobServiceClient`
- As a way of enforcing stricter typing the Azure storage SDK has a lot of wrapper types for things like setting headers when uploading blobs. To make this a little easier to use in `put_blob`, I'm pulling out `kwargs` so that people people can supply `content_type=` as a keyword argument instead of importing the `ContentSettings` type, initializing it with `content_type`, and passing it to `put_blob` in the `content_settings` keyword argument. If that's wrapping the API too much though I can remove it

Feedback is welcome! I also wrote up some notes on the process of writing a connector for the first time that I'll share in the Slack",[],[],1
898,Sdutta/targets-bug-fix,https://api.github.com/repos/move-coop/parsons/issues/293,Shilpa Dutta,293,closed,2020-07-03 00:26:01+00:00,2020-07-06 13:22:50+00:00,2020-07-06 13:22:50+00:00,"-VAN Targets' create_target_export() had the wrong property (target_id instead of targetId) 
-json key error using [0], when it was not needed",[],[],0
899,Fix broken links in useful_resources,https://api.github.com/repos/move-coop/parsons/issues/292,Pat Sier,292,closed,2020-07-02 19:37:54+00:00,2020-07-06 13:23:26+00:00,2020-07-06 13:23:26+00:00,"Fixes the broken link to Python 3 Setup and removes the link to Local Development Setup since it looks like that file was removed. Let me know if anything looks off, thanks!",[],[],0
900,Documentation Error in VAN,https://api.github.com/repos/move-coop/parsons/issues/291,elyse-weiss,291,closed,2020-07-02 17:38:31+00:00,2020-07-19 16:10:15+00:00,2020-07-19 16:10:15+00:00,"In Common Wokflows, a function is referred to as: `van.score_update_status(job_id,'approved')`

It is actually called: `update_score_status()`","[Label(name=""documentation"")]",[],0
901,"add ""How to Build a Connector"" doc",https://api.github.com/repos/move-coop/parsons/issues/290,Eliot Stone,290,closed,2020-07-02 01:35:58+00:00,2020-07-02 15:49:12+00:00,2020-07-02 15:49:09+00:00,"This commit adds the ""How to Build a Connector"" doc to the docs/
folder to help contributors get started on adding a new connector
to Parsons. This commit also:
 * adds a new ""Contributing Docs"" section to the index.rst TOC,
   with an entry for the new ""How to Build a Connector"" doc as
   well as the contributing.md
 * Move the contributing.md from within docs/ to the root as
   CONTRIBUTING.md to make it easier to find
 * add the `m2r` library to the docs/requiremnts.txt to support
   embedding the existing CONTRIBUTING.md file into an RST file
 * tweak the docs/conf.py to support m2r and embedding a md file",[],[],0
902,Adding thread option to Slack.message_channel [WIP],https://api.github.com/repos/move-coop/parsons/issues/289,elyse-weiss,289,closed,2020-06-28 23:36:00+00:00,2020-07-09 12:46:15+00:00,2020-07-09 12:46:00+00:00,I'm not even sure how to test this....could definitely use help!,[],[],2
903,Add Thread Option to Slack.message_channel,https://api.github.com/repos/move-coop/parsons/issues/288,elyse-weiss,288,closed,2020-06-28 23:17:59+00:00,2020-07-14 01:36:48+00:00,2020-07-14 01:36:48+00:00,https://api.slack.com/messaging/sending#threading,"[Label(name=""connector update"")]",[],0
904,Add phoneOptInStatus to VAN.upsert_person(),https://api.github.com/repos/move-coop/parsons/issues/287,,287,closed,2020-06-25 18:40:00+00:00,2020-08-11 18:25:22+00:00,2020-08-11 18:25:22+00:00,"Include the option to set phoneOptInStatus when upserting a person into VAN.

For organizations using Frakture (and possibly other) tools to sync data out of VAN/EA, this field is necessary to determine which records/phone numbers can be synced.","[Label(name=""enhancement""), Label(name=""medium priority"")]",[],1
905,Improve contributor onboarding,https://api.github.com/repos/move-coop/parsons/issues/286,Shauna Gordon-McKeon,286,closed,2020-06-25 18:31:02+00:00,2022-03-17 17:51:10+00:00,2022-03-17 17:51:09+00:00,"Highest Priority:

- [x] move contributing.md file to the top level directory
- [x] update contributing guide to link to useful_resources, not sample_code folder (must have gotten moved)
- [x] contributing.md should have clear walkthrough of steps, including cloning from github, setting up a virtualenv, up through running tests when they've finished installing
- [x] contributing.md (and maybe also readme.md) should include links to communications channels so people know how to talk to us, where to ask for help, how to report bugs, feature requests, etc
- [x] quickstart guide needs to not require creating accounts, setting env variables, etc.  need to pick a connectorwhere we can create a dummy account (or the API itself provides a dummy endpoint) that we can use here.
- [x] add instructions for building the docs (see https://github.com/move-coop/parsons/pull/313#issuecomment-660397875)

Lower Priority:
- [x] add ""capitalize Parsons"" to best practices in contrib docs
- [ ] contributing.md file should provide extra context/explanations for things that might be new to people, beyond just ""run this command""
  - unit testing & what they should see when they run pytest
  - setting up a virtual environment
  - the difference between installing via pip and setting up as a developer (cloning, etc)
- [ ] we should make sure install is testing on Windows and Linux as well as Mac, and it's clear what people on those platforms need to do differently, if anything
- [ ] look into adding install instructions for Anaconda users

","[Label(name=""documentation""), Label(name=""high priority"")]",[],9
906,Redshift: Not reading passed AWS Credentials,https://api.github.com/repos/move-coop/parsons/issues/285,,285,closed,2020-06-23 20:39:40+00:00,2020-07-09 14:54:24+00:00,2020-07-09 14:54:24+00:00,"When you instantiate the Redshift class, you optionally have the ability to pass in AWS credentials for copying data. This should be read for when you run the copy method, however it doesn't work.

**Should work, but doesn't...**
```
rs = Redshift(..., aws_access_key_id='MyFakeID', aws_secret_access_key='MyFakeSecret')
rs.copy(tbl, 'schema.table')
```","[Label(name=""bug"")]",[],0
907,Determine method for getting Slack API Key,https://api.github.com/repos/move-coop/parsons/issues/284,elyse-weiss,284,closed,2020-06-23 20:25:22+00:00,2020-09-07 16:25:11+00:00,2020-09-07 16:25:11+00:00,"Slack shutdown their legacy tokens, and it's unclear how one should generate an API key for use with the Slack class.","[Label(name=""documentation""), Label(name=""medium priority"")]",[],3
908,Hot Fix: Fix Offset Code in DB Sync,https://api.github.com/repos/move-coop/parsons/issues/283,,283,closed,2020-06-23 18:33:37+00:00,2020-07-09 12:13:45+00:00,2020-06-23 18:52:11+00:00,"Was missing an ""f"" string and thus, failing.",[],[],0
909,Unit test failures on Windows,https://api.github.com/repos/move-coop/parsons/issues/282,Eliot Stone,282,closed,2020-06-23 15:07:36+00:00,2020-12-03 19:09:35+00:00,2020-12-03 19:09:35+00:00,"A user saw test failures when running `pytest` on Windows against the Parsons test package. Specifically, they saw 8 failures in the `test.test_smtp.TestSMTP` and `test.test_gmail.TestGmail`.","[Label(name=""medium priority""), Label(name=""testing"")]","[NamedUser(login=""seth-russell"")]",5
910,Remove Empty Lists from Action Network add_person() POST request,https://api.github.com/repos/move-coop/parsons/issues/281,,281,closed,2020-06-22 20:19:55+00:00,2020-09-20 17:43:01+00:00,2020-09-20 17:43:01+00:00,"Right now, all arguments aside from `email_address` have default values and those default values that are empty lists `[]` cause the call to return an error. Filtering out any arguments that haven't been supplied in the final call should solve for that pretty simply.","[Label(name=""bug""), Label(name=""low priority"")]",[],0
911,GoogleSheets: Format Cells,https://api.github.com/repos/move-coop/parsons/issues/280,,280,closed,2020-06-20 16:49:58+00:00,2020-07-09 12:13:46+00:00,2020-06-22 18:30:58+00:00,"A new method that allows the user to format a cell or a range of cells.

Addresses #251.",[],[],0
912,bump version to 0.14.0,https://api.github.com/repos/move-coop/parsons/issues/279,Eliot Stone,279,closed,2020-06-17 20:23:09+00:00,2020-06-18 17:18:55+00:00,2020-06-18 17:18:51+00:00,"Also, this commit cleans up a few stray `print` calls.",[],[],0
913,VAN: Update Event,https://api.github.com/repos/move-coop/parsons/issues/278,,278,closed,2020-06-17 18:16:33+00:00,2021-08-10 22:53:31+00:00,2020-09-22 17:53:04+00:00,"This PR adds in a few new methods:

`VAN.update_event()`
`VAN.add_event_location()`",[],[],1
914,GoogleSheets: Bump gspread package.,https://api.github.com/repos/move-coop/parsons/issues/277,,277,closed,2020-06-15 16:01:52+00:00,2020-06-18 19:38:48+00:00,2020-06-15 16:25:27+00:00,Upgrades `gspread` package to 3.3.0,[],[],0
915,A couple more minor edits...,https://api.github.com/repos/move-coop/parsons/issues/276,Joe McLaughlin,276,closed,2020-06-11 19:38:27+00:00,2020-06-16 19:17:04+00:00,2020-06-16 19:17:04+00:00,,[],[],0
916,Minor edits,https://api.github.com/repos/move-coop/parsons/issues/275,Joe McLaughlin,275,closed,2020-06-11 19:12:10+00:00,2020-06-11 19:21:36+00:00,2020-06-11 19:15:54+00:00,,[],[],2
917,Cover new column expansion case in Redshift testing,https://api.github.com/repos/move-coop/parsons/issues/274,Soren Spicknall,274,closed,2020-06-11 18:58:03+00:00,2020-06-18 19:38:56+00:00,2020-06-15 19:52:52+00:00,"In #273, we implemented automatic resizing of varchar columns when using the Redshift upsert function. This PR adds testing to cover that new case.",[],[],3
918,Redshift upsert: auto resize column widths if necessary,https://api.github.com/repos/move-coop/parsons/issues/273,Soren Spicknall,273,closed,2020-06-10 19:54:29+00:00,2020-06-18 19:38:58+00:00,2020-06-11 16:49:32+00:00,"Resolves #250. Resizes the target table columns in Redshift based on the incoming Parsons table before creating the staging table in Redshift, so that when the staging table is created it already is at the correct column widths. This also means that an upsert will sometimes error out earlier if the incoming table doesn't match the columns of the target table, a side effect that could save some processing time on failed upserts.",[],[],0
919,alter_table not working on upsert,https://api.github.com/repos/move-coop/parsons/issues/272,elyse-weiss,272,closed,2020-06-10 17:47:05+00:00,2020-06-10 19:15:27+00:00,2020-06-10 19:15:26+00:00,"Ideally if the parsons table you are upserting requires a varchar extension, the alter_table would run on destination table before anything else happens.","[Label(name=""bug"")]","[NamedUser(login=""SorenSpicknall"")]",2
920,Generally Round Up When Creating Redshift Tables,https://api.github.com/repos/move-coop/parsons/issues/271,elyse-weiss,271,open,2020-06-10 17:42:07+00:00,2023-02-21 19:20:15+00:00,,"We should never create columns with `SMALLINT`, only `INT`.

Also, the varchar length should probably be at least `varchar(100)`.","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update"")]",[],0
921,Periscope Connector,https://api.github.com/repos/move-coop/parsons/issues/270,elyse-weiss,270,closed,2020-06-08 13:06:45+00:00,2020-10-08 13:03:48+00:00,2020-10-08 13:03:48+00:00,"https://developer.sisense.com/display/API2/REST+API+Reference+-+v1.0

There are some key endpoints that would be super helpful:

[Dashboards](https://developer.sisense.com/display/API2/REST+API+Reference+-+v1.0#RESTAPIReference-v1.0-Dashboards)

MVP:
- get/dashboards
- get/dashboards/{id}
V2 & beyond:
- get/dashboards/{dashboardId}/shares
- get/dashboards/{dashboardId}/shares/{id}
- post/dashboards/{id}/publish

For the publish piece, I want to make sure you can set filter values:
https://dtdocs.sisense.com/article/embed-api#PublishDashboard","[Label(name=""medium priority""), Label(name=""new connector"")]","[NamedUser(login=""rgriff23"")]",6
922,Improve connector documentation,https://api.github.com/repos/move-coop/parsons/issues/269,Shauna Gordon-McKeon,269,closed,2020-06-02 20:36:29+00:00,2021-07-06 20:32:23+00:00,2021-07-06 20:32:23+00:00,"The connector documentation to be more consistent.  Each connector needs, in order:

1) an overview, which explains the service that the connector connects to, and summarizes what subsets of endpoints currently are covered.  
2) a detailed guide to getting authentication info.  
3) a ""quick start guide"" that demonstrates instantiating the connector and using a method.  

See [the action kit connector documentation](https://move-coop.github.io/parsons/html/action_kit.html) for an example of what we're looking for.

This doesn't need to be done by a single person in a single PR - feel free to grab a connector by leaving a comment below and submit the updated docs as its own PR.

Here's the current status of the connector docs (thanks to @rgriff23 for the audit!):

| Connector | Overview | Authentication Guide | Quickstart | Notes | 
| -------------- | :--------- | ----------: | ----------: | ----------: | 
| Action Kit | ✔️ | ✔️ | ✔️ | |
| Action Network |  ✔️| ✔️ | ✔️ | |
| Airtable | ✔️ | ✔️ |  ✔️ | |
| AWS: ALL |   ||| needs a global overview at the top to orient the reader |
| AWS: Lambda | to add | to add | to add | |
| AWS: S3 | ✔️ | ✔️| ✔️ | |
| AWS: Redshift | ✔️ |  ✔️| ✔️| |
| Azure: Blob Storage | ✔️ |  ✔️| ✔️| |
| Bill.com | ✔️ |  ✔️| ✔️| |
| Braintree |  ✔️ | ✔️ | ✔️ | |
| Civis |  ✔️ | ✔️ | ✔️ | |
| Copper |  ✔️ | ✔️ | ✔️ | |
| CrowdTangle |  ✔️ | ✔️ | ✔️ | |
| Databases: ALL | ✔️ | | |  |
| Databases: MySQL | ✔️ | ✔️ | ✔️ | |
| Databases: Postgres | ✔️ | ✔️ | ✔️ | |
| Databases: Database Sync | ✔️ | ✔️  | ✔️ | |
| Facebook Ads |  ✔️ | ✔️ | ✔️ | |
| FreshDesk |  ✔️ | ✔️ | ✔️ | |
| Github | ✔️ | ✔️  | ✔️ | |
| Google: ALL | | | | needs a global overview at the top to orient the reader |
| Google: Big Query | ✔️| to add | to add | |
| Google: CloudStorage | needs improvement| to add| to add | |
| Google: Civic | ✔️ | to add| to add| |
| Google: Google Sheets | needs improvement|needs improvement | to add | |
| Hustle | ✔️ |  ✔️ | ✔️ | |
| MailChimp | ✔️ | ✔️  | ✔️ | |
| Mobilize America |  ✔️ | ✔️ | ✔️ | |
| New/Mode | ✔️ | ✔️ | ✔️ | |
| Ngpvan |✔️ |✔️ |✔️ | |
| PDI |✔️ |✔️ |✔️ | |
| Phone2Action | ✔️ |✔️ |✔️ | |
| Redash | ✔️ |✔️ |✔️ | |
| Rock the Vote | ✔️ |✔️ |✔️ | |
| Salesforce | ✔️ |✔️ |✔️ | |
| SFTP | ✔️ |✔️ |✔️ | |
| TargetSmart | ✔️ |✔️ |✔️ | |
| Twilio |  ✔️ | ✔️ | ✔️ | |
| TurboVote |  ✔️ | ✔️ | ✔️ | |
| Zoom| ✔️ | ✔️ | ✔️ | |","[Label(name=""documentation""), Label(name=""high priority"")]",[],8
923,Sdutta/targets,https://api.github.com/repos/move-coop/parsons/issues/268,Shilpa Dutta,268,closed,2020-05-29 20:25:53+00:00,2020-06-11 12:40:52+00:00,2020-06-11 12:40:51+00:00,,[],[],2
924,utilities.files.has_data produce True if only header row,https://api.github.com/repos/move-coop/parsons/issues/267,elyse-weiss,267,closed,2020-05-28 20:20:24+00:00,2020-07-18 17:43:14+00:00,2020-07-18 17:43:14+00:00,Ideally this would be False if there was only a header. Perhaps we could add an argument determining if one should look for a header or not?,"[Label(name=""bug""), Label(name=""low priority"")]",[],2
925,Add Bloomerang class,https://api.github.com/repos/move-coop/parsons/issues/266,Melissa Woods,266,closed,2020-05-27 22:15:23+00:00,2020-10-05 21:14:02+00:00,2020-10-05 21:14:02+00:00,"This ticket is for adding a Bloomerang class to Parsons.

[API documentation is here.](https://bloomerang.co/features/integrations/api/rest-api/)

The most important capabilities will be:

- Pull constituents and their emails, addresses, and phone numbers
- Pull donations, recurring donations, recurring donation payments, pledges, and pledge payments
- Pull custom fields and custom field values
- Create & update constituents
- Create donations, recurring donations, and recurring donation payments
- Create interactions","[Label(name=""medium priority""), Label(name=""new connector"")]","[NamedUser(login=""rgriff23"")]",5
926,phone2action: handle empty advocates results,https://api.github.com/repos/move-coop/parsons/issues/265,Eliot Stone,265,closed,2020-05-27 13:57:01+00:00,2020-05-28 19:08:55+00:00,2020-05-28 19:08:52+00:00,"This commit updates the `Phone2Action` connector to handle an
empty list from the call to the advocates endpoint on the P2A API.
Before the commit, the call to get advocates was failing when
trying to unpack the advocates data.

This commit also adds the `__bool__` magic method to the Parsons
`Table` class to support simple bool checks on the emptiness of
a table (e.g. `if not tbl:` instead of `if tbl.num_rows > 0`)",[],[],0
927,materialize_to_file method,https://api.github.com/repos/move-coop/parsons/issues/264,Eliot Stone,264,closed,2020-05-26 18:03:02+00:00,2020-05-27 21:16:11+00:00,2020-05-27 21:16:07+00:00,"This commit adds the `materialize_to_file` method, which allows
a user to apply any pending transformations to a table. The table
data is written to a temp file, which overcomes a limitation of
the current `materialize` method, which requires loading all of
the data from the table into memory.",[],[],0
928,GoogleSheets Refactor,https://api.github.com/repos/move-coop/parsons/issues/263,,263,closed,2020-05-24 02:53:17+00:00,2020-06-11 14:46:07+00:00,2020-06-07 16:30:40+00:00,"This PR does some significant refactoring to the GoogleSheets class and reflects new patterns developed since that time. 

- Previously, there were separate methods to read worksheets, depending on whether you passed the worksheet index or the title of the worksheet. The methods have now been combined and it determines whether it is a title of an index based on the value passed.

- The old methods have been removed from the documentation, but still exist and throw a deprecation warning in the log.

- Adds ``**kwargs`` to a few methods to make them backward compatible.

- Renames some methods that referred to worksheets as `sheets`, which could easily be confused with `spreadsheets`. 

- Adds a method `list_worksheets()` that returns a list of worksheets in a given spreadsheet.
",[],[],1
929,SFTP: Allow batched methods on a connection.,https://api.github.com/repos/move-coop/parsons/issues/262,,262,closed,2020-05-24 02:05:18+00:00,2020-05-26 16:08:30+00:00,2020-05-26 16:08:26+00:00,"All of the methods now allow you to pass an `SFTP.connection` object as an argument. This allows the user to utilize the context manager pattern.

```
with conn as connection:
   sftp.make_directory('my_dir', connection=conn)
   sftp.put_file('my_csv.csv', connection=conn)
```

If a connection argument is not passed, then the method works as normal, opening and closing a connection.

Addresses a request #248.

Note: The new method created in #260 does not follow this pattern. A future PR will address it.",[],[],0
930,phone2action: properly pass through pagination parameter,https://api.github.com/repos/move-coop/parsons/issues/261,Eliot Stone,261,closed,2020-05-24 00:55:04+00:00,2020-05-25 01:03:35+00:00,2020-05-25 01:03:32+00:00,"This commit updates the Phone2Action connector to properly pass
the pagination parameters through the call chain. As well, this
commit adds a regression test to ensure the error can be caught
in unit tests.",[],[],0
931,SFTP.get_table() method,https://api.github.com/repos/move-coop/parsons/issues/260,,260,closed,2020-05-23 17:06:41+00:00,2020-05-25 03:15:28+00:00,2020-05-25 03:15:24+00:00,"This method downloads a file from an SFTP server and returns a Parsons table. The file must be a csv, though may be compressed.

Issue #248.",[],"[NamedUser(login=""eliotst"")]",0
932,PDI: Add Documentation To Index,https://api.github.com/repos/move-coop/parsons/issues/259,,259,closed,2020-05-23 03:12:47+00:00,2020-06-11 14:46:10+00:00,2020-05-26 15:03:00+00:00,Add's PDI to Table of Contents and adds doc strings for one method.,[],[],0
933,"phone2action: manually pagination, update data model",https://api.github.com/repos/move-coop/parsons/issues/258,Eliot Stone,258,closed,2020-05-22 18:15:48+00:00,2020-05-22 20:59:06+00:00,2020-05-22 20:59:04+00:00,"This commit updates the `Phone2Action` connector to:
 * Allow manual pagination of advocates
 * Stop calling `unpack_dict` on the `updated_at` and `created_at`
 columns to reflect the data returned from the API

---
Tested locally.",[],[],1
934,HOTFIX: elif to if inside Redshift copy to account for upsert staging table issues,https://api.github.com/repos/move-coop/parsons/issues/257,Soren Spicknall,257,closed,2020-05-22 14:27:13+00:00,2020-06-11 14:46:21+00:00,2020-05-22 20:04:01+00:00,"Passing copyargs through the Redshift upsert function occasionally runs into issues with the alter_table logic not running when it naturally should. Specifically, this happens when the copy function is leveraged to create a staging table based on the DDL of the destination table. By simply changing an elif to an if, we can resolve this issue.",[],"[NamedUser(login=""elyse-weiss"")]",0
935,Useful resources updates,https://api.github.com/repos/move-coop/parsons/issues/256,Melissa Woods,256,closed,2020-05-20 17:28:27+00:00,2022-03-17 17:51:45+00:00,2022-03-17 17:51:45+00:00,"The sample code in useful resources contains the function `set_env_var()` which folks outside TMC staff can't use. Could we change that to something external users can use?

Could we add instructions in computer setup for Windows Powershell?

","[Label(name=""documentation""), Label(name=""medium priority"")]",[],1
936,Documentation Housekeeping,https://api.github.com/repos/move-coop/parsons/issues/255,,255,closed,2020-05-20 17:23:27+00:00,2020-05-22 20:07:47+00:00,2020-05-22 20:07:42+00:00,"Did a bit of work trying to consolidate some of the tools (AWS, Google) and just some minor issues that I saw. No live code was touched.",[],"[NamedUser(login=""SorenSpicknall"")]",0
937,Bump httplib2 from 0.12.0 to 0.18.0,https://api.github.com/repos/move-coop/parsons/issues/254,,254,closed,2020-05-20 17:16:10+00:00,2020-05-20 19:35:24+00:00,2020-05-20 19:35:21+00:00,"Bumps [httplib2](https://github.com/httplib2/httplib2) from 0.12.0 to 0.18.0.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/httplib2/httplib2/blob/master/CHANGELOG"">httplib2's changelog</a>.</em></p>
<blockquote>
<p>0.18.0</p>
<p>IMPORTANT security vulnerability CWE-93 CRLF injection
Force %xx quote of space, CR, LF characters in uri.
Special thanks to Recar <a href=""https://github.com/Ciyfly"">https://github.com/Ciyfly</a> for discrete notification.
<a href=""https://cwe.mitre.org/data/definitions/93.html"">https://cwe.mitre.org/data/definitions/93.html</a></p>
<p>0.17.4</p>
<p>Ship test suite in source dist
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/pull/168"">httplib2/httplib2#168</a></p>
<p>0.17.3</p>
<p>IronPython2.7: relative import iri2uri fixes ImportError
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/pull/163"">httplib2/httplib2#163</a></p>
<p>0.17.2</p>
<p>python3 + debug + IPv6 disabled: https raised
&quot;IndexError: Replacement index 1 out of range for positional args tuple&quot;
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/issues/161"">httplib2/httplib2#161</a></p>
<p>0.17.1</p>
<p>python3: no_proxy was not checked with https
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/issues/160"">httplib2/httplib2#160</a></p>
<p>0.17.0</p>
<p>feature: Http().redirect_codes set, works after follow(_all)_redirects check
This allows one line workaround for old gcloud library that uses 308
response without redirect semantics.
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/issues/156"">httplib2/httplib2#156</a></p>
<p>0.16.0</p>
<p>IMPORTANT cache invalidation change, fix 307 keep method, add 308 Redirects
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/issues/151"">httplib2/httplib2#151</a></p>
<p>proxy: username/password as str compatible with pysocks
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/issues/154"">httplib2/httplib2#154</a></p>
<p>0.15.0</p>
<p>python2: regression in connect() error handling
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/pull/150"">httplib2/httplib2#150</a></p>
<p>add support for password protected certificate files
<a href=""https://github-redirect.dependabot.com/httplib2/httplib2/pull/143"">httplib2/httplib2#143</a></p>
</tr></table> ... (truncated)
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/httplib2/httplib2/commit/8373177d3a9e4dd9c956f9bded22a5f96a00957b""><code>8373177</code></a> v0.18.0 release</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/9fef207e85eef0534574d71fe1338c01874eba46""><code>9fef207</code></a> pyproject.toml</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/a1457cc31f3206cf691d11d2bf34e98865873e9e""><code>a1457cc</code></a> IMPORTANT security vulnerability CWE-93 CRLF injection</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/9413ffc973a2dc90abf787509ee82238345d5602""><code>9413ffc</code></a> v0.17.4 release</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/fe3136ac369199abd9d6afd2d2a61a11da9e32ac""><code>fe3136a</code></a> Ship new test suite in source dist</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/f5684876ef5e3b57c81f716c08b316fa36684f08""><code>f568487</code></a> v0.17.3 release</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/fa0c4d2cb98ece0a7b156583d308ddf4370dc7d7""><code>fa0c4d2</code></a> Switched the iri2uri import to a relative import</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/067b3f250769e921dc397e28f3aadd0ffa14e17b""><code>067b3f2</code></a> v0.17.2 release</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/59586b5f0ccfa9d570d8c7d908d00d34cf1f3c89""><code>59586b5</code></a> Fix debug in HTTPSConnectionWithTimeout.connect</li>
<li><a href=""https://github.com/httplib2/httplib2/commit/0d490f692ca5f73cd2caaf9cc5e42e9f7702e453""><code>0d490f6</code></a> travis says matrix is alias for jobs now</li>
<li>Additional commits viewable in <a href=""https://github.com/httplib2/httplib2/compare/v0.12.0...v0.18.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=httplib2&package-manager=pip&previous-version=0.12.0&new-version=0.18.0)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/move-coop/parsons/network/alerts).

</details>","[Label(name=""dependency update"")]",[],0
938,Create Upsert Method that is Redshift to Redshift,https://api.github.com/repos/move-coop/parsons/issues/253,elyse-weiss,253,open,2020-05-20 13:25:34+00:00,2023-02-21 19:21:50+00:00,,"Sometimes I want to do an upsert from an existing Redshift table, rather than a Parsons table, and pulling data back into Parsons seems silly since the staging table already exists.","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""connector update""), Label(name=""needs finishing"")]",[],1
939,Fix iso_to_unix function,https://api.github.com/repos/move-coop/parsons/issues/252,Daniel,252,closed,2020-05-19 21:58:06+00:00,2021-03-29 19:06:57+00:00,2020-05-20 19:35:42+00:00,There was an issue where the test wasn't passing locally but was passing on the CI server. It appeared to be related to timezones. This fix adds support for timezones and sets the default timezone to UTC. Tests passed locally.,[],[],0
940,Adding Format() capability to the GoogleSheets class in parsons,https://api.github.com/repos/move-coop/parsons/issues/251,,251,closed,2020-05-19 17:58:36+00:00,2020-06-22 18:31:16+00:00,2020-06-22 18:31:16+00:00,"This issue is for adding more capabilities to the google sheets class -- 
Most specifically it would be great to be able to leverage the [`format()` function](https://gspread.readthedocs.io/en/latest/api.html#gspread.models.Worksheet.format) in the GoogleSheets() class in parsons. ","[Label(name=""enhancement""), Label(name=""high priority""), Label(name=""connector update"")]","[NamedUser(login=""jburchard"")]",0
941,Add alter_table functionality to upsert,https://api.github.com/repos/move-coop/parsons/issues/250,,250,closed,2020-05-19 00:55:14+00:00,2020-06-11 16:49:32+00:00,2020-06-11 16:49:32+00:00,"This would probably involve running `alter_varchar_column_widths()` at some point early on in `upsert()`, after it checks that the table already exists.","[Label(name=""enhancement""), Label(name=""good first issue"")]","[NamedUser(login=""SorenSpicknall"")]",0
942,Slack: Handle rate limiting and binary files,https://api.github.com/repos/move-coop/parsons/issues/249,Daniel,249,closed,2020-05-18 22:15:14+00:00,2021-03-29 19:06:38+00:00,2020-05-19 23:01:50+00:00,"* The connector will now automatically retry if it is rate limited. It uses the `Retry-After` header to determine how long to wait.

* The connector can now upload/post files that are binary (e.g. `pdf`, `jpg`, etc). To do so, pass `is_binary=True`. 

```python
...
slack.upload_file(['some_channel'], 'myfile,pdf', is_binary=True):
```",[],[],0
943,Better SFTP 'helpers',https://api.github.com/repos/move-coop/parsons/issues/248,Schuyler Duveen,248,closed,2020-05-18 21:29:07+00:00,2021-07-21 21:42:33+00:00,2021-07-21 21:42:33+00:00,"I have a few use-cases where some 'helper-ish' functions or restructuring in the SFTP driver would be useful.  Some use-cases:

* Using a connection for several steps at once (e.g. making create_connection() public and maybe each method takes an optional conn= argument)
* File and directory patterns -- instead of dir listing, then file listing, it would be nice to pass some regex patterns for a directory and file name, and then either download all files that match or at least list all the files that match.
* When downloading a file, sometimes there are some additional processing steps, but mostly after the steps, there's a CSV to turn into a table -- it would be nice to get a function (possibly with a lambda processing arg in-between), that downloads and returns the Table object straight-away","[Label(name=""medium priority""), Label(name=""connector update"")]","[NamedUser(login=""tiburona"")]",4
944,Add PDI class,https://api.github.com/repos/move-coop/parsons/issues/247,Daniel,247,closed,2020-05-18 21:07:54+00:00,2021-03-29 19:06:29+00:00,2020-05-22 19:54:38+00:00,"The PDI class implements a connector to [PDI's API](https://api.bluevote.com/docs/index#). The class implements the following endpoints:
- FlagIDs
- Flags
- Universes
- Questions
- AcquisitionTypes 

~Additionally, it appears that PDI has deprecated a few of them.~ PDI has deprecated some endpoints, none of which are implemented by this class.

The local tests have passed. Live tests were tested about a month ago and passed (I no longer have an api key so I can't test them again).",[],"[NamedUser(login=""SorenSpicknall"")]",3
945,bump version to v0.13.1,https://api.github.com/repos/move-coop/parsons/issues/246,Eliot Stone,246,closed,2020-05-15 20:57:45+00:00,2020-05-15 20:57:58+00:00,2020-05-15 20:57:54+00:00,,[],[],0
946,HOTFIX: Add __init__.py to ActionNetwork connector,https://api.github.com/repos/move-coop/parsons/issues/245,Soren Spicknall,245,closed,2020-05-15 20:22:27+00:00,2020-05-22 14:22:55+00:00,2020-05-15 20:32:54+00:00,,[],[],0
947,add ActionNetwork to docs/index.rst,https://api.github.com/repos/move-coop/parsons/issues/244,Eliot Stone,244,closed,2020-05-14 17:23:20+00:00,2020-05-14 17:41:19+00:00,2020-05-14 17:41:16+00:00,,[],[],0
948,bump version to v0.13.0,https://api.github.com/repos/move-coop/parsons/issues/243,Eliot Stone,243,closed,2020-05-14 13:15:54+00:00,2020-05-14 15:49:33+00:00,2020-05-14 15:49:31+00:00,,[],[],0
949,HOTFIX: make Freshdesk transform_table function a passthrough when no data is present,https://api.github.com/repos/move-coop/parsons/issues/242,Soren Spicknall,242,closed,2020-05-12 15:20:50+00:00,2020-05-12 20:29:27+00:00,2020-05-12 15:56:08+00:00,"In any code that uses the current Freshdesk connector, evaluation of an empty table after running one of the getter functions will fail. This is because when the table is created, transform_table is called. That function should only move or expand columns when data is present, since there is no explicit data model to rely upon in this connector. With no data present, the function attempts to move standard columns that aren't there.",[],[],0
950,Zoom: Initial Methods,https://api.github.com/repos/move-coop/parsons/issues/241,,241,closed,2020-05-09 16:10:07+00:00,2020-05-13 19:06:26+00:00,2020-05-13 19:06:22+00:00,"This initial class includes the following methods:

- `get_users()`
- `get_meetings()`
- `get_past_meetings()`
- `get_past_meeting_participants()`

Future PRs will address webinars.

Addresses #218 and #191 ",[],[],0
951,GoogleSheets: Share A Spreadsheet,https://api.github.com/repos/move-coop/parsons/issues/240,,240,closed,2020-05-07 04:42:56+00:00,2020-05-10 03:41:45+00:00,2020-05-08 13:22:44+00:00,"Adds the following methods:

- ``get_spreadsheet_permissions()``
- ``share_spreadsheet()``

Addresses Issue #238 ",[],[],0
952,Create Parsons Mailchimp connector,https://api.github.com/repos/move-coop/parsons/issues/239,Soren Spicknall,239,closed,2020-05-06 20:08:28+00:00,2020-05-14 13:14:25+00:00,2020-05-14 13:14:21+00:00,,[],"[NamedUser(login=""eliotst""), NamedUser(login=""elyse-weiss"")]",3
953,GoogleSheets() class ability to share sheet,https://api.github.com/repos/move-coop/parsons/issues/238,,238,closed,2020-05-06 18:53:16+00:00,2020-05-08 13:35:22+00:00,2020-05-08 13:35:22+00:00,"The GoogleSheets() parsons class doesn't currently provide the capability to just share a sheet with an individual nor make the sheet readable, or editable by anyone. Looks like gspread has the capability to do so via `share` -- more info here: https://gspread.readthedocs.io/en/latest/api.html#gspread.models.Spreadsheet.share and it would be great to just have the capacity to do that via parsons too. 

I am most interested in being able to quickly make a sheet viewable by anyone with the link after it's created.  ",[],[],0
954,GoogleSheets: Fix Append Rows Bug,https://api.github.com/repos/move-coop/parsons/issues/237,,237,closed,2020-05-05 20:01:00+00:00,2020-05-07 06:07:38+00:00,2020-05-05 22:17:49+00:00,"This PR addresses Issue #234 in which empty sheets would fail if you attempted to append data to them.

- Fixed `append_to_sheet()` bug.
- Did a little bit of minor tidying of the class structure.",[],[],0
955,Action network,https://api.github.com/repos/move-coop/parsons/issues/235,Kevin O'Sullivan,235,closed,2020-05-05 04:35:26+00:00,2020-10-26 17:26:53+00:00,2020-05-13 12:53:31+00:00,,[],"[NamedUser(login=""SorenSpicknall"")]",1
956,Google Sheets append_to_sheet() handling empty sheets,https://api.github.com/repos/move-coop/parsons/issues/234,,234,closed,2020-05-04 20:30:06+00:00,2020-05-07 04:06:05+00:00,2020-05-07 04:06:05+00:00,"I was attempting to use the append_to_sheet() function to insert a parsons table into a blank worksheet and was met with the following full error text:

> ---------------------------------------------------------------------------
> IndexError                                Traceback (most recent call last)
> <ipython-input-63-7eb3b1673b96> in <module>
> ----> 1 full_test_sheet=gs.append_to_sheet('1bVi6pKbqJiDBvWrg84_LEKNDT6K_U93y40JqLCm7_oQ', test_list, user_entered_value=True)
> 
> ~/Environments/parsons/lib/python3.7/site-packages/parsons/google/google_sheets.py in append_to_sheet(self, spreadsheet_id, table, sheet_index, user_entered_value)
>     188         # TODO Figure out a way to do a batch append without having to read the whole sheet first.
>     189         # Maybe use gspread's low-level batch_update().
> --> 190         existing_table = self.read_sheet(spreadsheet_id, sheet_index)
>     191 
>     192         cells = []
> 
> ~/Environments/parsons/lib/python3.7/site-packages/parsons/google/google_sheets.py in read_sheet(self, spreadsheet_id, sheet_index)
>      85 
>      86         sheet = self._get_sheet(spreadsheet_id, sheet_index)
> ---> 87         records = sheet.get_all_records()
>      88 
>      89         return Table(records)
> 
> ~/Environments/parsons/lib/python3.7/site-packages/gspread/models.py in get_all_records(self, empty2zero, head, default_blank, allow_underscores_in_numeric_literals)
>     627 
>     628         data = self.get_all_values()
> --> 629         keys = data[idx]
>     630         values = [
>     631             numericise_all(
> 
> IndexError: list index out of range

I wrote some text in the sheet, and re-ran the line and it completed successfully.

I then deleted all the text from the sheet and changed the line to overwrite_sheet() and the line completed successfully. 

It would be great if either: 
1. the append_to_sheet() function worked on blank sheets 
OR 
2. if the documentation about append_to_sheet() could be updated to reflect that the function does not work on a blank sheet and to update the error text to be more descriptive  ","[Label(name=""documentation""), Label(name=""enhancement"")]",[],0
957,ActionKit Updates,https://api.github.com/repos/move-coop/parsons/issues/233,Melissa Woods,233,closed,2020-05-04 20:14:39+00:00,2020-07-17 22:28:58+00:00,2020-07-17 22:28:58+00:00,"This issue is for building out functions to update events and signups in the ActionKit class. API documentation for these is:

- [Updating an event](https://roboticdogs.actionkit.com/docs/manual/api/rest/examples/updateevent.html)
- [Updating a signup](https://roboticdogs.actionkit.com/docs/manual/api/rest/examples/updatesignup.html)","[Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""connector update"")]",[],1
958,bigquery: DB sync,https://api.github.com/repos/move-coop/parsons/issues/232,Eliot Stone,232,closed,2020-05-04 00:49:24+00:00,2020-05-04 19:39:33+00:00,2020-05-04 19:39:29+00:00,"This commit updates the Parsons BigQuery connector to support DB
syncs to/from BigQuery. In order to make this change, the commit
also modifies the `query` method in the connector to use the
BigQuery
[DB-API](https://googleapis.dev/python/bigquery/latest/dbapi.html)
interface.

The commit also updates the BigQuery connector API to
require users to qualify their table names with the name of the
dataset, so align the BigQuery API with the more standard SQL
connectors (Redshift, MySQL).

This commit also makes a few minor changes to the DB sync class
to fix some small bugs, namely:
 * Add the `table` method to MySQL to allow using MySQL for with
 `DBSync`
 * Ensure that the connection and cursor in the Redshift connector
 are closed, even if the query raises an exception.
 * Fixes a bug where the DB sync API wouldn't support incremental
 table syncs where the primary key wasn't an integer.
 * Work around a limitation in BigQuery where the `autodetect`
 of a copy job would confuse headers for a data row in the case
 where the table data was all strings.
 * Fix DB Sync `table_sync_incremental` so it works even if the
 destination table is empty.",[],[],0
959,Contribution Methods in VAN class,https://api.github.com/repos/move-coop/parsons/issues/231,,231,open,2020-04-30 18:41:28+00:00,2023-02-21 19:26:02+00:00,,"For EveryAction users with any kind of Fundraising package to be able add, delete, and modify contributions.

Not sure if this should also include Designation and Financial Batch endpoints.","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],1
960,Update twilio.py,https://api.github.com/repos/move-coop/parsons/issues/230,Frydafly,230,closed,2020-04-30 00:13:02+00:00,2020-04-30 00:22:34+00:00,2020-04-30 00:22:34+00:00,"fixed typo of word ""specfic"" and changed to ""specific""",[],[],0
961,GitHub Connector,https://api.github.com/repos/move-coop/parsons/issues/229,Eliot Stone,229,closed,2020-04-28 23:42:20+00:00,2020-07-30 14:30:54+00:00,2020-07-30 14:30:54+00:00,"As the Parsons repository sees more usage and contributions, it would be great to be able to better report on things like outside contributions and automatically track progress on Parsons issues. To start, we would love to be able to pull down data about Pull Requests and Issues.","[Label(name=""medium priority""), Label(name=""new connector"")]",[],3
962,Rock the Vote: Upload registrations,https://api.github.com/repos/move-coop/parsons/issues/228,Eliot Stone,228,open,2020-04-28 23:37:20+00:00,2023-02-21 19:27:56+00:00,,"We just added an RTV connector class, but right now it only supports downloading data from the API. It would be great to add a method to the class to upload registrations to the Rock the Vote API.","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""low priority""), Label(name=""connector update"")]",[],2
963,Add RTV to index,https://api.github.com/repos/move-coop/parsons/issues/227,,227,closed,2020-04-27 19:58:30+00:00,2020-04-30 00:23:13+00:00,2020-04-27 22:01:04+00:00,Adding Rock The Vote documentation to docs index.,[],[],0
964,EveryAction/VAN Target Export Jobs,https://api.github.com/repos/move-coop/parsons/issues/226,Matt Brooks,226,closed,2020-04-24 19:52:13+00:00,2020-06-18 20:04:58+00:00,2020-06-18 20:04:58+00:00,"EveryAction has added an endpoint for Target Export Jobs:

https://developers.everyaction.com/van-api#target-export-jobs

This is a feature request to add target export creation/retrieval to Parsons.",[],[],2
965,add the Rock the Vote connector,https://api.github.com/repos/move-coop/parsons/issues/225,Eliot Stone,225,closed,2020-04-24 13:16:44+00:00,2020-04-27 17:50:46+00:00,2020-04-27 17:50:42+00:00,"This commit adds the `RockTheVote` connector that wraps around the
Rock the Vote's Rocky API. This commit only implements endpoints
for requesting registrant reports, which is a report to list out
registration records for a given partner.

---
Fixes #119 ",[],[],0
966,Facebook Marketing API Connector,https://api.github.com/repos/move-coop/parsons/issues/224,Eliot Stone,224,open,2020-04-21 23:13:51+00:00,2023-02-21 19:28:21+00:00,,"Parsons has an existing Facebook connector for generating custom audiences from people in a Parsons table. However, don't have anything in Parsons to let users download reporting data about the ads they are running. 

The [Facebook Insights API](https://developers.facebook.com/docs/marketing-api/reference/ads-insights/) provides this data, so we should add a Parsons connector to integrate with the Insights API.

As well, the Marketing API supports tracking conversations from [Facebook pixels](https://developers.facebook.com/docs/marketing-api/audiences-api/pixel). 

Probably to start, we'd want methods that could help us answer the questions:
- how many people have viewed ad X on Y day
- how many people have clicked ad X on Y day
- analyzing the above by demographic variable","[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""low priority""), Label(name=""new connector"")]",[],0
967,Redshift.upsert: default to compupdate=False because compression isn't useful for temp tables,https://api.github.com/repos/move-coop/parsons/issues/223,Schuyler Duveen,223,closed,2020-04-20 21:22:44+00:00,2020-04-21 17:14:40+00:00,2020-04-21 17:14:39+00:00,"Our Redshift CPU was starting to get quite stressed when we deployed something with parsons' redshift.upsert.  After investigation, it seemed that the default of compupdate=True in the `copy` command was causing very slow analysis (even with several thousand rows) -- but especially with tables with a lot of *columns* (ours had 109).

It seems that especially for temporary tables, like in upsert, leaving compupdate=True is unlikely to be helpful, since the table will be deleted soon.

I'm shy about suggesting that the default in `.copy()` change, but it seems that there are a lot of 'upload a temporary-ish table' that `.copy()` is/might be used for where compupdate=False is the better option/default.  Maybe, if nothing else, it would be worth documenting.

cc: @ydamit (after discussion)",[],[],1
968,resolve requirements.txt conflicts,https://api.github.com/repos/move-coop/parsons/issues/222,Eliot Stone,222,closed,2020-04-20 20:50:13+00:00,2020-04-21 02:58:00+00:00,2020-04-21 02:57:52+00:00,"This commit fixes some issues with version conflicts in the
requirements.txt file for Parsons.
 * Changes version for `mysql-connector-python` from 8.0.19 to
   8.0.18 to resolve an issue with a hardcoded version number for
   the protobuf library.
 * Removes google-auth, google-auth-httplib2, and
   google-auth-oauthlib from requirements.txt, since they are all
   installed by other libraries used by Parsons.

cc @ydamit ",[],[],0
969,Evaluating Max Col Lenth on Byte Length,https://api.github.com/repos/move-coop/parsons/issues/221,,221,closed,2020-04-20 17:02:31+00:00,2020-04-21 17:59:38+00:00,2020-04-21 17:59:32+00:00,Hopefully this helps out Redshift's `alter_table` as well!,[],"[NamedUser(login=""eliotst"")]",0
970,VAN update events,https://api.github.com/repos/move-coop/parsons/issues/220,Melissa Woods,220,open,2020-04-20 13:42:46+00:00,2023-02-21 19:30:53+00:00,,"Currently there isn't a mechanism in Parsons to update existing events in VAN, but an endpoint does exist in the VAN API. This ticket is a request to add that to Parsons.

[Documentation here.](https://developers.everyaction.com/van-api#events-put-events--eventid)","[Label(name=""enhancement""), Label(name=""medium priority""), Label(name=""connector update"")]",[],1
971,Action Network Connector v1,https://api.github.com/repos/move-coop/parsons/issues/219,,219,closed,2020-04-16 19:38:55+00:00,2020-05-14 13:15:15+00:00,2020-05-14 13:15:15+00:00,"Initial methods would focus on CRUD of individual records. 

[API docs](https://actionnetwork.org/docs) could be found here","[Label(name=""enhancement"")]","[NamedUser(login=""SorenSpicknall"")]",1
972,Zoom Connector: Finish Unit Tests,https://api.github.com/repos/move-coop/parsons/issues/218,,218,closed,2020-04-16 19:34:27+00:00,2020-08-28 16:32:43+00:00,2020-05-14 13:14:55+00:00,"An initial v1 of a Zoom connector. This is 50% of the way there on [a stale branch](https://github.com/move-coop/parsons/compare/zoom) but is lacking unit tests. It is also a very stale branch, so it might make sense to begin a new one.

- `get_users`
- `get_webinars`
- `get_webinar_participants`





","[Label(name=""enhancement"")]",[],0
973,Box Connector v1,https://api.github.com/repos/move-coop/parsons/issues/217,,217,closed,2020-04-16 19:29:33+00:00,2020-10-16 19:28:26+00:00,2020-10-16 19:28:26+00:00,"This would serve as the initial connector for interacting with the Box API. There is a [robust SDK](https://github.com/box/box-python-sdk) that might be a good starting point.

- List folders
- List files
- Get file
- Save file to box","[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""new connector"")]","[NamedUser(login=""davidpablocohn"")]",4
974,add newmode back in,https://api.github.com/repos/move-coop/parsons/issues/216,Eliot Stone,216,closed,2020-04-10 17:38:04+00:00,2020-04-10 19:10:19+00:00,2020-04-10 19:10:15+00:00,"We wanted to do some checking out of the New/Mode external
library before we accepted it as a depency on parsons. After
reviewing it, we are adding it back in.",[],[],0
975,don't import newmode,https://api.github.com/repos/move-coop/parsons/issues/215,Eliot Stone,215,closed,2020-04-10 17:16:40+00:00,2020-04-10 17:22:58+00:00,2020-04-10 17:22:46+00:00,Forgot to also stop importing the `Newmode` client when I removed the requirement.,[],[],0
976,temporarily disable newmode requirement,https://api.github.com/repos/move-coop/parsons/issues/214,Eliot Stone,214,closed,2020-04-10 15:59:21+00:00,2020-04-10 16:04:34+00:00,2020-04-10 16:04:31+00:00,,[],[],0
977,redash: bugfix logger.debug() call to process args correctly,https://api.github.com/repos/move-coop/parsons/issues/213,Schuyler Duveen,213,closed,2020-04-09 18:31:34+00:00,2020-04-09 19:36:35+00:00,2020-04-09 19:36:35+00:00,"without %s inclusions, logger.debug throws an exception but only when logger.setLevel(""DEBUG"")",[],[],0
978,etl: convert_columns_to_str handle no types,https://api.github.com/repos/move-coop/parsons/issues/212,Eliot Stone,212,closed,2020-04-09 17:04:03+00:00,2020-04-09 17:44:51+00:00,2020-04-09 17:44:48+00:00,"This commit updates the `convert_columns_to_str` method in the
Parsons `Table` handle a case where `petl` was returning an
empty list for the types from the `typeset` function. The
Parsons code had assumed that the list returned from `typeset`
would always be non-empty, so when an empty value was returned
the code raised an index out of bounds exception.

It looks like the reason `typeset` was returning an empty list
of types is that the `Table` in question didn't have any rows,
so this commit exits early from the function if there's no data.
Also, to be safe, the commit updates the conditional check to
short circuit if there is less than one (no types) or more than
one (multiple types) value in the list.",[],[],0
979,Add MySQL to DB Sync documentation.,https://api.github.com/repos/move-coop/parsons/issues/211,,211,closed,2020-04-09 13:48:43+00:00,2020-04-30 00:23:15+00:00,2020-04-09 14:11:49+00:00,,[],[],0
980,bump version to 0.12,https://api.github.com/repos/move-coop/parsons/issues/210,Eliot Stone,210,closed,2020-04-08 19:34:34+00:00,2020-04-08 20:40:08+00:00,2020-04-08 20:39:53+00:00,,[],[],0
981,action_kit.bulk_upload_table: create an option not to overwrite on empty values,https://api.github.com/repos/move-coop/parsons/issues/209,Schuyler Duveen,209,closed,2020-04-08 15:10:13+00:00,2020-04-09 19:37:00+00:00,2020-04-09 19:37:00+00:00,"This splits up a table into several tables based on empties.

Changes the API slightly -- return value is a dict with success:  but then an array of ""results"".  The API is sufficiently new, that I think this should be ok, but we can change it conditionally based on the option, if we want to preserve full backwards compatibility.

bikesheds on what to name the parameter welcome :-)",[],[],2
982,Hustle: Return Lead Ids,https://api.github.com/repos/move-coop/parsons/issues/208,,208,closed,2020-04-08 02:30:51+00:00,2020-04-30 00:23:20+00:00,2020-04-08 21:17:55+00:00,Enhancement to `Hustle.create_leads()` method that returns a table of newly created lead_ids when creating new leads in the system.,[],[],1
983,added describe and describe fields along with tests,https://api.github.com/repos/move-coop/parsons/issues/207,,207,closed,2020-04-07 14:47:21+00:00,2020-04-08 12:42:29+00:00,2020-04-08 12:42:21+00:00,@retacg helped me with this!,[],"[NamedUser(login=""jburchard"")]",0
984,"Improve template table schema use, add testing for Redshift upsert",https://api.github.com/repos/move-coop/parsons/issues/206,Soren Spicknall,206,closed,2020-04-06 19:14:40+00:00,2020-04-08 12:53:56+00:00,2020-04-08 12:53:53+00:00,"Working off of discussion in #205, I changed the query used to generate a matching DDL for a specific template table in the Redshift copy function. Then, I added some testing to cover a case where this functionality is used: when an incoming Parsons table has different datatypes than the destination table.",[],"[NamedUser(login=""eliotst"")]",1
985,Improve Redshift upsert staging table column types,https://api.github.com/repos/move-coop/parsons/issues/205,Soren Spicknall,205,closed,2020-04-06 15:14:44+00:00,2020-04-20 19:37:07+00:00,2020-04-06 18:56:52+00:00,"When a destination table already exists, this added code ensures that the staging table(s) created during a Redshfit upsert have columns that mirror those of the destination table. This helps avoid issues with copying data from table to table further on down the line.",[],[],5
986,Redshift: Check and Fix Duplicate Column Names In A Table,https://api.github.com/repos/move-coop/parsons/issues/204,elyse-weiss,204,closed,2020-04-05 21:42:54+00:00,2020-07-28 19:14:13+00:00,2020-07-28 19:14:13+00:00,"![image](https://user-images.githubusercontent.com/29580051/78510618-9a74c400-7764-11ea-850c-a1b579cbd010.png)

I had a loop where I used `to_redshift()` with a Parsons Table. The Parsons Table had two columns of the same name, so the `to_redshift()` command failed. However, that failure was never logged anywhere.","[Label(name=""bug""), Label(name=""medium priority"")]","[NamedUser(login=""SorenSpicknall"")]",0
987,Hustle - return lead id for create_lead and create_leads,https://api.github.com/repos/move-coop/parsons/issues/202,Melissa Woods,202,closed,2020-04-03 14:11:02+00:00,2020-04-08 21:18:06+00:00,2020-04-08 21:18:06+00:00,Currently the Hustle commands `create_lead` and `create_leads` return `None`. This ticket is a request to return the ids of the leads that are created with either one.,[],[],0
988,--,https://api.github.com/repos/move-coop/parsons/issues/201,Eliot Stone,201,closed,2020-04-03 14:10:54+00:00,2020-04-09 00:55:03+00:00,2020-04-03 14:47:47+00:00,,[],[],0
989,Hustle Incremental Load of Data,https://api.github.com/repos/move-coop/parsons/issues/200,Eliot Stone,200,closed,2020-04-03 14:10:46+00:00,2020-04-03 14:47:54+00:00,2020-04-03 14:47:54+00:00,,[],[],0
990,Allow redshift.upsert to pass authentication and other copy arguments,https://api.github.com/repos/move-coop/parsons/issues/199,Schuyler Duveen,199,closed,2020-04-01 17:57:17+00:00,2020-04-06 01:28:23+00:00,2020-04-06 01:28:22+00:00,"This also adds a few arguments to redshift.copy for better/easier authentication, debugging and table creation.
iam_role allows copying from S3 with an IAM role assigned to the Redshift bucket.
This keeps authentication scope to what has been given to the Redshift instance.

We also add template_table argument -- The default table schema creation based on the data
could fail if column casting was required.  In theory, passing more copy arguments (now possible!
) would be sufficient, but this way, more can be done ""automatically"" -- i.e. date/int/etc columns
will be loaded and automatically cast by redshift itself.

This could change the current behavior slightly -- errors might occur during S3 copy instead of
during the upsert -- however, this is probably a good thing:  Currently, if there are errors on
import they would occur after deletion of existing rows.  Even within a transaction, it seems
better to ""fail early"".",[],[],0
991,NGPVAN: Fix Create Code,https://api.github.com/repos/move-coop/parsons/issues/198,,198,closed,2020-03-31 20:57:06+00:00,2020-04-30 00:23:23+00:00,2020-03-31 22:42:01+00:00,Fixes a bug with VAN.create_code(),[],[],0
992,Add New/Mode integration,https://api.github.com/repos/move-coop/parsons/issues/197,,197,closed,2020-03-31 15:46:02+00:00,2020-04-10 15:42:46+00:00,2020-04-10 15:42:45+00:00,Adding a new integration with New/Mode's API.,[],[],4
993,braintree initial support with transactions/disputes,https://api.github.com/repos/move-coop/parsons/issues/196,Schuyler Duveen,196,closed,2020-03-26 15:05:20+00:00,2020-04-20 00:51:30+00:00,2020-04-20 00:51:29+00:00,"TODO:
 - [x] documentation
 - [x] tests
 - [x] adding some more logging",[],[],5
994,Redshift: DIST/SORT key nudge.,https://api.github.com/repos/move-coop/parsons/issues/195,,195,closed,2020-03-24 17:15:55+00:00,2020-09-02 01:51:24+00:00,2020-09-02 01:51:24+00:00,We should include a logger warning in the Redshift copy that alerts the user is there is not a DIST/SORT key association with their copy statement. This could nudge folks towards utilizing Redshift best practices.,"[Label(name=""good first issue""), Label(name=""housekeeping/meta""), Label(name=""medium priority"")]",[],6
995,Redshift: Add DIST/SORT keys to upsert.,https://api.github.com/repos/move-coop/parsons/issues/194,,194,closed,2020-03-24 17:14:21+00:00,2022-03-17 17:55:25+00:00,2022-03-17 17:55:24+00:00,"In order to speed up the upserts, we should add the `pk` as the distribution and the sort key.","[Label(name=""enhancement""), Label(name=""housekeeping/meta"")]",[],1
996,DBSync: Add BigQuery Support,https://api.github.com/repos/move-coop/parsons/issues/193,,193,closed,2020-03-24 17:08:42+00:00,2020-05-09 21:12:43+00:00,2020-05-09 21:12:43+00:00,Add support to DBSync for BigQuery.,"[Label(name=""enhancement"")]",[],0
997,Cookbook Documentation,https://api.github.com/repos/move-coop/parsons/issues/192,,192,closed,2020-03-24 17:06:22+00:00,2022-03-17 17:56:04+00:00,2022-03-17 17:56:04+00:00,We should have a section that includes scripts that tie various classes together for full workflows.,"[Label(name=""documentation""), Label(name=""high priority"")]",[],1
998,Zoom Connector,https://api.github.com/repos/move-coop/parsons/issues/191,,191,closed,2020-03-24 17:05:17+00:00,2020-05-14 13:15:04+00:00,2020-05-14 13:15:04+00:00,"A Zoom connector MVP that allows for the following methods:

`get_users()`
`get_webinars()`
`get_webinar_attendees()`","[Label(name=""enhancement"")]","[NamedUser(login=""jburchard"")]",0
999,Merge Redshift and PostgresCore,https://api.github.com/repos/move-coop/parsons/issues/190,,190,closed,2020-03-24 17:03:27+00:00,2022-03-17 17:56:17+00:00,2022-03-17 17:56:17+00:00,"The methods are largely overlapping -- given that Redshift is an offshoot of Postgres -- so we should merge the code base, where possible to reduce lines of code.","[Label(name=""housekeeping/meta""), Label(name=""medium priority"")]",[],1
1000,Handle nonexistent target table during upsert,https://api.github.com/repos/move-coop/parsons/issues/189,Soren Spicknall,189,closed,2020-03-23 16:56:50+00:00,2020-03-24 19:38:12+00:00,2020-03-24 19:38:06+00:00,"Added functionality to catch a failed upsert due to a missing target table, roll back that query, and transfer the entirety of the staging table to a newly-created target table.",[],[],1
1001,actionkit bulk_user_upload with user_fields_only optimization,https://api.github.com/repos/move-coop/parsons/issues/188,Schuyler Duveen,188,closed,2020-03-20 19:18:47+00:00,2020-03-23 04:06:31+00:00,2020-03-23 04:06:31+00:00,"This preserves an optimization which ActionKit has for user_field-only uploads.

From @jburchard 's flag of max-ing out memory, I switch away from using the optimization if the record length exceeds a million.  Happy to use another arbitrary number to balance parson's users -- maybe something worth documenting and/or deciding in-terms of when/what to keep in-memory.

Also, I'm dribbling out changes, as I start to use my own e.g. past PR -- lmk, if it's better to 'fully bake' things.  This is a pretty low-priority change, so no rush on review.",[],[],0
1002,MySQL: Create Table,https://api.github.com/repos/move-coop/parsons/issues/187,,187,closed,2020-03-18 15:15:34+00:00,2020-03-25 19:13:09+00:00,2020-03-24 16:01:07+00:00,"Adds the `MySQL.copy()` method similar to `Redshift` and `Postgres` in which the table is automatically created in the database if it does not already exist.

Of note, the copy method utilizes the MySQL multi-insert functionality rather than `LOAD DATA`. While this introduces a slight performance hit (less so than other DBs), there are security concerns with `LOAD DATA LOCAL`. Additionally, many configurations of MySQL DBs don't allow `LOAD DATA` by default.",[],[],1
1003,AWS-based framework to distribute table processing across Lambda invocations,https://api.github.com/repos/move-coop/parsons/issues/186,Schuyler Duveen,186,closed,2020-03-16 14:54:36+00:00,2020-04-20 00:52:55+00:00,2020-04-20 00:52:55+00:00,"The first call by divides up the table, uploading it to s3, and invoking lambdas to process portions of the data requesting specific data ranges in subsequent requests.

This is some code we used internally, and I'm adapting for within parsons.  I haven't tested it yet, but would like some early feedback on the approach:
* Leveraging the (undocumented) norm of putting `table` arguments first, this assumes there will be many methods that take a table as first argument and can process a table into actions/events/records in whatever system we are syncing to.
* I added support for class instantiation across Lambda invocations, since that is also a persistent pattern in Parsons -- however, it adds some complexity to the documented arguments and code.
* While the general pattern of class instantiation is good for services, I'm not quite convinced that the `bucket` argument should necessitate the same model -- especially when I can add check_env() for the bucket, it seems overkill to make a class, when Lambda will have all the other properties
* I was pretty shy about adding [Zappa](https://github.com/Miserlou/Zappa) as a dependency (do unto others...) -- while we may (or may not) make it a dependency ourselves (MoveOn), it seems better to keep it an optional dependency and to move over a few shims -- all the more so to add the class-supporting/instantiating arguments.

While this is pretty specific 'environment' code, I'd like to encourage use for those that are on AWS already.  It makes Lambda a more compelling (and friendly) platform for processing datasets -- even larger ones -- a lot cheaper than running an ec2 instance in most cases.",[],"[NamedUser(login=""jburchard""), NamedUser(login=""eliotst"")]",2
1004,fix redash query_api_key accidentally set to query_id,https://api.github.com/repos/move-coop/parsons/issues/185,Schuyler Duveen,185,closed,2020-03-14 14:14:54+00:00,2020-03-14 15:59:54+00:00,2020-03-14 15:59:54+00:00,embarrassing typo from the initial PR.,[],[],0
1005,MySQL: Add Table Object,https://api.github.com/repos/move-coop/parsons/issues/184,,184,closed,2020-03-14 04:53:42+00:00,2020-03-23 16:53:48+00:00,2020-03-18 15:08:04+00:00,This PR creates the MySQL Table object. This is needed in order to eventually add MySQL DBSync functionality.,[],[],0
1006,Remove test files.,https://api.github.com/repos/move-coop/parsons/issues/183,,183,closed,2020-03-13 18:00:48+00:00,2020-03-13 18:57:49+00:00,2020-03-13 18:57:46+00:00,Two test files -- no real data -- snuck into the repo. This removes them.,[],[],0
1007,Fixed API error from increasing max input,https://api.github.com/repos/move-coop/parsons/issues/182,,182,closed,2020-03-13 17:11:33+00:00,2020-03-15 15:21:49+00:00,2020-03-13 18:54:53+00:00,"Sorry, last change, I promise - there was an API error from the `max` input being incremented along with `start` when it should have been left alone.",[],[],0
1008,Actionkit bulk upload,https://api.github.com/repos/move-coop/parsons/issues/181,Schuyler Duveen,181,closed,2020-03-13 17:06:15+00:00,2020-03-13 21:36:27+00:00,2020-03-13 21:36:27+00:00,Support uploading batch uploads from a Table or csv to ActionKit (as documented at https://roboticdogs.actionkit.com/docs/manual/api/rest/uploads.html),[],[],1
1009,Redshift: Alter table width on Redshift.copy(),https://api.github.com/repos/move-coop/parsons/issues/180,,180,closed,2020-03-13 16:13:52+00:00,2020-03-25 19:13:10+00:00,2020-03-21 01:55:40+00:00,"This PR does the following:

- Improves the performance of `Redshift.copy()`. Previously, it was leveraging the `Redshift.copy_s3()` method, which - when creating a new table - was causing it to download the file from S3 an additional time to scan the data. Now it scans the data prior to uploading to S3.

- Add an `alter_table` argument to Redshift.copy() which will alter the width of a target table columns if they are too narrow for the incoming data. This should significantly reduce failures on copy. Unfortunately, if the table has an associated dependent view, this will fail. Hence, for now, the default will be `False`, but we will consider shifting that in the future.

",[],[],0
1010,Adding Pagination to List Methods,https://api.github.com/repos/move-coop/parsons/issues/179,,179,closed,2020-03-12 22:03:00+00:00,2020-03-13 16:48:58+00:00,2020-03-13 16:48:58+00:00,"By default, the `max` for list calls was set to 999, but it turns out there can be more than 999 of a thing. So I introduced a method to paginate results for list methods, and a test method for it as well.",[],[],0
1011,Add smtp notifications support and support StringIO/BytesIO attachments,https://api.github.com/repos/move-coop/parsons/issues/178,Schuyler Duveen,178,closed,2020-03-12 21:55:41+00:00,2020-03-17 23:09:16+00:00,2020-03-17 23:09:16+00:00,"Instead of sending email through Gmail, I'd like the same API for SMTP.  I'd also like to send StringIO/BytesIO objects, so files don't need to be saved to the local computer if they're generated in and already in-memory.

I haven't written any tests (nor have I even tested my smtp code myself) yet.  However, before doing so, I thought I'd get feedback on what I'm doing with the StringIO/BytesIO stuff -- in general, I'd love for more places to support in-memory files.  I don't have a grand-unified theory about this or anything -- it looks like we're pretty ad-hoc in different libraries.  While, the code is net-negative already for gmail.py, if you'd rather I move that kind of function to utilities/files.py I can do so -- also open to other thoughts.
",[],[],4
1012,bill_com: add missing __init__.py,https://api.github.com/repos/move-coop/parsons/issues/177,Eliot Stone,177,closed,2020-03-12 17:25:30+00:00,2020-03-12 18:45:39+00:00,2020-03-12 18:45:12+00:00,"A user found this. Not sure why we didn't hit it during the release testing. I can reproduce the error locally while still (in the same virtualenv) pass all of the tests. Python import paths are odd.

We will need to cut a new release (v0.11.1) after this is merged.",[],[],0
1013,MySQL Connector: Query,https://api.github.com/repos/move-coop/parsons/issues/176,,176,closed,2020-03-09 16:44:09+00:00,2020-03-13 16:06:53+00:00,2020-03-12 16:44:23+00:00,"Initial MySQL connector with the following two methods:

`MySQL.query()`
`MySQL.query_with_connection()`
",[],[],1
1014,bump version number,https://api.github.com/repos/move-coop/parsons/issues/175,Eliot Stone,175,closed,2020-03-09 00:46:34+00:00,2020-03-09 15:30:51+00:00,2020-03-09 15:30:48+00:00,,[],[],0
1015,Van people,https://api.github.com/repos/move-coop/parsons/issues/174,,174,closed,2020-03-08 22:40:59+00:00,2020-03-13 16:06:52+00:00,2020-03-09 15:20:18+00:00,"This PR is an attempt to allow for updating VAN records using VANIDs. This also necessitated cleaning up the `_person_search()` and `_valid_search()` methods, and resolving the confusion of how street name/number map to actual address data.

This was done in a way that hopefully remains backwards-compatible. Leaving the prospect of VAN ID as a named arg in the `upsert_person()` for a subsequent PR (`find_person()` may not require this since there's `get_person()`).

Successfully tested in the following scenarios:
- find_person() using match_map with `firstName`, `lastName`, `addressLine1`, `zipOrPostalCode`
- find_person() using the same match_map but with a different record's email in the `email` arg (to prove that match_map overrides)
- find_person() with `first_name`, `last_name`, `street_number`, `street_name`, and `zip` args (to prove that using args without match_map still works)
- find_person() using match_map that only has `vanId`
- upsert_person() using match_map that only has `vanId` and `middleName`
- upsert_person() using the same match_map but with a different record's email in the `email` arg (to prove that match_map overrides)
- upsert_person() with `first_name`, `last_name`, `street_number`, `street_name`, `zip`, and `phone` args (to update phone)",[],[],2
1016,VAN: Add Custom Fields Endpoint,https://api.github.com/repos/move-coop/parsons/issues/173,,173,closed,2020-03-08 17:47:21+00:00,2020-03-09 03:44:54+00:00,2020-03-08 23:44:09+00:00,"Adds the following methods:

`VAN.get_custom_field()` - Retrieve a single custom field object as a json.
`VAN.get_custom_fields()` - Retrieve all custom fields as a `Table` object.
`VAN.get_custom_fields_values()` - Retrieve a table of all custom fields values with their associated custom field id.

Also:

- Cleaned up some documentation for VAN.
- Fixed a bug for `VAN.get_code()`",[],[],0
1017,Redshift: Upsert shouldn't fail if destination table doesn't exist.,https://api.github.com/repos/move-coop/parsons/issues/172,,172,closed,2020-03-07 04:45:47+00:00,2020-04-07 16:49:27+00:00,2020-04-07 16:49:27+00:00,"When running an upsert, if the destination table does not exists, it currently fails. Instead, it should just run the `Redshift.copy()` method.",[],[],0
1018,Custom Fields endpoints in Parsons VAN,https://api.github.com/repos/move-coop/parsons/issues/171,,171,closed,2020-03-05 22:06:09+00:00,2020-03-08 23:44:30+00:00,2020-03-08 23:44:29+00:00,"VAN charges to add custom fields into Pipeline. Meanwhile, custom fields can be crucial to EveryAction use-cases, and loading a value to a custom field on a person record requires not only the field's numeric ID, but also its group's. Rather than requiring a sync's end users to have to find and submit both IDs as parameters, a single call to get all custom field data would allow comparison to a submitted custom field ID (or even custom field name).","[Label(name=""enhancement"")]",[],1
1019,support multiple primary keys for upsert,https://api.github.com/repos/move-coop/parsons/issues/170,Eliot Stone,170,closed,2020-03-03 18:41:05+00:00,2020-03-04 14:04:49+00:00,2020-03-04 14:04:46+00:00,"This commit updates the `Redshift` connector's `upsert` method to
allow users to specify multiple ""primary key"" columns to use for
deduplication when performing an upsert.",[],[],0
1020,VAN: Fix pagination for events.,https://api.github.com/repos/move-coop/parsons/issues/169,,169,closed,2020-02-28 16:55:08+00:00,2020-03-09 03:45:08+00:00,2020-03-05 18:00:24+00:00,The `VAN.get_events()` method was not paginating correctly. This fixes that bug.,[],[],0
1021,VAN.get_events() not paginating correctly.,https://api.github.com/repos/move-coop/parsons/issues/168,,168,closed,2020-02-27 22:50:05+00:00,2020-03-08 23:45:18+00:00,2020-03-08 23:45:18+00:00,It appears as though the `VAN.get_events()` method is not paginating correctly.,"[Label(name=""bug"")]",[],1
1022,upgrade to facebook business 6.0.0,https://api.github.com/repos/move-coop/parsons/issues/167,Bella Wang,167,closed,2020-02-26 17:41:08+00:00,2020-02-29 02:46:34+00:00,2020-02-29 02:46:34+00:00,"The Graph API was using v4.0, which does not work for new Facebook apps",[],[],2
1023,Table: To/From Postgres,https://api.github.com/repos/move-coop/parsons/issues/166,,166,closed,2020-02-25 17:05:13+00:00,2020-03-04 16:18:53+00:00,2020-02-27 19:18:01+00:00,"Add the following convenience methods:

`Table.to_postgres()`
`Table.from_postgres()`",[],[],1
1024,Create to/from Postgres Table Methods,https://api.github.com/repos/move-coop/parsons/issues/165,,165,closed,2020-02-23 18:57:42+00:00,2020-02-27 22:49:24+00:00,2020-02-27 22:49:24+00:00,"Similar to the Redshift pattern, create the `Table.to_postgres()` and the `Table.from_postgres()` convenience methods. ","[Label(name=""enhancement"")]",[],0
1025,Database Sync: Fix Empty Table Bug,https://api.github.com/repos/move-coop/parsons/issues/164,,164,closed,2020-02-23 00:07:23+00:00,2020-02-24 17:57:50+00:00,2020-02-23 18:44:14+00:00,The database sync class was failing when it encountered an empty source table. This detects empty tables and exits the sync prior to failing.,[],[],0
1026,Database Sync: Add Redshift Support,https://api.github.com/repos/move-coop/parsons/issues/163,,163,closed,2020-02-21 17:00:54+00:00,2020-02-24 17:57:50+00:00,2020-02-24 15:22:31+00:00,This PR adds support for Redshift databases to the DB Sync class.,[],[],0
1027,Bill.com Parsons Class,https://api.github.com/repos/move-coop/parsons/issues/162,,162,closed,2020-02-21 14:24:21+00:00,2020-03-09 03:45:27+00:00,2020-02-24 13:46:17+00:00,All credit goes to my friend Kevin (@dekedor). All blame (including potentially bungling the rebase) belongs with me.,[],[],0
1028,Fixed odd Freshdesk contacts parameter,https://api.github.com/repos/move-coop/parsons/issues/161,Soren Spicknall,161,closed,2020-02-20 22:19:58+00:00,2020-02-24 17:58:00+00:00,2020-02-21 16:28:55+00:00,"I noticed while testing Freshdesk-related code that the `updated_since` parameter is actually supposed to be `_updated_since` when being used with calls to their contacts endpoint, and the current parameter isn't recognized when passed to Freshdesk. I made a few quick substitutions to align the Parsons parameter with what's expected by the Freshdesk API for contacts.",[],[],1
1029,add noise to upsert staging table name,https://api.github.com/repos/move-coop/parsons/issues/160,Eliot Stone,160,closed,2020-02-20 19:56:34+00:00,2020-02-20 20:27:15+00:00,2020-02-20 20:27:12+00:00,"This commit changes the naming convention for the staging table
used by the `Redshift.upsert` method. This commit adds `stg` to
the name of the table. It also adds some random noise to the
end of the table. Before this commit, running the `upsert` command
in parallel against the same table could result in errors from
Redshift from trying to create the same table at the same time.",[],[],0
1030,Implement redash support,https://api.github.com/repos/move-coop/parsons/issues/159,Schuyler Duveen,159,closed,2020-02-19 21:31:55+00:00,2020-02-21 16:00:35+00:00,2020-02-21 16:00:35+00:00,"For https://redash.io/ for loading data from redash queries.

MoveOn uses Redash internally to manage and save/run SQL queries from several inhouse databases/sources.",[],[],1
1031,Database Sync: Initial Methods,https://api.github.com/repos/move-coop/parsons/issues/158,,158,closed,2020-02-17 17:41:32+00:00,2020-02-24 17:57:51+00:00,2020-02-20 21:54:57+00:00,"This class allows for for tables to be synchronized between multiple databases. The pattern is the following:

- `DbSync.table_sync_full()` copies _all_ data in a source table to a destination table.
- `DbSync.table_sync_incremental()` copies only new records based on a specified primary key. This only works for tables that are updated incrementally.

**Example Usage**

```
   # Create source and destination database objects
   source_rs = Postgres()
   destination_rs = Postgres()

   # Create db sync object and run sync.
   db_sync = DBSync(source_rs, destination_rs) # Create DBSync Object
   db_sync.table_sync_full('parsons.source_data', 'parsons.destination_data')

   # Create db sync object and run sync.
   db_sync = DBSync(source_pg, destination_pg) # Create DBSync Object
   db_sync.table_sync_incremental('parsons.source_data', 'parsons.destination_data', 'myid')
```

**Notes**

- **Currently, this class only allows for the synchronization of Postgres tables. Future PRs will provide support for Redshift and BigQuery.** 
- The `DbSync.table_sync_incremental()` can be likely be used for in conjunction with a `date_created` column, however this use case has not yet been tested extensively.
- This PR introduces a new `Table` class, which creates a table object. There are many potential future uses for this class outside of the DBSync model.",[],[],0
1032,document low resource patterns,https://api.github.com/repos/move-coop/parsons/issues/157,Eliot Stone,157,closed,2020-02-14 20:54:25+00:00,2020-02-16 01:45:41+00:00,2020-02-16 01:45:38+00:00,"This commit adds a section to the Parsons docs on utilizing
Parsons in environments where resource consumption is a concern.
The goal is to allow developers to avoid loading all of the
Parsons connectors (and dependent libraries) whenever loading
any of them. In order to make this possible, this commit
introduces an environment variable `PARSONS_SKIP_IMPORT_ALL`
that can be set to skip over importing all classes in the
root `parsons` package.

---
cc: @schuyler1d ",[],[],0
1033,fix salesforce import,https://api.github.com/repos/move-coop/parsons/issues/156,Eliot Stone,156,closed,2020-02-14 17:11:58+00:00,2020-02-14 18:45:00+00:00,2020-02-14 18:44:57+00:00,"This commit fixes a typo in the `parsons.salesforce` package
where the `Salesforce` client is erroneously imported as
`SalesForce`. This commit also removes the `try/except` guards
around imports at the root `parsons` package to prevent
`ImportError`s from being swallowed.

---
cc @schuyler1d ",[],[],0
1034,VAN: Apply a person code,https://api.github.com/repos/move-coop/parsons/issues/155,,155,closed,2020-02-13 20:39:05+00:00,2020-02-24 17:57:53+00:00,2020-02-13 21:08:27+00:00,This method allows the user to apply codes (which include source codes and tags) to a user record.,[],[],0
1035,VAN: Add ability to set phone type on person create.,https://api.github.com/repos/move-coop/parsons/issues/154,,154,closed,2020-02-13 19:32:56+00:00,2020-02-24 17:57:54+00:00,2020-02-13 21:08:13+00:00,Added parameter to `VAN.upsert_person` that allows the user to set the phone type when creating a record.,[],[],0
1036,Freshdesk Connector,https://api.github.com/repos/move-coop/parsons/issues/153,,153,closed,2020-02-06 05:35:34+00:00,2020-02-24 17:58:22+00:00,2020-02-06 23:08:31+00:00,"This is a connector for [Freshdesk](url) which is an omnichannel customer support platform. This initial PR focuses on core tables to extract from Freshdesk, though could be improved in the future to include methods that create tickets and other objects.

This PR also adds a new method `Table.sort()` since the Freshdesk API sorting is a little bit weird.","[Label(name=""enhancement"")]",[],0
1037,Avoid hard requirement for all vendor libraries to import parsons,https://api.github.com/repos/move-coop/parsons/issues/152,Schuyler Duveen,152,closed,2020-02-05 21:41:58+00:00,2020-02-10 14:03:59+00:00,2020-02-10 14:03:59+00:00,"Fixes #146 

This is take-2 from #147 after @eliotst  gave feedback.",[],[],1
1038,s3: support extra args in more methods,https://api.github.com/repos/move-coop/parsons/issues/151,Eliot Stone,151,closed,2020-02-05 17:34:16+00:00,2020-02-06 14:20:50+00:00,2020-02-06 14:20:47+00:00,"This commit adds support for passing extra args to more of the
methods in the `parsons.aws.s3.S3` class. The new methods are:

 * copy
 * get_file
 * list_keys",[],[],1
1039,Update setup.py to version 0.10.0,https://api.github.com/repos/move-coop/parsons/issues/150,,150,closed,2020-02-04 23:41:17+00:00,2020-02-24 17:58:23+00:00,2020-02-05 15:54:19+00:00,Prepare Parsons package for a new release.,[],[],0
1040,Salesforce class bug fix,https://api.github.com/repos/move-coop/parsons/issues/149,,149,closed,2020-02-03 20:07:47+00:00,2020-02-24 17:58:25+00:00,2020-02-03 20:19:12+00:00,Correcting a mistake in the `client` property,[],[],0
1041,Adds an option to delete original files when transferring between buckets,https://api.github.com/repos/move-coop/parsons/issues/148,Chris Cuellar,148,closed,2020-02-03 19:20:37+00:00,2020-02-24 17:58:24+00:00,2020-02-04 18:41:51+00:00,"Adds an additional arg `remove_original` to the `transfer_bucket` method. Default is set to False.

See #122 ",[],[],0
1042,stop the global package dependencies.,https://api.github.com/repos/move-coop/parsons/issues/147,Schuyler Duveen,147,closed,2020-01-31 21:24:06+00:00,2020-02-05 21:42:21+00:00,2020-02-05 21:42:21+00:00,"fixes #146 

This is a pretty nutso/invasive patch, however it manages to be backwards compatible but should still work if a subset of dependencies is removed/uninstalled.",[],[],2
1043,Too many dependencies,https://api.github.com/repos/move-coop/parsons/issues/146,Schuyler Duveen,146,closed,2020-01-31 16:12:54+00:00,2020-02-10 14:03:59+00:00,2020-02-10 14:03:59+00:00,"The current pattern in `parsons` is to load every submodule into `parsons/__init__.py`.

This is neither sustainable nor feasible for systems that require smaller upload sizes/memory footprints (e.g. AWS Lambda).  The top of that file makes the goal clear:
""Eg. This allows for: `from parsons import VAN`""
However, it means every install and deploy of parsons requires an ever-increasing number of dependencies.

Proposal:
* Remove the pre-amble -- or at least stop adding new ones.  Admittedly this is backwards incompatible
  * If we must keep backwards incompatibility, then let's create a second lib called `parsons_core` where we move the sub-directories, and then import them in `parsons/` from parsons_core.
* Start documenting the best practice of importing `from parsons_core.nvpvan import VAN`
* Document for low environments that they can install/import with `pip install --no-deps parsons` -- maintain a requirements-core.txt for cross-source dependencies that can/should stay in core.
",[],[],1
1044,Salesforce class,https://api.github.com/repos/move-coop/parsons/issues/145,,145,closed,2020-01-30 23:55:34+00:00,2020-01-31 23:03:21+00:00,2020-01-31 23:03:21+00:00,"Discovered during live testing that the handling of `update_record()` was slightly incorrect. Adjusted the method's code, as well as `test_update()`.",[],[],0
1045,redshift: `nullas` -> `null as` in copy statement,https://api.github.com/repos/move-coop/parsons/issues/144,Eliot Stone,144,closed,2020-01-30 18:12:22+00:00,2020-01-30 18:22:10+00:00,2020-01-30 18:20:46+00:00,"This commit updates the function in the Parson's Redshift client
for generating a copy statement when loading data from S3 into
Redshift. Previously, the statement generated used `nullas`
instead of `null as` when adding an optional null-as expression.
This commit fixes the typo.

---

Fixes #143 

cc @asonntag
",[],[],0
1046,"Redshift integration - ""nullas"" should be ""null as""",https://api.github.com/repos/move-coop/parsons/issues/143,Audrey Sonntag,143,closed,2020-01-30 17:43:58+00:00,2020-01-30 18:20:46+00:00,2020-01-30 18:20:46+00:00,"The [Redshift integration code](https://github.com/move-coop/parsons/blob/master/parsons/databases/redshift/rs_copy_table.py ) creates a sql string with the copy function. However, one of the SQL data conversion parameters is incorrect on line 57 of the code.

`if nullas:  sql += f""nullas {nullas}""`

According to the [Redshift data conversion parameters documentation](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-null-as), the correct version of this SQL should be ""NULL AS "" not ""NULLAS"". Therefore, the Python that is using an f-string to create the SQL query should read as follows:

`if nullas:  sql += f""null as {nullas}""`

@eliotst 

",[],[],0
1047,Remove Mobile Commons Connector,https://api.github.com/repos/move-coop/parsons/issues/142,,142,closed,2020-01-30 04:13:26+00:00,2020-02-24 17:58:26+00:00,2020-01-30 18:04:56+00:00,"Currently, the Mobile Commons connector does not work. The vendor is working on releasing a new version of their API later this year. At that point, we will rebuild the connector.","[Label(name=""bug"")]",[],0
1048,Airtable.get_records() Bug Fix,https://api.github.com/repos/move-coop/parsons/issues/141,,141,closed,2020-01-30 03:36:02+00:00,2020-02-24 17:58:28+00:00,2020-01-30 15:02:14+00:00,"Addresses Issue #140 

Checks to see if the table that is returned is empty. If so, does not attempt to run `Table.unpack_dict()`.","[Label(name=""bug"")]",[],0
1049,Airtable.get_records() produces errors when returning zero rows,https://api.github.com/repos/move-coop/parsons/issues/140,,140,closed,2020-01-30 02:18:44+00:00,2020-01-30 15:40:24+00:00,2020-01-30 15:40:24+00:00,"To reproduce: 
```
from parsons import Table, Airtable

# Assuming all credentials and other data in env vars
at = Airtable()

# Successful call
all_rows = at.get_records()
print(all_rows.num_rows)

# Call with filters that would return zero rows will appear to succeed, but...
no_rows = at.get_records(formula=""FIND('SOMETHING UNFINDABLE', {Some Column}) > 0"")
# Error kicks in when you try to do anything with the results
print(no_rows.num_rows)
```
This returns an error `ValueError: 'fields' is not in list`, which suggests that there may be a problem with the use of `unpack_dict()` inside of `get_records()`",[],[],0
1050,Add ability to authenticate using a private rsa key over sftp,https://api.github.com/repos/move-coop/parsons/issues/139,Angela Gloyna,139,closed,2020-01-28 21:25:08+00:00,2020-01-31 15:32:13+00:00,2020-01-31 15:32:12+00:00,"The following PR provides the option to authenticate an sftp connection using a private rsa key as an alternative to username and password authenticate. If a password is provided in addition to a key file path, both will be passed to the paramiko sftp connection configuration. I've added some tests, but they are tests that require a live test environment to succeed.",[],[],0
1051,Salesforce class,https://api.github.com/repos/move-coop/parsons/issues/138,,138,closed,2020-01-28 14:39:06+00:00,2020-01-28 14:50:30+00:00,2020-01-28 14:50:30+00:00,@elyse-weiss hopefully this will solve the problem.,[],[],0
1052,Bill com,https://api.github.com/repos/move-coop/parsons/issues/137,Kevin O'Sullivan,137,closed,2020-01-27 00:28:11+00:00,2020-02-21 13:55:10+00:00,2020-02-21 13:55:10+00:00,"Adds functionality for accessing the Bill.com API. This class will allow the use to

-list users and customers
-query customer and invoice information
-create customer and invoices
-send invoices",[],"[NamedUser(login=""ydamit"")]",0
1053,SFTP SSH,https://api.github.com/repos/move-coop/parsons/issues/136,Angela Gloyna,136,closed,2020-01-26 01:23:32+00:00,2020-01-28 14:49:11+00:00,2020-01-26 01:23:43+00:00,,[],[],6
1054,added trailing slash to generic action endpoint,https://api.github.com/repos/move-coop/parsons/issues/135,,135,closed,2020-01-22 14:41:45+00:00,2020-01-22 16:08:46+00:00,2020-01-22 16:08:43+00:00,Oh the difference one character can make...,[],[],1
1055,Update Docs,https://api.github.com/repos/move-coop/parsons/issues/134,,134,closed,2020-01-18 05:04:41+00:00,2020-01-23 15:39:24+00:00,2020-01-19 00:20:08+00:00,Some formatting changes to the documentation. Just standardizing language in the TOC.,[],[],0
1056,honor delimiter on redshift copy_s3,https://api.github.com/repos/move-coop/parsons/issues/133,Eliot Stone,133,closed,2020-01-16 21:33:43+00:00,2020-01-16 21:47:58+00:00,2020-01-16 21:47:54+00:00,"This commit passes through the delimiter to the `from_csv` method
used to load a CSV file when copying from S3 to Redshift. The call
in question is part of Parsons' effort to reflect on the data
being copied to build a `create table` statement in the case where
the target Redshift table does not exist.",[],[],0
1057,Easy Table Indexing,https://api.github.com/repos/move-coop/parsons/issues/132,,132,closed,2020-01-15 22:04:10+00:00,2020-01-16 19:48:42+00:00,2020-01-16 13:40:28+00:00,"This allows the user to more easily index rows and columns in a Parsons table. Basically, if you put in a string that is a column name, it will grab the column. If you put in an int, then it will look for the row.

h/t @eliotst for the great idea.

```
tbl = Table([{'a': 1, 'b': 2}, {'a': 3, 'b': 4}])

# Return a column as a list
tbl['a']
>> [1, 3]

# Return a row as a dict
tbl[1]
>> {'a': 3, 'b': 4}
```","[Label(name=""enhancement"")]",[],0
1058,"VAN.delete_supporter_group(), etc.",https://api.github.com/repos/move-coop/parsons/issues/131,,131,closed,2020-01-11 21:42:24+00:00,2020-01-23 15:39:23+00:00,2020-01-15 22:50:27+00:00,"- `VAN.delete_supporter_group()` deletes a supporter group.
- `VAN.connection.api_key_profiles()` returns meta data associated with the api key.",[],"[NamedUser(login=""elyse-weiss"")]",0
1059,Table.column_data(),https://api.github.com/repos/move-coop/parsons/issues/130,,130,closed,2020-01-11 19:56:29+00:00,2020-01-16 19:48:44+00:00,2020-01-14 21:01:31+00:00,"This method returns the rows in a column as a list.

```
tbl = Table([['a', 'b'],['1', 2], ['2', 1]])
lst = tbl.column_data('a')
print (lst)
>>> ['1', '2']
```

Potentially, the most elegant solution would be to be able to use the Pandas pattern for column selection, such as (`tbl['a']`). I'm interested in feedback on this one.",[],[],1
1060,Documentation: Add Redshift.alter_table_column_type(),https://api.github.com/repos/move-coop/parsons/issues/129,,129,closed,2020-01-06 20:29:15+00:00,2020-01-09 20:48:06+00:00,2020-01-07 15:57:11+00:00,Simply exposes the doc strings of an underlying Redshift method.,[],[],0
1061,alter_varchar_column_widths() - compare 2 redshift tables,https://api.github.com/repos/move-coop/parsons/issues/128,elyse-weiss,128,closed,2020-01-06 17:44:24+00:00,2022-03-17 17:56:26+00:00,2022-03-17 17:56:26+00:00,"It would be great to be able to compare 2 redshift tables, in addition to a parsons table and a redshift table. ","[Label(name=""enhancement""), Label(name=""low priority"")]",[],1
1062,bigquery: add copy functionality,https://api.github.com/repos/move-coop/parsons/issues/127,Eliot Stone,127,closed,2019-12-18 01:53:26+00:00,2019-12-20 17:24:42+00:00,2019-12-20 17:24:39+00:00,"This commit adds a `copy` method to the
`parsons.google.google_bigquery.GoogleBigQuery` connector. The
`copy` method can be used to load a Parsons table into a BigQuery
table by uploading the file to Google Cloud Storage.",[],[],2
1063,googlecloudstorage: another attempt at downloading blobs,https://api.github.com/repos/move-coop/parsons/issues/126,Eliot Stone,126,closed,2019-12-16 16:12:25+00:00,2019-12-16 20:27:35+00:00,2019-12-16 20:27:32+00:00,"This commit updates the `download_blob` method on the parsons
`GoogleCloudStorage` connector to take a different approach to
downloading blobs. Like the code being replaced, this commit
avoids having to fetch metadata about the bucket the blob is in
(and thus avoids unnecessary permission checks). Unlike the code
being replaced, this method seems more robust. The
`download_blob_to_file` function used by the old code seems to
not handle empty files well.

---
This seems to allow me to download files that you were having problems with, while still not requiring any access to the underlying bucket.",[],[],0
1064,Comp setup dox,https://api.github.com/repos/move-coop/parsons/issues/125,,125,closed,2019-12-12 23:10:21+00:00,2020-01-09 20:48:07+00:00,2019-12-30 19:33:20+00:00,I think this covers some of the most important setup stuff!,[],[],2
1065,Table.to_s3_csv(): Add ability to specify ACL,https://api.github.com/repos/move-coop/parsons/issues/124,,124,closed,2019-12-12 02:51:47+00:00,2019-12-18 04:08:08+00:00,2019-12-12 17:38:46+00:00,Adds the ability to specify the S3 acl for this method. It is simply lifting up an argument that is available in S3.put_file() ,[],[],0
1066,Redshift.alter_varchar_column_widths(),https://api.github.com/repos/move-coop/parsons/issues/123,,123,closed,2019-12-11 22:57:42+00:00,2019-12-18 04:08:08+00:00,2019-12-12 19:48:56+00:00,"This method does the following:

1. Takes a Parsons table and extracts the column widths.
2. Take a Redshift table and extracts the column widths.
3. Compares the widths of columns that match on name.
4. Alters the Redshift table if any of the varchar column widths are smaller than the Parsons table.

What this will do: Reduce the number of copy errors when appending to a table.

Long term: I wish to make this method a standard piece part of the `Redshift.copy()` method and all happen under the hood.","[Label(name=""enhancement"")]",[],0
1067,s3.transfer_bucket - document that original file is not removed from original bucket,https://api.github.com/repos/move-coop/parsons/issues/122,elyse-weiss,122,closed,2019-12-11 19:12:24+00:00,2020-02-04 23:31:28+00:00,2020-02-04 23:31:28+00:00,Documentation should be clear here. ,"[Label(name=""documentation""), Label(name=""enhancement""), Label(name=""good first issue"")]","[NamedUser(login=""ChrisC"")]",3
1068,Create an ActBlue class / connector,https://api.github.com/repos/move-coop/parsons/issues/121,Joe McLaughlin,121,closed,2019-12-11 17:32:45+00:00,2019-12-13 01:54:40+00:00,2019-12-13 01:54:40+00:00,"See this, to start - https://secure.actblue.com/docs/webhooks ",[],[],3
1069,google cloud storage: don't get bucket before dling blob,https://api.github.com/repos/move-coop/parsons/issues/120,Eliot Stone,120,closed,2019-12-10 17:27:08+00:00,2019-12-11 14:06:26+00:00,2019-12-11 14:06:23+00:00,"This commit updates the `GoogleCloudStorage` connector to not call
`get_bucket` when downloading a blob to a local file. It is
possible in Cloud Storage to have access to a blob in a bucket
without actually having access to get the bucket metadata (which
is what `get_bucket` does). The underlying Cloud Storage Python
client has a `download_blob_to_file` method which does not require
that we call `get_bucket`, so this commit switches the Parsons
`GoogleCloudStorage`'s `download_blob` method to use that.

---
Tested locally using a test GCS bucket and blob.

Fixes #115 . Just FYI, `get_bucket` will need the proper permission. This PR doesn't change that. It just makes it so `download_blob` doesn't call `get_bucket`.

cc @jburchard ",[],[],0
1070,Create Rock The Vote Class,https://api.github.com/repos/move-coop/parsons/issues/119,elyse-weiss,119,closed,2019-12-10 13:22:22+00:00,2020-04-27 17:50:42+00:00,2020-04-27 17:50:42+00:00,"We have code in the TMC private repo, and can probably easily create a class.

h/t to Gerard at ACRONYM who I originally stole said code from. ",[],"[NamedUser(login=""eliotst"")]",1
1071,Onboarding Process,https://api.github.com/repos/move-coop/parsons/issues/118,Anthony Martinez,118,closed,2019-12-09 18:37:25+00:00,2020-02-25 18:02:08+00:00,2020-02-25 18:02:08+00:00,"Hey! I mentioned this on the previous week's webinar but am now just making an Issue about it. 

## Problem
Parsons has a very important goal of lowering the barrier for people who wouldn't typically contribute to project or consider themselves technical enough to use parsons. I think that if we are trying to dramatically increase the amount of parsons users and contributors, then we need to focus more on the problems people encounter before they even make it to using parsons in their code- things like (speaking from very specific personal experience) how do I set up a development environment? 

## Proposed solution
I need at least one other collaborator to make a short guide on how to go from a place of knowing very little code (but having a lot of motivation to do something cool) to getting an environment set up and ready to use parsons (as in pip installing and ready to REPL some .py) and then building a small ""hello world"" type script using a feature (maybe something like splitting a large file into specific chunks, or turning json to csv- ideally something practical that real world people are always trying to do but are actually really straightforward to do in parsons).

I'm more than happy to take the lead on this but I would need help in splitting up this work for it to happen on a realistic timeline and be good enough to be useful.",[],[],4
1072,Redshift.upsert() Fix,https://api.github.com/repos/move-coop/parsons/issues/117,,117,closed,2019-12-06 19:44:38+00:00,2019-12-18 04:08:13+00:00,2019-12-06 20:53:01+00:00,"If the initial copy of the data takes place, it should drop the temporary table if it fails. This PR moves the copy statement into the try/finally block with the rest of the code.","[Label(name=""bug"")]",[],0
1073,Drop temp table if upsert fails,https://api.github.com/repos/move-coop/parsons/issues/116,elyse-weiss,116,closed,2019-12-06 14:36:22+00:00,2019-12-06 20:53:37+00:00,2019-12-06 20:53:37+00:00,"Right now, `rs.upsert()` creates a table with a timestamp. If the upsert fails, the timestamp table remains. Can we drop it?","[Label(name=""bug"")]",[],1
1074,GCS get_blob & download_blob methods shouldn't get/list bucket,https://api.github.com/repos/move-coop/parsons/issues/115,elyse-weiss,115,closed,2019-12-04 18:07:05+00:00,2019-12-11 14:06:23+00:00,2019-12-11 14:06:23+00:00,Parsons GCS `get_blob` and `download_blob` methods don't need to get/list the bucket first. This would allow for a more limited set of permissions. ,"[Label(name=""good first issue"")]",[],0
1075,Postgres Connector,https://api.github.com/repos/move-coop/parsons/issues/114,,114,closed,2019-12-03 23:35:56+00:00,2019-12-18 04:08:16+00:00,2019-12-11 22:17:40+00:00,"The new Postgres connector creates the following new methods:

`Postgres.query()` - Query the database.
`Postgres.copy()` - Copy a parsons table into the database.

Some notes:

- Under the hood, I've created a new `PostgresCore` class that the `Postgres` class utilitizes. In a future PR, the `Redshift` class will be refactored so that it also relies on the `PostgresCore` class.

- There are some advanced copy arguments that can/should be added to the `Postgres.copy()` method in furture PRs.",[],"[NamedUser(login=""tonywhittaker"")]",1
1076,ActionKit: Post Generic Actions,https://api.github.com/repos/move-coop/parsons/issues/113,,113,closed,2019-11-29 22:00:04+00:00,2019-12-18 04:08:17+00:00,2019-12-11 22:15:55+00:00,"Changes:

- `ActionKit.create_generic_action()` method.
- AK api does a pretty good job of returning errors. Added some functionality to surface them for the user.

@elyse-weiss (Addresses #93)","[Label(name=""enhancement"")]","[NamedUser(login=""eliotst"")]",0
1077,Table: Display Pretty Jupyter Notebook Tables,https://api.github.com/repos/move-coop/parsons/issues/112,,112,closed,2019-11-29 21:34:37+00:00,2019-12-19 01:13:31+00:00,2019-12-01 06:19:09+00:00,"Similar to pandas dataframes, Parson's tables can automatically be displayed as pretty tables in Jupyter Notebook.

![image](https://user-images.githubusercontent.com/5530043/69890824-9ec43880-12bd-11ea-82fa-ed01e9e8acd4.png)
","[Label(name=""enhancement"")]",[],0
1078,Pass view in Airtable get_records(),https://api.github.com/repos/move-coop/parsons/issues/111,,111,closed,2019-11-26 23:45:28+00:00,2019-11-27 15:23:57+00:00,2019-11-27 15:23:54+00:00,I think it was just a simple omission while creating the `kwargs` dict!,[],[],0
1079,URI option in van_connector and match map addition in _valid_search,https://api.github.com/repos/move-coop/parsons/issues/110,,110,closed,2019-11-26 23:34:03+00:00,2020-03-14 05:00:27+00:00,2020-03-14 05:00:27+00:00,,[],"[NamedUser(login=""ydamit"")]",0
1080,"Give the option for VANConnector uri parameter, set to default https:…",https://api.github.com/repos/move-coop/parsons/issues/109,,109,closed,2019-11-26 21:35:06+00:00,2019-11-26 23:13:37+00:00,2019-11-26 23:13:37+00:00,"Give the option for VANConnector uri parameter, set default to https://api.securevan.com/v4/
This will allow for connecting to other VAN instances such as VoteBuilder",[],[],0
1081,.to_s3_csv should have parameter for acl,https://api.github.com/repos/move-coop/parsons/issues/108,elyse-weiss,108,closed,2019-11-25 21:47:29+00:00,2019-12-13 01:53:30+00:00,2019-12-13 01:53:30+00:00,"`s3.put_file` allows folks to set permissions on a file, but `Table.to_s3_csv` does not, and defaults to full permissions. 

@eliotst feel free to edit!","[Label(name=""good first issue"")]","[NamedUser(login=""jburchard"")]",1
1082,VAN: Fix Bugs,https://api.github.com/repos/move-coop/parsons/issues/107,,107,closed,2019-11-23 01:20:15+00:00,2019-11-30 04:25:41+00:00,2019-11-28 04:45:09+00:00,"- ~~Remove validation requirement of `van_upsert_person()` per issue #10.~~ - Saving for a future PR. 

- Fix bug so that `van.find_person()` works properly. 

- Remove `scores` from default `van.get_person()` returned value. This is not a standard field which is permissioned to all users, so it was throwing an error with the default setting. (@dannyboy15 for viz)","[Label(name=""bug"")]",[],3
1083,VAN: Upload Saved Lists,https://api.github.com/repos/move-coop/parsons/issues/106,,106,closed,2019-11-22 23:07:04+00:00,2019-11-30 04:25:39+00:00,2019-11-26 04:35:10+00:00,"The current VAN API (v4) does not have and endpoint in order to upload saved lists. However, the old SOAP (v3) based API does. This method leverages the v3 API in order to push lists to VAN. 

Methods of note:

`VAN.upload_saved_list()` - This leverages the url_type in order to store a file in clould storage, similar to scores.

`VAN.connection._soap_client()` - A SOAP client that is generated when this method is called. It is not created until the call.","[Label(name=""enhancement"")]",[],1
1084,Gsheet user_entered_value option,https://api.github.com/repos/move-coop/parsons/issues/105,,105,closed,2019-11-22 13:39:12+00:00,2019-11-23 23:55:49+00:00,2019-11-23 23:55:44+00:00,"Need this in order to be able to enter formulas and not have Google Sheets convert them to text. Added a test function but wasn't sure if it was ok for me to run a live test or not, but I tested the functionality separately.",[],[],0
1085,Documentation Cleanup,https://api.github.com/repos/move-coop/parsons/issues/104,,104,closed,2019-11-21 06:50:55+00:00,2019-11-21 17:38:51+00:00,2019-11-21 17:32:59+00:00,Clean up some formatting issues.,[],[],1
1086,Connection Error in the Person class,https://api.github.com/repos/move-coop/parsons/issues/103,Claire Herdeman,103,closed,2019-11-20 21:35:34+00:00,2019-11-21 06:54:12+00:00,2019-11-20 22:13:11+00:00,"Hi there,

I found an error in the People class in the `people.py` file.  I got the following error when trying to use the find_people method: `'VAN' object has no attribute 'post_request'`. It looks like this is because it is calling the `get_request` and `post_request` methods from the VANConnector class on the VAN class. 

I think the simplest fix may be to change the People constructor to:

```
class People(object):

    def __init__(self, van_object):

        self.connection = van_object.connection
```

So that `self.connection` in the People class refers to the VAN connection object rather than the VAN object.",[],[],4
1087,Create MailChimp Class,https://api.github.com/repos/move-coop/parsons/issues/102,,102,closed,2019-11-19 16:58:07+00:00,2020-05-19 16:57:44+00:00,2020-05-19 16:57:44+00:00,,"[Label(name=""enhancement""), Label(name=""good first issue"")]","[NamedUser(login=""SorenSpicknall"")]",1
1088,Return additional data with get_columns,https://api.github.com/repos/move-coop/parsons/issues/101,Daniel,101,closed,2019-11-19 08:36:23+00:00,2019-11-19 17:30:30+00:00,2019-11-19 16:08:34+00:00,"[Decimal types in Redshift](https://docs.aws.amazon.com/redshift/latest/dg/r_Numeric_types201.html#r_Numeric_types201-decimal-or-numeric-type) are defined as `decimal(precision, scale)`. With this change, both the precision and the scale are returned.",[],[],1
1089,fix lint error,https://api.github.com/repos/move-coop/parsons/issues/100,Eliot Stone,100,closed,2019-11-19 04:00:53+00:00,2019-11-19 14:00:35+00:00,2019-11-19 14:00:32+00:00,,[],[],0
1090,Fix typos in docs,https://api.github.com/repos/move-coop/parsons/issues/99,Daniel,99,closed,2019-11-19 00:10:27+00:00,2019-11-19 07:35:32+00:00,2019-11-19 00:58:34+00:00,"Fix typos in `rst` files:
* index
* Google Cloud
* VAN",[],[],0
1091,Update and standardize expand_fields,https://api.github.com/repos/move-coop/parsons/issues/98,Daniel,98,closed,2019-11-18 21:57:59+00:00,2019-11-19 07:35:05+00:00,2019-11-19 02:01:53+00:00,"- The `expand` list in `get_event` included a field that isn't supported for that endpoint. Updated the list to include only supported fields.
- Standardize the `expand`/`fields` params and description across methods. Now `expand_fields`.
- Update the `expand_fields` for `get_events` and `get_person` to include missing fields.",[],[],0
1092,Check if file is remote before check it has_data,https://api.github.com/repos/move-coop/parsons/issues/97,Daniel,97,closed,2019-11-16 21:05:31+00:00,2021-03-29 19:05:16+00:00,2019-11-17 05:24:14+00:00,"The `from_s3_csv` method appears to support reading from remote sources (since it uses `petl.fromcsv` [under the hood](https://petl.readthedocs.io/en/stable/io.html#extract-read)). In fact,  the `VAN` class uses it to [download saved lists](https://github.com/move-coop/parsons/blob/master/parsons/ngpvan/saved_lists.py#L65).

To avoid `FileNotFoundError`, we can bypass the `has_data` check if the file is from a remote source.",[],[],0
1093,Fix reduce_row docs,https://api.github.com/repos/move-coop/parsons/issues/96,Daniel,96,closed,2019-11-16 20:37:32+00:00,2021-03-29 19:04:40+00:00,2019-11-17 05:26:48+00:00,"The tabs (`\t`) and newlines (`\n`) are being interpreted as such, instead of as text. By escaping the backslash, they're interpreted as literal text.",[],[],0
1094,Fix slack silent errors,https://api.github.com/repos/move-coop/parsons/issues/95,Daniel,95,closed,2019-11-16 00:56:33+00:00,2021-03-29 19:03:32+00:00,2019-11-17 05:22:37+00:00,"If there is an error with a slack method, it is now raised.",[],[],0
1095,update version of joblib,https://api.github.com/repos/move-coop/parsons/issues/94,Eliot Stone,94,closed,2019-11-15 22:23:35+00:00,2019-11-19 15:01:28+00:00,2019-11-19 15:01:25+00:00,This commit updates the version of joblib from 0.11 to 0.13.,[],[],2
1096,Add Actions to ActionKit Class,https://api.github.com/repos/move-coop/parsons/issues/93,elyse-weiss,93,closed,2019-11-15 20:03:59+00:00,2019-12-11 22:18:08+00:00,2019-12-11 22:18:08+00:00,https://roboticdogs.actionkit.com/docs/manual/api/rest/actionprocessing.html,"[Label(name=""enhancement"")]",[],0
1097,update contributing.md document,https://api.github.com/repos/move-coop/parsons/issues/92,Eliot Stone,92,closed,2019-11-15 17:12:56+00:00,2019-11-15 21:17:07+00:00,2019-11-15 21:17:04+00:00,"This commit updates the contributing.md document to add more
information about contributing to the Parsons project.

---
Aside from copy-editing, looking for a critical eye around making sure this is helpful for people new to open source and/or Parsons.",[],[],0
1098,Email validation/parsing,https://api.github.com/repos/move-coop/parsons/issues/91,Matt Klaber,91,open,2019-11-15 16:48:49+00:00,2023-02-21 19:31:26+00:00,,"Cleaning signup sheets and other human entered email addresses.

Here's the code I use that may be helpful to y'all. (It's a bit messy.) It uses [email-validator](https://pypi.org/project/email-validator/) and [pydash](https://pypi.org/project/pydash/) (because I'm sad I don't get to write in node.js).


```python
from email_validator import validate_email, EmailNotValidError, EmailSyntaxError, EmailUndeliverableError
import re
from pydash import predicates
from pydash.strings import trim, reg_exp_replace, clean, deburr
from typing import List
from pydash.collections import every, filter_
from pydash.arrays import flatten_deep


def empty_if_null(value: str) -> str:
    return value if value else """"

def trim_non_printing(value: str) -> str:
    value = trim(value)
    value = reg_exp_replace(value, '[\u202a\u25a0\u00a0\s]+$', '')
    value = reg_exp_replace(value, '^[\u202a\u25a0\u00a0\s]+', '')
    return value

def clean_email_string(value: str) -> str:
    if not predicates.is_string(value):
        return """"
    # lowercase everything
    value = trim_non_printing(clean(deburr(value)))
    # strip spaces in the middle of the address
    value = reg_exp_replace(value, r'\s+', '')
    return value

email_display_name_re = re.compile(r"".+\<(?P<email>[^@]+@[^\>]+)\>"")

def fix_common_email_problems(value: str) -> str:
    if email_display_name_re.match(value):
        components = email_display_name_re.search(value)
        value = components.group('email')
    value = clean_email_string(value)
    # trim off the start or end: ,  .  :  ""  >  <  '
    # then trim whitespace again
    value = trim(trim(value, ',.:""><\''))
    # fix common suffix issues (could do a better job with this though...)
    value = reg_exp_replace(value, r',com$', '.com')
    return value

def clean_emails(email: str) -> List[str]:
    def _clean_emails(email: str, already_fixed: bool) -> List[any]:
        try:
            return [validate_email(email)['email'].lower()]
        except EmailNotValidError as e:
            msg = str(e)
            if 'It must have exactly one @-sign' in msg:
                print(f'try splitting or {email} with {email.count(""@"")} @ signs')
                for delim in [';', '/', ',', '|']:
                    # if email is split by this delimiter, do we end up with one @ in each set?
                    # if so, split on that delimiter and treat each as their own address in need 
                    # of cleaning.
                    if every(email.split(delim), lambda x: x.count('@') == 1):
                        print(f'the delimiter is {delim}')
                        return list(map(lambda x: _clean_emails(x, False), email.split(delim)))
                print(""Can't figure out what delimiter it is so lets just try cleaning in otherways"")
            if not already_fixed:
                return _clean_emails(fix_common_email_problems(email), True)
            print(f'Giving up, {email} is probably just a really bad address due to {msg}')
            return []
    results = _clean_emails(email, False)
    return filter_(flatten_deep(results))
```
","[Label(name=""enhancement""), Label(name=""parsons core""), Label(name=""low priority"")]",[],1
1099,Plz add OSDI support,https://api.github.com/repos/move-coop/parsons/issues/90,Matt Klaber,90,closed,2019-11-15 16:31:09+00:00,2019-11-19 19:01:36+00:00,2019-11-19 19:01:35+00:00,I can haz OSDI support? ;-),[],[],2
1100,VAN Logging Notification,https://api.github.com/repos/move-coop/parsons/issues/89,,89,closed,2019-11-14 22:14:49+00:00,2019-11-21 17:38:50+00:00,2019-11-15 01:14:59+00:00,A false logging notification was being triggered. This removes it.,[],[],0
1101,VAN apply_activist_code() logger text,https://api.github.com/repos/move-coop/parsons/issues/88,Melissa Woods,88,closed,2019-11-14 19:49:34+00:00,2019-11-15 01:15:49+00:00,2019-11-15 01:15:49+00:00,"When a script uses apply_activist_code a message comes through saying “Method deprecated. Use apply_activist_code() or remove_activist_code().” The AC still gets applied, but it’s a lot of scary red text when it happens for every record.![Screen Shot 2019-10-29 at 10.06.59 AM.png](https://images.zenhubusercontent.com/5d39fbd53f010b79423e5b0c/dec43851-e7b4-4c89-8795-16bea76593de)",[],"[NamedUser(login=""jburchard"")]",1
1102,Postgres support,https://api.github.com/repos/move-coop/parsons/issues/87,Julie Goldberg,87,closed,2019-11-13 22:07:36+00:00,2019-12-11 22:18:22+00:00,2019-12-11 22:18:22+00:00,It would be great if you could write tables to postgres servers as well as redshift. Being able to run queries from them and transfer data out of them would be a bonus. ,"[Label(name=""enhancement"")]",[],4
1103,VAN POST Bulk Import Method,https://api.github.com/repos/move-coop/parsons/issues/86,,86,closed,2019-11-13 13:38:47+00:00,2020-08-14 17:00:02+00:00,2020-08-14 17:00:02+00:00,"As the incidence of VAN usage along with other tools increases, the need to create syncs that move data into VAN will as well. The prospect of using the Bulk Import endpoint rather than individual upserts could streamline syncs into VAN and make them more feasible at scale.","[Label(name=""enhancement""), Label(name=""high priority"")]",[],1
1104,to_redshift() - max varchar length issues,https://api.github.com/repos/move-coop/parsons/issues/85,elyse-weiss,85,closed,2019-11-12 19:28:51+00:00,2019-12-18 20:25:53+00:00,2019-12-18 20:25:52+00:00,"I personally have a number of syncs that use `to_redshift(if_exists='append')` and the call often fails because of varchar length. I am proposing a few changes:

- [ ] for any columns that are varchar, set the column to at lease varchar(100)
- [ ] for other varchar lengths, round up! so varchar(862) would become varchar(1000)",[],"[NamedUser(login=""jburchard"")]",2
1105,Colliding Depedencies,https://api.github.com/repos/move-coop/parsons/issues/84,,84,closed,2019-11-11 19:45:47+00:00,2019-11-21 17:38:50+00:00,2019-11-11 22:27:08+00:00,"Running into some issues in terms of dependencies for Google BigQuery and Google Cloud Storage.

This should fix it.","[Label(name=""bug"")]",[],0
1106,Lint Fix,https://api.github.com/repos/move-coop/parsons/issues/83,,83,closed,2019-11-11 16:57:20+00:00,2019-11-21 17:38:48+00:00,2019-11-11 16:58:14+00:00,There was a push that was causing some lint failures. Not sure why this was not showing up locally with testing. A series of failures on my part.,[],[],0
1107,Salesforce class,https://api.github.com/repos/move-coop/parsons/issues/82,,82,closed,2019-11-11 15:54:58+00:00,2020-01-23 19:54:25+00:00,2020-01-23 19:54:24+00:00,"@jburchard , here's the very basic start to the Salesforce class. I haven't created any unit tests yet, but I have done linting. Do we want the methods to return Parsons Tables, or is our preference to return lists of dicts and let folks make them into Parsons Tables as needed?","[Label(name=""enhancement"")]",[],2
1108,VAN: Upload Scores Method,https://api.github.com/repos/move-coop/parsons/issues/81,,81,closed,2019-11-09 18:07:44+00:00,2019-11-21 17:39:05+00:00,2019-11-11 18:40:25+00:00,"This is meant to replace the old load scores methods. The goal is to make, what is a multistep process a little bit easier.

The old workflow, `VAN.upload_create_load_multi()` required that convert the Parsons table to a compressed zip. It also required that you post the file to the internet, for VAN to grab as well. You also had to calculate averages on your own for the auto approve. Those were a lot of steps.

`VAN.upload_scores()` now allows you to pass in a Parsons table and then does all of the rest of the work for you.

From the example documentation:

```
from parsons import VAN
van = VAN(db='MyVoters') # API key stored as an environmental variable

# If you don't know the id, you can run van.get_scores() to list the
# slots that available and their associated score ids.
score_id = 9999

# Load the Parsons table with the scores. The first column of the table
# must be the person id (e.g. VANID). You could create this from Redshift or
# another source.
tbl = Table.from_csv('winning_scores.csv')

# Specify the score id slot and the column name for each score.
config = [{'id': score_id, 'column': 'winning_model'}]

# If you have multiple models in the same file, you can load them all at the same time.
# In fact, VAN recommends that you do so to reduce their server loads.
config = [{'id': 5555, 'column': 'score1'}, {'id': 5556, 'column': 'score2'}]

# The score file must posted to the internet. This configuration uses S3 to do so. In this
# example, your S3 keys are stored as environmental variables. If not, you can pass them
# as arguments.
job_id = van.upload_scores(tbl, config, url_type='S3', email='info@tmc.org', bucket='tmc-fake')
```

This introduces a new utility `utilities.cloudstorage.post_file()` which is a generalizable method to post files to a public url on the internet. Currently, you need S3 in order to use the `upload_scores()` method, but in the future, we could easily add the ability to do so with Google Cloud Storage or an SFTP.","[Label(name=""enhancement"")]",[],2
1109,rework temp file handling,https://api.github.com/repos/move-coop/parsons/issues/80,Eliot Stone,80,closed,2019-11-08 15:49:00+00:00,2019-11-11 03:01:52+00:00,2019-11-11 03:01:49+00:00,"This commit changes how we manage temporary files in Parsons. We
were using `tempfile.NamedTemporaryFile` from the Python standard
library, but this was causing errors with some Parsons methods
when run on Windows.

The problem is that `NamedTemporaryFile` opens a file handle and
keeps it open until it is cleaned up. However, the
`parsons.utilities.files.create_temp_file` function that used the
`NamedTemporaryFile` was returning the path to the temp file
created, and not to the file handle itself. As such, users of
these temp files had to open them to be able to read data from and
write data to the temp file. On Windows opening the same file
twice can result in an error.

This commit creates a new class `TempFilePath` modelled after
`NamedTemporaryFile` that creates a temp file on disk, but does
not hold onto an open file handle. Instead it just returns the
path to the newly created temp file, which can then be accessed
by others. When the `TempFilePath` instance is cleaned up (e.g.,
when the current Python process ends), it removes the file from
disk. This should give us the best of both worlds.",[],[],2
1110,VAN: Final Connector Cleanup,https://api.github.com/repos/move-coop/parsons/issues/79,,79,closed,2019-11-08 00:30:15+00:00,2019-11-11 16:59:26+00:00,2019-11-08 16:47:36+00:00,"Now that all of the methods have been refactored, we can get rid of the old code that was used to make requests. Also, found out that I missed refactoring `update_signup()` and included it in this PR.",[],[],0
1111,utilities.files.has_data() method,https://api.github.com/repos/move-coop/parsons/issues/78,,78,closed,2019-11-07 03:27:33+00:00,2019-11-08 00:14:30+00:00,2019-11-07 15:58:29+00:00,"This method checks to see if a file has any data in it. It is helpful in a couple of ways:

1. Should be used as a check on its own when running a sync as some vendors post empty files.

```
file = s3.get_file(bucket, key)
if utilities.files.has_data(file):
     ....
else:
    logger.info('File empty. Skipping...')
```

2. It's been incorporated into `Table.from_csv()` so that it raises an exception when a file is empty. This addresses the request raised by @elyse-weiss in #72.",[],[],0
1112,Table: Attribute to return first value in table.,https://api.github.com/repos/move-coop/parsons/issues/77,,77,closed,2019-11-07 02:42:17+00:00,2019-11-08 00:14:29+00:00,2019-11-07 18:52:43+00:00,"This is a convenience method to return the first value in a Parsons Table. 

This is intended to be used with database queries in which a single value is expected to be returned. Ex: `tbl = rs.query('select count(*) from my_schema.my_table')`.

While this required accessing the index previously, now you just run `tbl.first` to access the first value in the table.

Not sure that `value` is the best name for this attribute and open to suggestions.

Intended to address Issue #55.","[Label(name=""enhancement"")]",[],3
1113,Twilio: Documentation Update,https://api.github.com/repos/move-coop/parsons/issues/76,,76,closed,2019-11-05 19:56:42+00:00,2019-11-08 00:14:30+00:00,2019-11-07 16:58:09+00:00,"- Fixed documentation so that it displays the methods.
- Added some examples.",[],[],0
1114,"VAN: PATCH, PUT, DELETE Requests",https://api.github.com/repos/move-coop/parsons/issues/75,,75,closed,2019-11-03 16:16:42+00:00,2019-11-11 16:59:27+00:00,2019-11-08 00:14:12+00:00,"- [x] Added in the standardized `PATCH`, `PUT`, `DELETE` Requests similar to the work done with `POST` and `GET`.
- [x] Updated methods.
- [x] Updated tests.

This should, hopefully, mark the end of the refactoring for the VAN requests...",[],[],1
1115,Twilio: Connector,https://api.github.com/repos/move-coop/parsons/issues/74,,74,closed,2019-11-02 19:09:29+00:00,2019-11-08 00:14:42+00:00,2019-11-05 19:42:17+00:00,"The initial methods for a Twilio connector that allow you to access usage by accounts and type of Twilio resource.

`get_account()`
`get_accounts()`
`get_account_usage()`
`get_messages()`","[Label(name=""enhancement"")]",[],0
1116,Build Azure Class,https://api.github.com/repos/move-coop/parsons/issues/73,elyse-weiss,73,closed,2019-10-31 18:24:30+00:00,2020-07-09 12:12:55+00:00,2020-07-09 12:12:55+00:00,[here is some helpful code](https://gist.github.com/elyse-weiss/6bd367da334268b572aa9951446a3877) to get started with!,"[Label(name=""enhancement""), Label(name=""low priority""), Label(name=""new connector"")]","[NamedUser(login=""eliotst"")]",3
1117,More specific error when Table.from_csv() fails because of empty file,https://api.github.com/repos/move-coop/parsons/issues/72,elyse-weiss,72,closed,2019-10-31 15:18:58+00:00,2019-11-07 19:10:36+00:00,2019-11-07 19:10:36+00:00,"I was running some code to pull down files from Google Cloud Storage, and move them Redshift. I received this error when running `Table.from_csv()`, and when I investigated, it looked like it was just an empty file. Would love for the logging to reflect that, if possible.

![image](https://user-images.githubusercontent.com/29580051/67959989-fa70a880-fbcf-11e9-8952-e8e9faf21e8d.png)
",[],[],1
1118,permission denied error on Windows,https://api.github.com/repos/move-coop/parsons/issues/71,Eliot Stone,71,closed,2019-10-30 18:33:09+00:00,2019-11-11 16:13:04+00:00,2019-11-11 16:13:04+00:00,"Users are seeing a ""Permission denied"" on Windows machines error when running the `Redshift.query` method.

It looks like there is a bug in how Parsons manages temporary files in Windows.

Parsons used the Python standard library's `tempfile.NamedTemporaryFile` to create and track temporary files. The documentation says:

```
Whether the name can be used to open the file a second time, while the named temporary file is still open, varies across platforms (it can be so used on Unix; it cannot on Windows NT or later).
```

So, the problem comes down to Parsons opening a temporary file, and then attempting to open that same file again later on, which doesn't work on Windows.",[],"[NamedUser(login=""eliotst"")]",1
1119,Create CiviCRM Class,https://api.github.com/repos/move-coop/parsons/issues/70,elyse-weiss,70,open,2019-10-29 21:06:49+00:00,2023-02-21 19:29:16+00:00,,Some helpful existing documentation: https://github.com/4ndygu/civicrm_osdi,"[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""medium priority""), Label(name=""new connector"")]",[],11
1120,Sample Code,https://api.github.com/repos/move-coop/parsons/issues/69,elyse-weiss,69,closed,2019-10-28 22:39:23+00:00,2019-10-30 18:17:03+00:00,2019-10-30 18:16:59+00:00,,[],[],1
1121,Adding quotes property to VAN fileLoadingJobs endpoint,https://api.github.com/repos/move-coop/parsons/issues/68,,68,closed,2019-10-25 21:22:32+00:00,2019-11-08 00:14:45+00:00,2019-10-28 13:43:36+00:00,This bug in the VAN `create_file_load_multi` is messing up your VAN score helper.,"[Label(name=""bug"")]",[],0
1122,fix up exception message from APIConnector,https://api.github.com/repos/move-coop/parsons/issues/67,Eliot Stone,67,closed,2019-10-24 18:38:18+00:00,2019-10-27 14:21:42+00:00,2019-10-27 14:21:38+00:00,"This commit fixes the message passed to the `HTTPError` exception
being raised when the 
`parsons.utilities.api_connector.APIConnector` validates an HTTP
response. The new exception message includes the status code and
reason (if provided), along with the body of the response if it
is JSON data.",[],[],1
1123,add Google BigQuery connector,https://api.github.com/repos/move-coop/parsons/issues/66,Eliot Stone,66,closed,2019-10-24 17:25:48+00:00,2019-10-30 01:47:02+00:00,2019-10-30 01:46:59+00:00,"This commit adds a connector for Google BigQuery for running 
queries against a BigQuery table and getting back a parsons table.

---
First stab at #17 ",[],[],2
1124,Fixed bug where params arg wasn't passed into request call,https://api.github.com/repos/move-coop/parsons/issues/65,,65,closed,2019-10-23 12:02:57+00:00,2019-10-25 21:15:22+00:00,2019-10-23 13:36:59+00:00,"Found this bug while trying to use your VAN Score Helper. In `approve_scores()`, you're using the Parsons VAN method `get_score_updates()`, passing `score_id` as an argument. But because of this bug, that argument never gets passed as parameters to the actual request, and so the results aren't actually filtered and we get ALL the score updates. 

We could just filter the Parsons Table it returns, but I figure we'll need to be able to pass parameters to GET requests in the future, and this was such a simple fix.",[],[],0
1125,APIConnector get_request not passing params,https://api.github.com/repos/move-coop/parsons/issues/64,,64,closed,2019-10-22 14:49:29+00:00,2019-11-01 13:50:01+00:00,2019-11-01 13:50:01+00:00,"On line 83 of utilities/api_connector.py :
`r = self.request(url, 'GET', params=None)`

But I think it should be:
`r = self.request(url, 'GET', params=params)`

Unless it intentionally isn't?","[Label(name=""bug"")]","[NamedUser(login=""ydamit"")]",2
1126,VAN: logger error,https://api.github.com/repos/move-coop/parsons/issues/63,Melissa Woods,63,closed,2019-10-22 13:40:26+00:00,2019-11-02 19:11:49+00:00,2019-11-02 19:11:49+00:00,"The error message `TypeError: string indices must be integers` comes up when using toggle_activist_code() , with the following:

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/parsons/ngpvan/people.py in apply_response(self, id, response, id_type, contact_type_id, input_type_id, date_canvassed, result_code_id)
    435             logger.info(f'{id_type.upper()} {id} updated.')
    436         else:
--> 437             logger.info(f""{r[1]['errors'][0]['code']}: {r[1]['errors'][0]['text']}"")
    438             raise ValueError(f""{r[1]['errors'][0]['text']}"")
    439 

It looks like an API error from VAN is coming in as a string, but the logger in Parsons assumes it is a dictionary.
",[],[],2
1127,Build Twilio Endpoints,https://api.github.com/repos/move-coop/parsons/issues/62,elyse-weiss,62,closed,2019-10-18 19:49:24+00:00,2019-11-06 22:51:34+00:00,2019-11-06 22:51:34+00:00,"I think we are most interested in the following endpoints:

- [ ] https://www.twilio.com/docs/sms/api/message-resource#read-multiple-message-resources",[],[],3
1128,VAN: POST Requests,https://api.github.com/repos/move-coop/parsons/issues/61,,61,closed,2019-10-17 19:55:26+00:00,2019-11-08 00:14:32+00:00,2019-11-05 19:41:46+00:00,Refactored all of the POST requests to use the updated methods.,[],[],0
1129,Utilities: Remove Empty Dict Keys Test,https://api.github.com/repos/move-coop/parsons/issues/60,,60,closed,2019-10-16 16:40:35+00:00,2019-10-17 16:31:11+00:00,2019-10-17 15:02:24+00:00,Added tests for `utilities.json_format.remove_empty_keys()`,[],[],0
1130,JSON Remove Keys Fix,https://api.github.com/repos/move-coop/parsons/issues/59,,59,closed,2019-10-16 15:39:09+00:00,2019-10-16 15:50:02+00:00,2019-10-16 15:50:02+00:00,This was causing issues with the Hustle Connector.,[],[],0
1131,temporary file management,https://api.github.com/repos/move-coop/parsons/issues/58,Eliot Stone,58,closed,2019-10-16 02:27:11+00:00,2019-11-14 20:11:48+00:00,2019-10-17 15:45:21+00:00,"This commit updates how Parsons handles temporary files. The
changes are meant to minimize the need for users to manage temp
files themselves, specifically when using a Parsons `Table`.

This commit introduces a new class
`parsons.utilities.resources.ResourceManager` that tracks
resources that are used by one or more `Table` so that the
resources (i.e. temp files) are cleaned up when they are no
longer being used by the table(s).

---

This PR does not deal with issues around temporary files returned by `S3.get_file` or `STP.get_file`, etc. I think figuring out how to manage those temporary files might involve changes to the API's, which I was hoping to avoid.",[],[],3
1132,Add README badges,https://api.github.com/repos/move-coop/parsons/issues/57,,57,closed,2019-10-15 21:43:15+00:00,2019-10-15 22:31:13+00:00,2019-10-15 22:31:10+00:00,Adds Circle CI badge and pypi traffic badge.,[],"[NamedUser(login=""jburchard"")]",0
1133,VAN: Changed Entity Methods,https://api.github.com/repos/move-coop/parsons/issues/56,,56,closed,2019-10-14 22:47:06+00:00,2019-10-17 20:09:42+00:00,2019-10-17 20:09:38+00:00,A couple of the VAN changed entity GET methods.,"[Label(name=""enhancement"")]",[],0
1134,Redshift: One Value Query,https://api.github.com/repos/move-coop/parsons/issues/55,,55,closed,2019-10-11 20:40:42+00:00,2019-11-07 19:10:26+00:00,2019-11-07 19:10:26+00:00,"Often times, when we run a query, we don't need a Parsons table, we just need a single value. This would be a convenience method that allows for this.
","[Label(name=""enhancement"")]",[],0
1135,Fix Circle CI,https://api.github.com/repos/move-coop/parsons/issues/54,,54,closed,2019-10-11 16:09:24+00:00,2019-10-11 20:27:33+00:00,2019-10-11 20:27:27+00:00,"With the change in the repo name, there was a weird issue with CircleCI. This should fix it.","[Label(name=""bug"")]",[],0
1136,Change Repository Name,https://api.github.com/repos/move-coop/parsons/issues/53,,53,closed,2019-10-11 04:24:13+00:00,2019-10-11 04:24:48+00:00,2019-10-11 04:24:45+00:00,Change the Repo name and associated links in the documentation.,[],[],0
1137,VAN: POST Requests,https://api.github.com/repos/move-coop/parsons/issues/52,,52,closed,2019-10-10 21:48:27+00:00,2019-11-21 17:39:06+00:00,2019-10-16 16:45:26+00:00,"Reworking of POST requests.

- [x] Clean up `get_codes()` which I forgot to do in the last PR.
- [x] Update `apply_response()` to use new POST method.
- [x] Update `create_supporter_group()` to use new POST method.
- [x] Added `post_request` to `van_connector`.
- [x] Added `post_request` to `utilities.api_connector`
- [x] Improved response validation for `utilities.api_connector`
- [x] Broke out `toggle_activist_code()` into `apply_activist_code()` and `remove_activist_code()` for the sake of simplicity. 
- [x] Moved activist code class to its own test file.",[],[],0
1138,VAN: Refactor GET Requests (Round 1),https://api.github.com/repos/move-coop/parsons/issues/51,,51,closed,2019-10-05 04:38:32+00:00,2019-10-07 21:55:07+00:00,2019-10-07 19:58:25+00:00,"Moving the rest of the GET requests to the new API connector.

- [x] Codes
- [x] Canvass Responses
- [x] Events
- [x] Locations
- [x] Scores
- [x] Saved Lists
- [x] People
- [x] Signups
- [x] Survey Questions
- [x] Supporter Groups",[],[],0
1139,Salesforce Endpoints,https://api.github.com/repos/move-coop/parsons/issues/50,,50,closed,2019-10-03 16:56:51+00:00,2020-01-28 17:47:54+00:00,2020-01-28 17:47:54+00:00,"Could probably just be a wrapper around https://github.com/simple-salesforce/simple-salesforce

I will probably have to use the above on a member project, and maybe be able to contribute to this from those lessons learned.","[Label(name=""enhancement"")]","[NamedUser(login=""ydamit"")]",0
1140,Standard Request Connector - Second Try,https://api.github.com/repos/move-coop/parsons/issues/49,,49,closed,2019-10-02 22:41:50+00:00,2019-10-07 21:55:08+00:00,2019-10-04 19:16:38+00:00,"Refactor of PR #32.

Based on conversations with @eliotst, the connector is not going to attempt to paginate on `GET` requests as there is just a lot of variability in how pagination is handled across APIs. So, for the time being, we are going to make that functionality specific to each class.",[],[],0
1141,SFTP: Get File Size,https://api.github.com/repos/move-coop/parsons/issues/48,,48,closed,2019-10-02 20:03:34+00:00,2019-10-02 23:40:33+00:00,2019-10-02 21:46:43+00:00,"Doing some work with some SFTP transfers and realized that for logging purposes, it is helpful to know the size of a file that you are downloading.

",[],[],0
1142,Table: File Size Attribute,https://api.github.com/repos/move-coop/parsons/issues/47,,47,closed,2019-10-02 04:26:36+00:00,2019-11-21 17:39:08+00:00,2019-11-09 18:15:03+00:00,"Starting to do a little bit of work to prepare for VAN Bulk Import endpoints. They have some limitations around file size, so I thought that this might be useful for that and other things.

`Table.csv_size` - An attribute to tables that essentially ball parks the eventual CSV size by pulling the first 1K rows of the file and creating a CSV. It then extrapolates the entire file size. This should be definitely considered an estimation.","[Label(name=""enhancement"")]",[],3
1143,Table: Fix zip compression bugs.,https://api.github.com/repos/move-coop/parsons/issues/46,,46,closed,2019-10-01 23:47:18+00:00,2019-10-02 23:40:42+00:00,2019-10-02 19:57:13+00:00,"The `Table.to_s3_csv()` method was not actually compressing the files correctly.

- Fixed compression in `Table.to_s3_csv()`
- Added parameter to `Table.to_csv()` that allows you to specify the underlying file in the archive. I'm hesistant to add more parameters to the `.to_csv()` but I didn't see a better path absent to majorly hacky code. I think that it should be fine.","[Label(name=""bug"")]",[],0
1144,VAN: Get Bulk Import Resources,https://api.github.com/repos/move-coop/parsons/issues/45,,45,closed,2019-10-01 17:14:17+00:00,2019-10-07 21:55:06+00:00,2019-10-07 21:52:31+00:00,"The first PR of many in order to add the ability to access the bulk imports VAN end point.

`VAN.get_bulk_import_resources()` - Returns a list of resources (types of bulk imports) that have been provisioned to the API user.","[Label(name=""enhancement"")]",[],0
1145,Fix Docs Requirements,https://api.github.com/repos/move-coop/parsons/issues/44,,44,closed,2019-10-01 15:40:48+00:00,2019-10-02 23:40:44+00:00,2019-10-01 16:31:04+00:00,The Sphinx document build was failing because it requires the Parsons dependencies installed to build out the website. This PR adds the Parsons requirements to the `./docs/requirements.txt`,"[Label(name=""bug"")]",[],0
1146,ActionKit: Fix Domain,https://api.github.com/repos/move-coop/parsons/issues/43,,43,closed,2019-10-01 04:47:41+00:00,2019-10-02 23:40:44+00:00,2019-10-02 02:03:58+00:00,"Currently to instantiate the ActionKit class, you have to pass in a `domain` argument. It will then hit a url like `domain.actionkit.org`. However, in a few instances, some users have totally custom urls like `ak.myorg.com`. Those with custom orgs cannot currently utilize the connector.

This PR fixes this by expanding out the domain argument to require that the user pass in the full domain, not just the actionkit subdomain.

Was: `domain = 'myorg'`

Now is: `domain = 'myorg.actionkit.org'`

Note: @elyse-weiss this is a breaking change, so you will need to update internal sync scripts.","[Label(name=""bug"")]",[],0
1147,Remove Pandas requirement.,https://api.github.com/repos/move-coop/parsons/issues/42,,42,closed,2019-10-01 03:16:06+00:00,2019-10-02 23:40:45+00:00,2019-10-01 15:42:27+00:00,"Currently, we only us `pandas` only in order to allow tables to be converted to data frames. This is a really minor feature that we rarely, if ever, use. The creates dependencies in `pandas` and `numpy` which require 100M. It is contributing significantly to Parsons bloat.

I've removed the pandas dependency and noted in the documentation, that it is an optional dependency.",[],[],0
1148,Remove documentation building packages from dependencies.,https://api.github.com/repos/move-coop/parsons/issues/41,,41,closed,2019-10-01 02:54:00+00:00,2019-10-02 23:40:46+00:00,2019-10-01 05:59:02+00:00,"The `sphinx` packages that are used to build Parsons documentation were part of our standard requirements for the package. This was added an unneeded 50M in dependencies.

- Removed `sphinx` and `sphinx-rtd-theme` from `requirements.txt`
- Created a new `/docs/requirements.txt` that is called by CircleCI, when it is building the refreshed documentation. This is the only time that we need `sphinx`.",[],[],0
1149,update setup.py for 0.6.1 release,https://api.github.com/repos/move-coop/parsons/issues/40,Tony Whittaker,40,closed,2019-09-30 01:06:22+00:00,2019-10-02 23:40:47+00:00,2019-09-30 01:07:08+00:00,,[],"[NamedUser(login=""tonywhittaker"")]",0
1150,Docs: Redshift Cleanup,https://api.github.com/repos/move-coop/parsons/issues/39,,39,closed,2019-09-29 22:20:26+00:00,2019-10-07 21:55:12+00:00,2019-10-03 18:50:40+00:00,"I spent a bit of time trying to clean up the Redshift Documents a bit.

- Created a few sections in the documentation, (Core Methods, Table Utilities, Schema Utilities) to make it easier to navigate everything that is there.
- I did a bunch of clean up to standardize the language.","[Label(name=""documentation"")]",[],0
1151,Docs: Fix code blocks in docs.,https://api.github.com/repos/move-coop/parsons/issues/38,,38,closed,2019-09-29 03:16:59+00:00,2019-09-29 03:24:57+00:00,2019-09-29 03:17:24+00:00,,"[Label(name=""documentation"")]",[],0
1152,VAN: Update Scores Methods,https://api.github.com/repos/move-coop/parsons/issues/37,,37,closed,2019-09-29 03:06:35+00:00,2019-10-02 23:40:43+00:00,2019-10-02 19:56:13+00:00,"- [x] Adds a little bit of logging
- [x] Standardizing a bit of documentation.
- [x] Unpacks the nested json in the `get_score_updates()` method (@elyse-weiss).
- [x] Returns a dict rather than a table for the single object responses (`get_score`, `get_score_update`)","[Label(name=""enhancement"")]",[],0
1153,VAN: Approve Score Method,https://api.github.com/repos/move-coop/parsons/issues/36,,36,closed,2019-09-28 19:14:16+00:00,2019-11-11 22:31:51+00:00,2019-11-11 22:31:51+00:00,"@elyse-weiss - This would be a great contribution for you to work on.

`van.approve_scores(score_ids, raise_on_error=False)`

Pass in a list of score ids. If they can be approved, then approve. Allow the user to specify if it should fail if it cannot be approved or just to log and return nothing.",[],[],1
1154,"Civis: Instantiate Class Improvement, Access Client",https://api.github.com/repos/move-coop/parsons/issues/35,,35,closed,2019-09-28 16:59:27+00:00,2019-09-29 03:25:01+00:00,2019-09-29 03:19:39+00:00,"- [ ] Removed custom error checking for the `env var` and moved to the standard Parsons utility.
- [ ] Added the ability to access the Civis client directly. This is helpful in allowing for access to low level methods which we may not surface in Parsons.

This PR is the first step in trying to build out more methods that are used in our internal scripts when we interact with Civis. Future PRs will include common methods that access the Civis client.","[Label(name=""enhancement"")]",[],0
1155,"Hustle: Agents, Tags, Docs Cleanup",https://api.github.com/repos/move-coop/parsons/issues/34,,34,closed,2019-09-24 00:54:22+00:00,2019-10-01 03:18:23+00:00,2019-09-29 03:24:26+00:00,"**Added methods**
`get_agent()`
`get_agents()`
`create_agent()`
`update_agent()`
`get_tags()`
`get_tag()`

**Other**
- Cleaned up the documentation and strings from v1
- Fixed a couple of bugs in the v1 code.","[Label(name=""documentation""), Label(name=""enhancement"")]",[],1
1156,"Revert ""VAN: Add ID to find_person() and upsert_person()""",https://api.github.com/repos/move-coop/parsons/issues/33,,33,closed,2019-09-21 20:15:54+00:00,2019-09-23 16:38:37+00:00,2019-09-23 16:38:33+00:00,"I'm having some failing errors with this with the validation and having some issues sorting them out.
Additionally, and this is on me, I think that we are trying to do _too_ many things in the _find_person()_ method.

My suggestion:
- We just direct users to the `get_person()` method if they have an id. That is more intuitive and in line with typical APIs.
- We move the `match_map` to a separate method called, `find_person_dict()` or something like that. I don't think that it needs to have the search validation, as it is an advanced method.

However, in the midst of this, you were fixing a bug too, right?","[Label(name=""bug"")]",[],1
1157,Standardized Requests API Connector,https://api.github.com/repos/move-coop/parsons/issues/32,,32,closed,2019-09-21 18:36:47+00:00,2019-10-07 21:55:26+00:00,2019-10-02 22:51:36+00:00,"One of the original sins of this project was that we never established an underlying API connector class that wraps around the `requests` library. Most of our api connectors utilize `requests` and we written pagination and request in a custom manner for each api.

I propose that we should start moving towards, where possible this basic API connector class for all of our API requests.

**Concerns**
- If there is one thing that we have learned from this project is that there is not standardization across all apis. Pagination is handled in a dozen different ways.
- We should balance the development of this class and not try to overcomplicate it to deal with every weird API out there. That is, if an api is just totally weird, we might just want to build a custom connector.

**This PR**
- I'm starting with just `GET` requests and pagination for this PR.
- I've implemented it for just two methods of the VAN class.

**Moving Forward**
- I'd like to continue working on moving all of the VAN methods to this new connector. The VAN connector is a total mess, which was the original motivation for this PR.
- I need to add the `GET`, `PUT`, `DELETE` methods to this.

**Decision Points**
- I'd like this connector to always return a dict or a list of dicts. The connector class (e.g. VAN, Copper) should handle the conversion to a Parsons Table where appropriate (probably on the method level). Looking through the VAN Class, the decision to try to decide what needed to be a table and what didn't created some ugly code.

","[Label(name=""enhancement"")]","[NamedUser(login=""tonywhittaker""), NamedUser(login=""eliotst""), NamedUser(login=""ydamit"")]",0
1158,VAN: Update Docs,https://api.github.com/repos/move-coop/parsons/issues/31,,31,closed,2019-09-21 04:15:10+00:00,2019-09-29 03:25:14+00:00,2019-09-27 15:18:30+00:00,"The VAN documentation could use some love just cleaning it up and standardizing it.

This is the first half of that work.","[Label(name=""documentation"")]",[],0
1159,Hustle: Create A Simple Global Opt Out Method,https://api.github.com/repos/move-coop/parsons/issues/30,,30,closed,2019-09-20 22:45:50+00:00,2019-12-17 17:34:40+00:00,2019-12-17 17:34:40+00:00,My suspicion is that this is going to be a pretty common use case. The idea is to be able to pass in a phone number and add it to the global DNC list.,[],[],1
1160,Hustle Connector v1,https://api.github.com/repos/move-coop/parsons/issues/29,,29,closed,2019-09-20 22:25:25+00:00,2019-09-29 03:25:10+00:00,2019-09-23 16:30:35+00:00,"Initial pass at a Hustle connector.

Includes the following methods:

`get_organization()`
`get_organizations()`
`get_group()`
`get_groups()`
`get_lead()`
`get_leads()`
`create_lead()`
`create_leads()`
`update_lead()`

Also created a utility to allow for smart column mapping, so different column names can be passed in and it smartly matches them. That is, you can pass in `fn`, `first` or `first_name` and it will all treat the same.
",[],[],1
1161,Phone Number Validation/Parsing,https://api.github.com/repos/move-coop/parsons/issues/28,,28,open,2019-09-18 16:17:04+00:00,2023-11-26 17:21:19+00:00,,We should add in [phone number validation/parsing](https://github.com/daviddrysdale/python-phonenumbers) tools.,"[Label(name=""enhancement""), Label(name=""good first issue""), Label(name=""parsons core""), Label(name=""low priority"")]",[],9
1162,Redshift copy col list,https://api.github.com/repos/move-coop/parsons/issues/27,,27,closed,2019-09-17 22:14:10+00:00,2019-10-02 23:40:52+00:00,2019-09-20 21:42:51+00:00,"@jburchard opening a pull request because this might be the best venue to communicate about further testing. I've set up my tests with the following:

tables to be copied:
```
# Columns in same order as Redshift tables
copy_me = Table([
    {""integer"": 1, ""text"": ""one"", ""boolean"": True, ""manual_time"": datetime.today()}
])
# Columns out of order
order_me = Table([
    {""text"": ""two"", ""boolean"": False, ""manual_time"": datetime.today(), ""integer"": 2}
])
# Columns in order but meant for replacing data rather than appending
truncate_me = Table([
    {""integer"": 3, ""text"": ""three"", ""boolean"": True, ""manual_time"": datetime.today()}
])
```
- Created Redshift table `tmc_scratch.copy_test_simple` with following columns
-- integer
-- text
-- boolean
-- manual_time
- Created Redshift table `tmc_scratch.copy_test_complex` with following columns
-- id (`IDENTITY(1,1)` and `DISTKEY` and `SORTKEY`)
-- all the columns from `copy_test_simple`
-- redshift_created_at (auto-populated timestamp)

I've tried the following tests:
- *Normal append to simple*: `copy_me.to_redshift('tmc_scratch.copy_test_simple', if_exists=""append"")` (Success)
- *Normal append to complex*: `copy_me.to_redshift('tmc_scratch.copy_test_complex', if_exists=""append"")` (Fail with expected error)
- *Append to complex with specifycols*: `copy_me.to_redshift('tmc_scratch.copy_test_complex', if_exists=""append"", specifycols=True)` (Success, random `id`, `redshift_created_at` good)
- *Disordered append to simple*: `order_me.to_redshift('tmc_scratch.copy_test_simple', if_exists=""append"")` (Fail with expected error)
- *Disordered append to simple with specifycols*: `order_me.to_redshift('tmc_scratch.copy_test_simple', if_exists=""append"", specifycols=True)` (Success)
- *Normal truncate to simple*: `truncate_me.to_redshift('tmc_scratch.copy_test_simple', if_exists=""truncate"")` (Success)
- *Normal truncate to complex*: `truncate_me.to_redshift('tmc_scratch.copy_test_complex', if_exists=""truncate"")` (Fail with expected error)
- *Truncate to complex with specifycols*: `truncate_me.to_redshift('tmc_scratch.copy_test_complex', if_exists=""truncate"", specifycols=True)` (Success, random `id`, `redshift_created_at` good)",[],[],1
1163,CrowdTangle: Change String Args To Lists,https://api.github.com/repos/move-coop/parsons/issues/26,,26,closed,2019-09-17 12:49:13+00:00,2019-09-29 03:25:18+00:00,2019-09-17 14:39:51+00:00,"- Cleaned up and clarified the doc strings for the methods.

- Some of the arguments allowed for multiple values, separated by commas. Those arguments have now been changed to `lists`, which is a bit more pythonic.

e.g. `list_ids = '1,2,3,4'` is now `list_ids = ['1','2','3','4']`","[Label(name=""bug"")]",[],0
1164,VAN: Add ID to find_person() and upsert_person(),https://api.github.com/repos/move-coop/parsons/issues/25,,25,closed,2019-09-16 14:53:15+00:00,2019-12-19 01:13:35+00:00,2019-09-20 22:01:58+00:00,"@jburchard besides passing the tests I adapted in `test_people.py`, I've tested this functionality in the following scenarios, and include results:

Able to find myself without error, using all of the following:
- `van.find_person(id=100520240)`
- `van.find_person(firstName=""Yotam"", lastName=""Amit"", email=""paxarchy@gmail.com"")`
- `van.find_person(id=100520240, firstName='Yotam')`
- `van.find_person(match_map={""vanid"": 100520240})`

Able to update my data with both of the following:
- `van.upsert_person(id=100520240, match_map={""middleName"": ""David""})`
- `van.upsert_person(firstName=""Yotam"", lastName=""Amit"", email=MY EMAIL, match_map={""dateOfBirth"": ""1984-08-03""})`","[Label(name=""bug""), Label(name=""enhancement"")]",[],0
1165,Create CODE_OF_CONDUCT.md,https://api.github.com/repos/move-coop/parsons/issues/24,,24,closed,2019-09-15 18:26:47+00:00,2019-09-17 14:46:12+00:00,2019-09-15 18:27:27+00:00,,"[Label(name=""documentation"")]",[],0
1166,Documentation Bugs,https://api.github.com/repos/move-coop/parsons/issues/23,,23,closed,2019-09-15 16:51:42+00:00,2019-09-29 03:25:18+00:00,2019-09-16 02:22:20+00:00,Sphinx was throwing a bunch of warnings and errors. This fixes most of them.,"[Label(name=""bug"")]",[],0
1167,Hustle Leads Endpoint,https://api.github.com/repos/move-coop/parsons/issues/22,elyse-weiss,22,closed,2019-09-14 19:18:13+00:00,2019-09-23 16:39:00+00:00,2019-09-23 16:39:00+00:00,"https://api.hustle.com/docs/#operation/createLead

createLead is priority, but all lead endpoints needed","[Label(name=""enhancement"")]","[NamedUser(login=""jburchard"")]",1
1168,MobileCommons Class,https://api.github.com/repos/move-coop/parsons/issues/21,elyse-weiss,21,closed,2019-09-14 19:15:29+00:00,2020-01-28 17:55:14+00:00,2020-01-28 17:55:14+00:00,,"[Label(name=""blocked"")]",[],1
1169,Hustle Class,https://api.github.com/repos/move-coop/parsons/issues/20,elyse-weiss,20,closed,2019-09-14 19:14:02+00:00,2019-09-29 03:42:59+00:00,2019-09-29 03:42:59+00:00,"Hustle just released a beta API, and we're eager to be able to get this incorporated into Parsons.

Docs here: https://api.hustle.com/docs/

Categories of endpoints in priority order are:
- [x] Leads
- [x] Agents
- [x] Groups
- [x] Tags
",[],[],0
1170,Add Create Saved List to NGPVAN,https://api.github.com/repos/move-coop/parsons/issues/19,elyse-weiss,19,closed,2019-09-12 16:58:54+00:00,2019-11-26 04:39:28+00:00,2019-11-26 04:39:28+00:00,"This is a common need, so would be good to have. ",[],"[NamedUser(login=""jburchard"")]",2
1171,Documentation - Lots of Small Updates/Edits,https://api.github.com/repos/move-coop/parsons/issues/18,,18,closed,2019-09-12 03:32:10+00:00,2019-09-29 03:25:20+00:00,2019-09-17 14:44:36+00:00,,"[Label(name=""documentation"")]",[],0
1172,Google BigQuery Connector,https://api.github.com/repos/move-coop/parsons/issues/17,,17,closed,2019-09-11 20:43:02+00:00,2019-11-01 16:04:04+00:00,2019-11-01 16:04:04+00:00,We need to add this sooner rather than later.,"[Label(name=""enhancement"")]","[NamedUser(login=""eliotst"")]",0
1173,Table.to_s3_csv(): Add ability to set a public url,https://api.github.com/repos/move-coop/parsons/issues/16,,16,closed,2019-09-11 19:52:05+00:00,2019-09-17 14:45:59+00:00,2019-09-12 19:16:18+00:00,This allows you to now create a public url from this method. Interested in thoughts and feedback.,"[Label(name=""enhancement"")]",[],0
1174,VAN Connector,https://api.github.com/repos/move-coop/parsons/issues/15,,15,closed,2019-09-11 16:47:23+00:00,2019-11-08 00:15:45+00:00,2019-11-08 00:15:45+00:00,"The VAN connector works, but it was the first one that we built and needs some love.

- [x] Create scaffolding for standardized APIConnector class
- [x] Create API `GET` methods that paginate
- [x] Refactor all relevant VAN methods to use the new `GET` method.
- [x] Create API `POST` method.
- [x] Refactor all relevant VAN methods to use the new `POST` method.
- [x] Create API `DELETE` methods.
- [x] Refactor all relevant VAN methods to use the new `DELETE` method.
- [x] Create API `PATCH` methods.
- [x] Refactor all relevant VAN methods to use the new `PATCH` method.",[],"[NamedUser(login=""jburchard"")]",0
1175,Table.to_csv Zip Compression,https://api.github.com/repos/move-coop/parsons/issues/14,,14,closed,2019-09-10 21:55:07+00:00,2019-09-11 19:03:34+00:00,2019-09-11 19:03:31+00:00,"- Add compression type `.zip` to `Table.to_csv()` method.
- Add stand alone `Table.to_csv_zip()` method that allows for the specification of the file name and the ability to append to an existing archive.","[Label(name=""enhancement"")]",[],1
1176,Redshift: Copy method to include column list,https://api.github.com/repos/move-coop/parsons/issues/13,,13,closed,2019-09-09 21:55:32+00:00,2019-09-24 10:42:20+00:00,2019-09-24 10:42:20+00:00,"For appending to existing target tables, but source tables don't have all the columns/we want to use default values for one or more columnns (e.g. timestamps)","[Label(name=""enhancement"")]",[],0
1177,CrowdTangle Connector,https://api.github.com/repos/move-coop/parsons/issues/12,,12,closed,2019-09-08 01:59:33+00:00,2019-09-15 18:23:17+00:00,2019-09-11 15:51:59+00:00,Connector for CrowdTangle.,"[Label(name=""enhancement"")]",[],0
1178,Update VAN Docs,https://api.github.com/repos/move-coop/parsons/issues/11,,11,closed,2019-09-08 00:44:22+00:00,2019-09-15 18:23:38+00:00,2019-09-11 16:15:53+00:00,"Addresses Issue #9 

@kbenker ","[Label(name=""documentation"")]",[],0
1179,VAN: validation should be optional for upsert_person,https://api.github.com/repos/move-coop/parsons/issues/10,Kayla Benker,10,closed,2019-09-06 17:34:38+00:00,2019-12-03 17:59:13+00:00,2019-12-03 17:59:13+00:00,"When you pass a record to `upsert_person`, it validates that ""the minimum combination of fields were passed."" The VAN API doesn't actually reject records without this info; it will just always create new records if there's insufficient info to match on. 

I understand that some users may want this validation to avoid adding duplicates, but others may still want to add new records for people they have sparser information on.",[],[],2
1180,docs: upsert_person doesn't return Parsons Table,https://api.github.com/repos/move-coop/parsons/issues/9,Kayla Benker,9,closed,2019-09-06 17:29:30+00:00,2019-09-11 16:17:25+00:00,2019-09-11 16:17:25+00:00,"The [docs ](https://move-coop.github.io/parsons_public/html/ngpvan.html#people)for the `upsert_person` method say it returns a Parsons Table object, but when I used it, it returned a dictionary of the vanid and other info about the person (if matched), or the vanid and a `status` (if unmatched/new).",[],[],0
1181,Mobilize America - New Endpoints,https://api.github.com/repos/move-coop/parsons/issues/8,,8,closed,2019-09-05 15:07:14+00:00,2019-09-06 16:26:18+00:00,2019-09-06 16:26:12+00:00,"Added methods:

- `get_events_organization()`
- `get_people()`","[Label(name=""enhancement"")]",[],0
1182,MobileCommons - Add Connection Info to Documentation,https://api.github.com/repos/move-coop/parsons/issues/7,elyse-weiss,7,closed,2019-09-03 13:43:31+00:00,2020-01-28 17:48:54+00:00,2020-01-28 17:48:53+00:00,Other classes have clear connection info at the top of documentation. We should add for MobileCommons. ,"[Label(name=""documentation""), Label(name=""blocked"")]",[],1
1183,MobileCommons - Addl Endpoints,https://api.github.com/repos/move-coop/parsons/issues/6,elyse-weiss,6,closed,2019-09-03 13:39:24+00:00,2020-01-28 17:48:47+00:00,2020-01-28 17:48:47+00:00,"[MC API Docs](https://drive.google.com/file/d/1ga2X0yO-el-4-gr7dbluZ73fusqtE8_u/view?ts=5d6e6b7b)

We'll want to add functions for the following endpoints, listed in priority order:

- [ ] List mConnects
- [ ] Count MConnect Calls
- [ ] List Keywords
- [ ] List Campaign Subscribers
- [ ] List Incoming Messages
- [ ] List Outgoing Messages
- [ ] List Broadcast
- [ ] List Broadcasts
- [ ] List Calls
- [ ] Donation Summary
- [ ] List Web Clicks
- [ ] List Tinyurls
- [ ] Add Tag
- [ ] Remove Tag
- [ ] List Tags
- [ ] Campaign Opted-In
- [ ] Attachments
- [ ] Profile Update Attribute
- [ ] List mData
- [ ] List mData Queries","[Label(name=""enhancement""), Label(name=""blocked"")]",[],1
1184,MobileCommons Error Handling,https://api.github.com/repos/move-coop/parsons/issues/5,elyse-weiss,5,closed,2019-09-03 13:26:53+00:00,2020-01-28 17:48:57+00:00,2020-01-28 17:48:57+00:00,"When the credentials are incorrect, the script raises xml.parsers.expat.ExpatError: mismatched tag:. Instead, it should check for a <Response [401]> and raise a more descriptive error like Invalid credentials.","[Label(name=""enhancement""), Label(name=""blocked"")]",[],1
1185,MobileCommons Bugs,https://api.github.com/repos/move-coop/parsons/issues/4,elyse-weiss,4,closed,2019-09-03 13:23:37+00:00,2020-01-28 17:48:33+00:00,2020-01-28 17:48:33+00:00,"There are a few bugs I discovered in the existing MobilleCommons class:

- [ ] profiles filters do not seem to work beyond limit - page seems a little useless because there is no order by and pagination is already incorporated.
- [ ] group members filters do not seem to work
- [ ] We will want to add automatic pagination to groups and group members. Pagination exists in profiles, so the code can be lifted from there. 

Please additionally test the endpoints that add and delete. ","[Label(name=""bug""), Label(name=""blocked"")]",[],1
1186,Update installation instructions ,https://api.github.com/repos/move-coop/parsons/issues/3,Tony Whittaker,3,closed,2019-08-29 01:48:08+00:00,2019-09-17 14:45:43+00:00,2019-09-02 18:16:09+00:00,,"[Label(name=""documentation"")]","[NamedUser(login=""tonywhittaker"")]",0
1187,Misc. updates for release,https://api.github.com/repos/move-coop/parsons/issues/2,Tony Whittaker,2,closed,2019-08-29 01:11:20+00:00,2019-09-17 14:45:29+00:00,2019-08-29 01:13:37+00:00,Updates to setup.py and related files for first release.,"[Label(name=""bug"")]",[],0
1188,Documentation v1,https://api.github.com/repos/move-coop/parsons/issues/1,,1,closed,2019-08-25 20:20:22+00:00,2019-09-17 14:45:02+00:00,2019-08-26 15:04:01+00:00,,"[Label(name=""documentation"")]",[],0
